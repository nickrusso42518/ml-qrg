{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f731ace-762d-4381-aa92-9ea3d2d1122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell first if you intend to run other Python cells later.\n",
    "# This cell notwithstanding, all other cells are independent.\n",
    "# If you are only reading the text and equations, you don't have to run this cell.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f39c6c7-dfd8-4f61-b9dd-e1c2e0643328",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ba01e-47c3-45df-a1ee-8490109af68f",
   "metadata": {},
   "source": [
    "### Matrix shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c8644-fa7b-4337-b032-96f82076e01f",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes the shape (dimensions) of common matrices in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663807a5-8c8f-4ae4-b17d-4ef2aedcea09",
   "metadata": {},
   "source": [
    "#### X: Input dataset\n",
    "Each column is a training example and each row is a feature. The transpose of this matrix is used for linear and logistic regression analyses with sklearn. The number of training examples is $m$ and the number of input features is either $n_0$ or $n_x$. If each training example is a multi-dimensional array, such as a grayscale image (2D: length x width) or color image (3D: length x width x 3 RBG channels), it must be unrolled into a column vector whereby the number of rows is equal to the production of all dimensions.\n",
    "\n",
    "It is common to vertically split $X$ into 3 parts after shuffling the examples: training, validating, and testing. Typical percentage splits between $J_{train}$/$J_{cv}$/$J_{test}$ are 70/15/15 or 60/20/20. As the number of training examples increases, $J_{train}$ can also increase. If there are millions of examples, a 98/1/1 split may be adequate.\n",
    "\n",
    "$$\n",
    "X ({n_0}, m) = A^{[0]} =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\cdots & x^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x^{(1)}_{n_0} & x^{(2)}_{n_0} & \\cdots & x^{(m)}_{n_0}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7490804-44b9-41e3-bf43-7a46b868f2a9",
   "metadata": {},
   "source": [
    "#### Y: Expected outputs/labels\n",
    "For linear and logistic regression problems, there will be a \"right answer\" corresponding to each training example. These neural networks will have a single unit in the output layer $(n_L=1)$. A multi-class problem (e.g. softmax) will also have one label per training example, although it is commonly one-hot encoded (OHE) into one label per class, represented by rows. This corresponds to one unit per class in the output layer of the neural network. Likewise, a multi-label problem (e.g. many yes/no questions at once) will have a similar shape, albeit without the one-hot restriction. Predictions/inferences from the model are stored in $\\hat{y}$ or \"y-hat\" which always has the same shape as $y$ once OHE, if required, is complete.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Y_{singleclass} (1, m) & =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\cdots & y^{(m)} \\\\\n",
    "\\end{bmatrix} \\\\\n",
    "y_{multiclass} (n_L, m) & =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_1 & y^{(2)}_1 & \\cdots & y^{(m)}_1 \\\\\n",
    "y^{(1)}_2 & y^{(2)}_2 & \\cdots & y^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(1)}_{n_L} & y^{(2)}_{n_L} & \\cdots & y^{(m)}_{n_L}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4823298-acea-4fe1-b0e4-2c9420f43186",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### W: Weights\n",
    "Each value corresponds to an edge, interconnecting units from the current layer to units in the previous layer. The rows correspond to the current layer's units while the columns correspond to the previous layer's units. Note that this matrix is a function of the neural network architecture and not a function of the training example quantity. No $W$ matrix ever has a dimension of $m$.\n",
    "\n",
    "$$\n",
    "W^{[l]} (n_l, {n_{l-1}}) =\n",
    "\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,n_{l-1}} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,n_{l-1}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n_l,1} & w_{n_l,2} & \\cdots & w_{n_l,n_{l-1}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b03b54d-3e3f-407f-8999-b459d60c4901",
   "metadata": {},
   "source": [
    "#### b: Biases\n",
    "A column vector with a value for each unit in the current layer. Like weights, the shape of this matrix depends upon the neural network's architecture (current layer only) and not the number of training examples $m$.\n",
    "\n",
    "$$\n",
    "b^{[l]} (n_l, 1)=\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{n_l} \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ea9da-cca1-478c-8e37-fbba5de67d98",
   "metadata": {},
   "source": [
    "#### Z: Linear outputs\n",
    "The algebraic combination of weights, biases, and previous layer activations. At the first layer, the previous layer activations is equal to $X$, the input data. Each column corresponds to a training example just like $X$ and $Y$ while the rows correspond to the number of units in the current layer.\n",
    "\n",
    "$$\n",
    "Z^{[l]} (n_l, m)=\n",
    "\\begin{bmatrix}\n",
    "z^{(1)}_1 & z^{(2)}_1 & \\cdots & z^{(m)}_1 \\\\\n",
    "z^{(1)}_2 & z^{(2)}_2 & \\cdots & z^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "z^{(1)}_{n_l} & z^{(2)}_{n_l} & \\cdots & z^{(m)}_{n_l}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ec2166-bc71-4fa6-90f8-01361dd45ac9",
   "metadata": {},
   "source": [
    "#### A: Activations\n",
    "A transform function that enables a layer to represent the linear output in a more meaningful way to the next layer. Same shape as $Z$.\n",
    "\n",
    "$$\n",
    "A^{[l]} (n_l, m)=\n",
    "\\begin{bmatrix}\n",
    "a^{(1)}_1 & a^{(2)}_1 & \\cdots & a^{(m)}_1 \\\\\n",
    "a^{(1)}_2 & a^{(2)}_2 & \\cdots & a^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "a^{(1)}_{n_l} & a^{(2)}_{n_l} & \\cdots & a^{(m)}_{n_l}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e5561-faa5-488d-ab85-a87b7fb1a4ca",
   "metadata": {},
   "source": [
    "### Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a3950-55f1-48b4-9988-23b6eafe1712",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Sometimes features (rows of $X$ or $Y$) are represented by non-numeric identifiers, such as strings. This section explains how such data can be encoded in a machine-friendly format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ef5a0-5676-4084-a473-fbc25ba5febf",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding\n",
    "Ordinal encoding maps each unique string into a unique number. Such a technique is useful for decision trees or other algorithms that can learn/split on categorical values. It also works well when there is an intrinsic order to the categories, such as small (1), medium (2) and large (3). Ordinal encoding is sometimes known as \"label encoding\", not to be confused with the expected output labels of $Y$.\n",
    "\n",
    "Consider $X$ with 3 features: height in inches, weight in pounds, and T-shirt size. Ordinal encoding answers the open-ended question in a single row of $X$: \"What is the T-shirt size?\"\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "60 & 65 & 62 & \\cdots & 70 \\\\\n",
    "100 & 140 & 110 & \\cdots & 180 \\\\\n",
    "\\text{small} & \\text{medium} & \\text{small} & \\cdots & \\text{large}\n",
    "\\end{bmatrix}\n",
    "\\verb|--(ordinal_encode)-->|\n",
    "\\begin{bmatrix}\n",
    "60 & 65 & 62 & \\cdots & 70 \\\\\n",
    "100 & 140 & 110 & \\cdots & 180 \\\\\n",
    "1 & 2 & 1 & \\cdots & 3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Ordinal encoding does not require a meaningful amount of additional disk space or memory as it does not change the dimensionality of the source matrix. However, it's a poor choice for nominal, unorderable features. For example, the average of man (1), bear (2), pig (3) would be bear (2), which doesn't make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd044f-4f24-4026-b2be-efd197df4647",
   "metadata": {},
   "source": [
    "#### One-hot Encoding (OHE)\n",
    "One-hot encoding can be applied to features of $X$ or $Y$ (rows) to create a binary representation of the classes. Each training example in $X$ or label in $Y$ (columns) will have a single 1 with all other values set to 0.\n",
    "\n",
    "For input features of $X$, OHE works better for nominal categories whereby there is no implicit ordering, such as the eeye colors of brown, blue, and green. Unlike T-shirt sizes, eye color is unlikely to be correlated with height or weight, and there is no direct comparison between the colors. A new feature is created for each possible category, which each answer a binary question:\n",
    "  1. Is the eye color brown?\n",
    "  2. Is the eye color blue?\n",
    "  3. Is the eye color green?\n",
    "\n",
    "Consider $X$ with 3 features: height in inches, weight in pounds, and eye color.\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "60 & 65 & 62 & \\cdots & 70 \\\\\n",
    "100 & 140 & 110 & \\cdots & 180 \\\\\n",
    "\\, \\text{brown} & \\text{green} & \\text{brown} & \\cdots & \\text{blue}\\, \n",
    "\\end{bmatrix}\n",
    "\\, \\verb|--(one_hot_encode)-->|\\, \n",
    "\\begin{bmatrix}\n",
    "60 & 65 & 62 & \\cdots & 70 \\\\\n",
    "100 & 140 & 110 & \\cdots & 180 \\\\\n",
    "1 & 0 & 1 & \\cdots & 0 \\\\\n",
    "0 & 0 & 0 & \\cdots & 1 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For output features of $Y$, OHE is the preferred approach for softmax regression as each training example will corresponded to a prediction containing $N$ different probabilities where $N$ is the number of categories. This is the structure of $\\hat{Y}$. Consider $Y$ that corresponds with the $X$ example above containing height in inches, weight in pounds, and eye color. The model predicts hair color based on those inputs, selecting from the colors of black, brown, or blonde ($N=3$). A new feature is created for each possible category, which answers a binary question:\n",
    "  1. Is the hair color black?\n",
    "  2. Is the hair color brown?\n",
    "  3. Is the hair color blonde?\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "\\, \\text{black} & \\text{brown} & \\text{black} & \\cdots & \\text{blonde}\\, \n",
    "\\end{bmatrix}\n",
    "\\, \\verb|--(one_hot_encode)-->|\\, \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 & \\cdots & 0 \\\\\n",
    "0 & 1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & 0 & \\cdots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The one-hot encoded version of $Y$ is now directly comparable with the softmax predictions $\\hat{Y}$. This simplifies the cost calculation its corresponding derivatives.\n",
    "\n",
    "The benefit of OHE is that it allows most machine learning algorithms, such as those used in neural networks, to train on nominal categories. The drawback is the increase in dimensionality. For a large number of input categories, the number of rows in $X$ (features) increases rapidly. At a minimum, this requires a large input layer to account for these features of $X$, increeasing the size of $W[1]$ and $b[1]$. Likewise, many output categories increases the number of rows of $Y$, thus increasing the size of $W[L]$ and $b[L]$. Training such a model may be more computationally expensive and require more time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bc3db3-1ecb-4a74-b202-883d192801af",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ad8a3-5453-4cae-bf4f-eb15df2eaf9f",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This unsupervised machine learning algorithm is not directly related to neural networks. However, it is commonly used to visualize the input data $X$. When $X$ has many features, PCA can compress the data to reduce the number of features. Visualizing highly dimensional data in 2 or 3 dimensions helps engineers surface correlations and patterns. PCA's ability to reduce dimensionality also reduces the computing time and power required to train a neural network.\n",
    "\n",
    "In practice, PCA is more commonly used for visualization/exploration. It is less commonly used to speed up training as this reduction does cause some loss of granularity/variance, leading to worse predictions. Given the computing resources available in modern times, compressing the input data is less important than it was in years past.\n",
    "\n",
    "Some practitioners have suggested using PCA to reduce dimensionality when one-hot encoding (OHE) is in use. For example, applying OHE to a classification with 4 choices only yields 4 features. Apply the same logic to a country code may yield 200 features. This explosion of data may not have much predictive relevance for the model; reducing it with PCA might be worthwhile."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "326c7b86-a514-4552-8013-5750bbdf3839",
   "metadata": {},
   "source": [
    "#### Data Visualization\n",
    "Suppose we have this input data matrix $X$, which compares the height, shoulder width, hip circumference, weight, and body fat percentange for several human adults. As is typical with deep network input matrices, these features are listed along the rows. Lengths are measured in inches and weights are measured in pounds. Columns identify training examples. This is the standard $(n,m)$ structure used for deep learning.\n",
    "\n",
    "The final row identifies whether the person is a man (1)or not (0), which are labels of vector $y$. For simplicity, the first 10 are men and the last 10 are not men.\n",
    "\n",
    "|         |   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |  10 |  11 |  12 |  13 |  14 |  15 |  16 |  17 |  18 |  19 |  20 | \n",
    "| ------- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| Height  |  71 |  68 |  64 |  61 |  70 |  74 |  69 |  65 |  62 |  66 |  49 |  52 |  55 |  50 |  51 |  50 |  53 |  54 |  55 |  53 |\n",
    "| S width |  14 |  15 |  16 |  14 |  14 |  18 |  17 |  17 |  16 |  18 |  12 |  15 |  15 |  14 |  14 |  16 |  12 |  14 |  12 |  16 |\n",
    "| H circ  |  40 |  41 |  40 |  41 |  37 |  37 |  40 |  40 |  38 |  41 |  39 |  41 |  41 |  36 |  39 |  40 |  39 |  41 |  36 |  37 |\n",
    "| Weight  | 180 | 173 | 170 | 145 | 190 | 255 | 212 | 168 | 140 | 225 | 100 | 120 | 155 | 105 |  98 | 131 | 116 | 130 | 170 | 127 |\n",
    "| BFat %  |  13 |  16 |  12 |  12 |  13 |  25 |  11 |  10 |  14 |  18 |  23 |  23 |  29 |  31 |  22 |  25 |  29 |  31 |  30 |  22 |\n",
    "| IS MAN? |   1 |   1 |   1 |   1 |   1 |   1 |   1 |   1 |   1 |   1 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |   0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805b7a30-b983-4e9f-8e32-315b53538afd",
   "metadata": {},
   "source": [
    "The code below defines the input matrix $X$. Many of the features within scikit-learn (`sklearn`) require the rows of $X$ to represent training examples and the columns to represent features. Therefore, we first transpose $X$ to achieve a shape of $(m,n)$. Note that the label vector $y$ remains undefined as labels do not factor into PCA.\n",
    "\n",
    "You must first decide how many principal components (PCs) are required. Each PC is ranked by magnitude of explained variance. That is to say, PC1 will always explain the greatest amount of variance in the training data, with PC2 explaining the second greatest amount, etc. In data sets where correlation between features is high, a small number of PCs can capture most of the variance. In noisier data sets with weaker correlation between features, more PCs are required to capture most of the variance.\n",
    "\n",
    "In this example, PC1 explains nearly all of the variance with PC2 contributing a small amount. Together, these two PCs account for more than 99% of the variance. Although the data set $X$ had 5 unique features, PCA has reduced it to 2 features while still retaining the vast majority of the variance. This is because many of the features are highly correlated, such as height and weight, weight and body fat, etc. Some features, such as hip circumference (at least in this fabricated example) do not appear predictive. They likely correlated poorly to other features as the mean values are similar for \"men\" and \"not men\" labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3152ce7f-046c-4bca-a598-04bdb070f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 1 explained variance ratio: 96.6138%\n",
      "PCA 2 explained variance ratio: 2.8472%\n",
      "Total variance retained by PCA: 99.4611%\n"
     ]
    }
   ],
   "source": [
    "X = np.array([\n",
    "    [71, 68, 64, 61, 70, 74, 69, 65, 62, 66, 49, 52, 55, 50, 51, 50, 53, 54, 55, 53],\n",
    "    [14, 15, 16, 14, 14, 18, 17, 17, 16, 18, 12, 15, 15, 14, 14, 16, 12, 14, 12, 16],\n",
    "    [40, 41, 40, 41, 37, 37, 40, 40, 38, 41, 39, 41, 41, 36, 39, 40, 39, 41, 36, 37],\n",
    "    [180, 173, 170, 145, 190, 255, 212, 168, 140, 225, 100, 120, 155, 105, 98, 131, 116, 130, 170, 127],\n",
    "    [13, 16, 12, 12, 13, 25, 11, 10, 14, 18, 23, 23, 29, 31, 22, 25, 29, 31, 30, 22],\n",
    "])\n",
    "X = X.T\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X)\n",
    "for i, evr in enumerate(pca_2.explained_variance_ratio_):\n",
    "    print(f\"PCA {i+1} explained variance ratio: {(evr * 100):.4f}%\")\n",
    "print(f\"Total variance retained by PCA: {(np.sum(pca_2.explained_variance_ratio_) * 100):.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ac8ac-952a-4e55-ab2e-b14ec968cdef",
   "metadata": {},
   "source": [
    "To visualize the data, use a scatterplot. This one is enhanced with color-coding based on the expected label; again, labels have no influence over PCA. This helps emphasize the diagonal decision boundary separating \"men\" from \"not men\". It is also not common to annotate individual points with their example number. Doing so here helps further reinforce the visualization of the clusters.\n",
    "\n",
    "By definition, PC1 accounts for a greater share of the variance than PC2. The difference between the leftmost (15) and rightmost (6) values is about 160 along the PC1 axis. The difference between the topmost (19) and bottommost (8) is about 22 along the PC2 axis. If we completely ignored PC2 and only projected all the points onto a number line representing PC1, we still have a pretty good idea of how the data separates. However, adding the second dimension with the PC2 axis gives us a clearer picture. It also helps to draw a better decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "507ecaa2-5301-4349-9508-8425cd12b3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12a841060>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_pca_2 = pca_2.transform(X)\n",
    "pc1 = X_pca_2[:, 0]\n",
    "pc2 = X_pca_2[:, 1]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].scatter(pc1[:10], pc2[:10], color=\"b\", label=r\"$y=1$\")\n",
    "ax[0].scatter(pc1[10:], pc2[10:], color=\"r\", label=r\"$y=0$\")\n",
    "ax[0].set_xlabel(\"PC1\"); ax[0].set_ylabel(\"PC2\")\n",
    "ax[0].set_title(\"PC1 and PC2 in 2D\"); ax[0].legend()\n",
    "for i in range(X.shape[0]):\n",
    "    ax[0].annotate(i+1, (pc1[i], pc2[i]))\n",
    "\n",
    "ax[1].scatter(pc1[:10], np.zeros(10), color=\"b\", label=r\"$y=1$\")\n",
    "ax[1].scatter(pc1[10:], np.zeros(10), color=\"r\", label=r\"$y=0$\")\n",
    "ax[1].set_xlabel(\"PC1\"); ax[1].get_yaxis().set_visible(False)\n",
    "ax[1].set_title(\"PC1 only in 1D\"); ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ffaf67-31fb-4527-9324-5cbea5192b5b",
   "metadata": {},
   "source": [
    "#### Simplified Supervised Training\n",
    "The `transform()` function performs dimensionality reduction after the PCs have been discovered by the `fit()` function. The code below compares a logistic regression model trained on all features of $X$, versus another logistic regression model trained on the PCA-reduced dimensions of $X$. Because the PCs represent more than 99% of the variance, we expect both models to perform about the same, with the PCA-trained model performing slightly worse. Whenever you compress data using PCA, you forfeit a bit of granularity for a significant decrease in dimensionality (at least in theory).\n",
    "\n",
    "There are 4 example queries shown below. The first 2 are \"men\" and the second 2 are \"not men\". When making predictions with a PCA-trained model, one must transform the input query (containing all 5 features) into a PCA-compatible query containing only 2 features represented by PCs. Behind the scenes, PCA applies Z-score normalization (standardization) to the data; this is also required at prediction time, hence the call to `transform()`. In this simple test, both models get all 4 predictions correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d633c8-a6ca-4ecf-b617-60d5a0ef001a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full X shape: (20, 5)\n",
      "[[70, 15, 42, 195, 16]] True\n",
      "[[67, 14, 39, 180, 13]] True\n",
      "[[56, 14, 39, 110, 20]] False\n",
      "[[60, 12, 45, 130, 22]] False\n",
      "\n",
      "PCA2 X shape: (20, 2)\n",
      "[[70, 15, 42, 195, 16]] True\n",
      "[[67, 14, 39, 180, 13]] True\n",
      "[[56, 14, 39, 110, 20]] False\n",
      "[[60, 12, 45, 130, 22]] False\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "queries = [\n",
    "    [[70, 15, 42, 195, 16]], # is a man, y=1\n",
    "    [[67, 14, 39, 180, 13]], # is a man, y=1\n",
    "    [[56, 14, 39, 110, 20]], # not a man, y=0\n",
    "    [[60, 12, 45, 130, 22]], # not a man, y=0\n",
    "]\n",
    "y = np.concatenate((np.ones(10), np.zeros(10)))\n",
    "\n",
    "# training with full-featured X\n",
    "full_logreg = linear_model.LogisticRegression()\n",
    "print(\"Full X shape:\", X.shape)\n",
    "full_logreg.fit(X, y)\n",
    "for query in queries:\n",
    "    print(query, bool(full_logreg.predict(query)[0]))\n",
    "\n",
    "# training with X after transform with 2 PCAs\n",
    "pca_logreg = linear_model.LogisticRegression()\n",
    "print(\"\\nPCA2 X shape:\", X_pca_2.shape)\n",
    "pca_logreg.fit(X_pca_2, y)\n",
    "for query in queries:\n",
    "    # must transform query based on PCA values (mean, etc).\n",
    "    pca_2_query = pca_2.transform(query)\n",
    "    print(query, bool(pca_logreg.predict(pca_2_query)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36797ac8-010e-4c33-9112-0ac4d4b474d0",
   "metadata": {},
   "source": [
    "#### Limitations\n",
    "PCA has several limitations. Because the purpose of PCA is to find correlations between sets of features, no/weak correlation makes PCA ineffective. The PCs would only account for small amounts of the variance.\n",
    "\n",
    "The PCs are linear combinations of numbers which makes it hard to determine which original features contribute most to a given PC. Due to PCA's linear nature, it is not effective at measuring non-linear relationships between features. Consider scaling your data using exponential (logarithmic curve to line) or logarithmic (exponential curve to line) transforms to make it linear before running PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d02189-e973-4aca-a3dd-e7f10737dc59",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cc50ab-e3b0-4298-bd23-8f135cb53237",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes normalizers used for feature or neural network layer scaling. For all methods, be sure to apply normalization when making predictions using the same computed values (such as $\\mu$, $\\sigma$, and various minimums/maximums) to the new input. Normalization is not applied to categorical features of $X$ or labels of $\\hat{Y}$.\n",
    "\n",
    "The main benefit is that the ranges between values of a feature, and between features, will shrink. This allows you to use a larger learning rate $\\alpha$ in gradient descent, reducing both time and computational energy required to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396b682-3fa0-41dd-a527-b39618c7ea43",
   "metadata": {},
   "source": [
    "#### Z-score\n",
    "Given $\\vec{x}$, this method subtracts the mean $\\mu$ then divides by the standard deviation $\\sigma$ (not related to sigmoid activation function). This is most commonly used for general features, although may perform poorly with some extreme outliers. The mean is 0 and the standard deviation is 1. Positive numbers represent values above the mean while negative numbers represent values below the mean. It is also called \"standardization\" or \"the standard scaler\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu = & \\frac{1}{n} \\cdot \\sum\\limits^n_{i=1}x_i \\\\\n",
    "\\sigma = & \\sqrt{\\frac{1}{n} \\cdot \\sum\\limits^n_{i=1}{(x_i)}^2} \\\\\n",
    "\\vec{x}' = & \\frac{\\vec{x}-\\mu}{\\sigma}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d53d95d-125f-47a9-8621-845a84e11690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12cb65ea0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def zscore_norm(xvec):\n",
    "    return (xvec - np.mean(xvec)) / np.std(xvec)\n",
    "\n",
    "x_1 = np.random.randint(0, 1000, (20,))\n",
    "x_2 = np.random.randint(0, 10, (20,))\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_title(\"No norm\")\n",
    "ax[0].plot(x_1, x_2, linestyle=\"\",marker=\"o\")\n",
    "\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_title(\"Z-score norm\")\n",
    "ax[1].plot(zscore_norm(x_1), zscore_norm(x_2), linestyle=\"\", marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d785d-05b9-4daa-bc52-2a78de4e3366",
   "metadata": {},
   "source": [
    "#### Clipping\n",
    "Given $\\vec{x}$ plus user-defined minimum and maximum thresholds, this method assigns extremely large outliers to the maximum and extremely small outliers to the minimum. Unlike the other methods, this only modifies a subset of values and may have no effect if the dataset lacks outliers.\n",
    "\n",
    "$$\n",
    "\\vec{x}' =\n",
    "\\begin{cases}\n",
    "x > \\text{user}_{max}: & x := \\text{user}_{max} \\\\\n",
    "x < \\text{user}_{min}: & x := \\text{user}_{min}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9586555-fea1-4c5b-b4c7-9d83106fd780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12cc74bb0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clip_norm(xvec, umin, umax):\n",
    "    xnew = np.copy(xvec)\n",
    "    xnew[xvec > umax] = umax\n",
    "    xnew[xvec < umin] = umin\n",
    "    return xnew\n",
    "\n",
    "x_1 = np.random.randint(40, 50, (18,))\n",
    "x_1 = np.append(x_1, 20)\n",
    "x_1 = np.append(x_1, 70)\n",
    "x_2 = np.random.randint(5, 10, (18,))\n",
    "x_2 = np.append(x_2, 1)\n",
    "x_2 = np.append(x_2, 15)\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_title(\"No norm\")\n",
    "ax[0].plot(x_1, x_2, linestyle=\"\",marker=\"o\")\n",
    "\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_title(\"Clip norm\")\n",
    "ax[1].plot(clip_norm(x_1, 37, 55), clip_norm(x_2, 4, 11), linestyle=\"\",marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea86ed8-7aa7-4b90-8aa4-7a0cf039de90",
   "metadata": {},
   "source": [
    "#### Mean\n",
    "Given $\\vec{x}$, this method subtracts the mean $\\mu$, then divides by the difference between the largest and smallest values within $\\vec{x}$. The values always sum to 0 with a maximum span of 1 between smallest and largest values. In other words, ${x_{max}-x_{min}} = 1$ after normalization. Because the values cluster near the origin, they are all relatively small.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu = & \\frac{1}{n} \\cdot \\sum\\limits^n_{i=1}x_i \\\\\n",
    "\\vec{x}' = & \\frac{\\vec{x}-\\mu}{x_{max}-x_{min}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9683b35e-a42a-477d-98bd-c24be2406df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12cd61d20>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mean_norm(xvec):\n",
    "    return (xvec - np.mean(xvec)) / (np.max(xvec) - np.min(xvec))\n",
    "\n",
    "x_1 = np.random.randint(0, 100000, (20,))\n",
    "x_2 = np.random.randint(0, 10, (20,))\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_title(\"No norm\")\n",
    "ax[0].plot(x_1, x_2, linestyle=\"\",marker=\"o\")\n",
    "\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_title(\"Mean norm\")\n",
    "ax[1].plot(mean_norm(x_1), mean_norm(x_2), linestyle=\"\",marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c9394-8697-4726-af5a-28e9483bfcb6",
   "metadata": {},
   "source": [
    "#### Linear\n",
    "Given $\\vec{x}$, this method subtracts the smallest value within $\\vec{x}$, then divides by the difference between the largest and smallest values within $\\vec{x}$. It works when the feature is uniformly distributed (evenly spaced) across a range. Results are guaranteed to be between 0 and 1. It is sometimes called \"min max\" normalization as well.\n",
    "\n",
    "$$\n",
    "\\vec{x}' = \\frac{\\vec{x}-x_{min}}{x_{max}-x_{min}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbfa2207-fc73-49c8-907b-0486b67bc032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x12cd91300>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def linear_norm(xvec):\n",
    "    return (xvec - np.min(xvec)) / (np.max(xvec) - np.min(xvec))\n",
    "\n",
    "x_1 = np.random.randint(0, 1000, (20,))\n",
    "x_2 = np.random.randint(0, 10, (20,))\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_title(\"No norm\")\n",
    "ax[0].plot(x_1, x_2, linestyle=\"\",marker=\"o\")\n",
    "\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_title(\"Linear norm\")\n",
    "ax[1].plot(linear_norm(x_1), linear_norm(x_2), linestyle=\"\",marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50abb3-d8d9-46b4-839f-2244a58fc089",
   "metadata": {},
   "source": [
    "#### Logarithmic\n",
    "Given $\\vec{x}$ and a user-specific log base $b$, this method applies a logarithm to \"power law\" feature values (ie, it looks like $f(x) = \\frac{1}{x}$ on a plot). The result is data better suited to linear arithmetic. Cannot be used with input value of 0 unless this case is explicitly handled in the implementation.\n",
    "\n",
    "$$\n",
    "\\vec{x}' = \\log_b(\\vec{x})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252a2daf-0a34-42e7-9701-d458e2d87afc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_xlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$x_1$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogarithmic norm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m ax[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlog_norm\u001b[49m(x_1), logarithmic_norm(x_2), linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,marker\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log_norm' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logarithmic_norm(xvec, base=10):\n",
    "    return np.emath.logn(base, xvec)\n",
    "\n",
    "x_1 = np.random.randint(1, 1000, (20,))\n",
    "x_2 = np.random.randint(1, 10, (20,))\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"$x_2$\")\n",
    "ax[0].set_xlabel(\"$x_1$\")\n",
    "ax[0].set_title(\"No norm\")\n",
    "ax[0].plot(x_1, x_2, linestyle=\"\",marker=\"o\")\n",
    "\n",
    "ax[1].set_ylabel(\"$x_2$\")\n",
    "ax[1].set_xlabel(\"$x_1$\")\n",
    "ax[1].set_title(\"Logarithmic norm\")\n",
    "ax[1].plot(log_norm(x_1), logarithmic_norm(x_2), linestyle=\"\",marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172efe8f-fd03-4543-85d5-ac11b62ed723",
   "metadata": {},
   "source": [
    "#### Batch\n",
    "Just as with $X$, layered outputs within a neural network can be normalized. It is possible to apply this normalization to the linear output $Z^{[l]}$ or the activation output $A^{[l]}$, although it is more commonly applied to $Z^{[l]}$. You could replace all instances of $Z^{[l]}$ with $A^{[l]}$ in these equations to batch normalize the results after activation.\n",
    "\n",
    "For every feature $_j$ of $Z^{[l]}$ (which corresponds to a unit in that layer), compute the mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mu & = \\frac{1}{m} \\cdot \\sum\\limits_{j=1}^m Z^{[l]}_j \\\\\n",
    "\\sigma^2 & = \\frac{1}{m} \\cdot \\sum\\limits_{j=1}^m (Z^{[l]}_j - \\mu)^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Then, normalize $Z^{[l]}_j$ to yield $Z^{[l]norm}_j$ which has a mean of 0 in a variance of 1 for each feature. The presence of $\\varepsilon$ is for numeric stability and has an infinitesimal impact on the mean and variance just described. Two additional per-layer parameters, $\\gamma^{[l]}$ and $\\beta^{[l]}$, are introduced to further transform the result. To retain a mean of zero, in a variance of one, use $\\gamma^{[l]}=1$ and $\\beta^{[l]}=0$. To nullify the batch normalization process entirely, use $\\gamma^{[l]}=\\sqrt{\\sigma^2 + \\varepsilon}$ and $\\beta^{[l]}=\\mu$. Like the weights $W^{[l]}$ and biases $b^{[l]}$, $\\gamma^{[l]}$ and $\\beta^{[l]}$ are learnable. Finally, all of $\\tilde{Z}^{[l]}$ becomes an input to the activation function $g()$ and the process continues as usual.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l]norm}_j & = \\frac{Z^{[l]}_j - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} \\\\\n",
    "\\tilde{Z}^{[l]}_j & = (\\gamma^{[l]} \\cdot Z^{[l]norm}_j) + \\beta^{[l]} \\\\\n",
    "A^{[l+1]} & = g(\\tilde{Z}^{[l]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that bias $b^{[l]}$ becomes irrelevant with batch normalization because the mean $\\mu$ is set to 0. Instead, $\\beta^{[l]}$ serves as the per-unit offset. You can either set all biases to 0 or remove it from your algorithm entirely. Like the bias matrix $b^{[l]}$, both $\\gamma^{[l]}$ and $\\beta^{[l]}$ are applied unit-wise and thus have a shape of $(n_l, 1)$. Additionally, batch normalization has a slight regularizing effect when combined with mini-batch gradient descent. Due to the scaling by the mean $\\mu$ and variance $\\sigma^2$ of a mini-batch (which doesn't represent all of $X$), the precise activation values become noisier. The smaller the mini-batch size, the greater the regularization effect.\n",
    "\n",
    "To make predictions when mini-batches are use, we must estimate $\\mu$ and $\\sigma^2$ since the values computed for a single mini-batch cannot be used at test time. Instead, use an EWMA across the $\\mu$ and $\\sigma^2$ values for each mini-batch. These are non-learnable parameters of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8642c9-4584-4c5b-b317-595a624ee3d2",
   "metadata": {},
   "source": [
    "### Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d76bd-8742-4644-8e24-2a6430f4cb31",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Proper initialization of weights is necessary to prevent exploding and vanishing gradient problems in deep neural networks (those with many layers). Derivatives used during back propagation can become too large (exploding), which causes divergence, if the weights are initialized too large. They can also become too small (vanishing), which causes an inability to converge, if the weights are initialized too small. Initializing the weights with values slightly less than 1 and slightly greater than 1 mitigates the problem, although does not completely solve it.\n",
    "\n",
    "Additionally, weight initialization avoids the \"symmetry\" problem whereby by every units learns the same thing, reducing the entire neural network to a single unit. Such a model is only as powerful as linear or logistic regression in isolation. Note that you can also initialize biases the same way. It is common to initialize biases to zero as this will not inhibit a model's ability to learn.\n",
    "\n",
    "In the formulas that follow, $\\mathcal{N}$ refers to a normal (Gaussian) distribution of random values matching the proper matrix shape at a given layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d138fb4f-1458-4e07-879f-86ea88b12efd",
   "metadata": {},
   "source": [
    "#### Xavier\n",
    "This method works best on layers using the linear, sigmoid, tanh, or softmax activation functions. As $n_{l-1}$ increases (which represents a more populous previous layer), the resulting vector values creep closer to zero.\n",
    "\n",
    "$$\n",
    "W^{[l]}_{init} = \\mathcal{N} \\cdot \\sqrt{\\frac{1}{n_{l-1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42409bb6-524a-4b75-bbd5-2db7f086e0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.random.randn(2, 5)\n",
    "print(f\"norm distro\\n{norm}\")\n",
    "for prev_n in np.array([10, 20, 50]):\n",
    "    print(f\"\\nn_l-1 = {prev_n}\\n{norm * np.sqrt(1/prev_n)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd681cc0-5ee8-4f6a-bfa0-8dc383bac814",
   "metadata": {},
   "source": [
    "#### Normalized Xavier\n",
    "This method is similar to Xavier except it considers both the input and output unit counts (sometimes called \"fan in\" and \"fan out\") between layers. If these two terms are equal, it yields identical results as Xavier. As the $(n_{l-1} + n_l)$ denominator term increases, the resulting vector values creep closer to zero.\n",
    "\n",
    "$$\n",
    "W^{[l]}_{init} = \\mathcal{N} \\cdot \\sqrt{\\frac{2}{n_{l-1} + n_l}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3701dd-827b-484e-ac94-0fb7bd7640e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.random.randn(2, 5)\n",
    "cur_n = 20\n",
    "print(f\"norm distro\\n{norm}\")\n",
    "for prev_n in np.array([10, 20, 50]):\n",
    "    print(f\"\\nn_l-1 = {prev_n}, n_l = {cur_n}\\n{norm * np.sqrt(2/(prev_n + cur_n))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b45e50-f7df-4a4d-b4b1-5dd2351532bd",
   "metadata": {},
   "source": [
    "#### He \n",
    "This method works best on layers using the ReLU activation function (and its variants). It is very similar to Xavier except uses 2 as the numerator instead of 1. This yields slightly smaller movements towards zero when compared to Xavier given the larger numerator.\n",
    "\n",
    "$$\n",
    "W^{[l]}_{init} = \\mathcal{N} \\cdot \\sqrt{\\frac{2}{n_{l-1}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d1c979-61d2-463c-a8ab-0026f10637d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = np.random.randn(2, 5)\n",
    "print(f\"norm distro\\n{norm}\")\n",
    "for prev_n in np.array([10, 20, 50]):\n",
    "    print(f\"\\nn_l-1 = {prev_n}\\n{norm * np.sqrt(2/prev_n)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be2594-8627-4637-872b-b7229afed02b",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c27da69-c53a-4bfe-9955-55ab0703cbed",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Overview\n",
    "This section describes activation functions applied to the linear outputs $Z$ to yield $A$, plus their derivatives. Note that the following generic, non-annotated, non-vectorized formula is true for all activations. The superscripts and subscripts may vary as the context changes. This equation is not repeated for individual activation summaries below.\n",
    "\n",
    "$$\n",
    "z = w \\cdot x + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412ad7c-23c8-458c-8581-105d6a11f142",
   "metadata": {},
   "source": [
    "#### Linear/None\n",
    "Returns the input $z$ unchanged. Not useful in a hidden layer, but could be used at the final layer for linear regression.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) & = z \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} & = 1\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57416262-7fbe-4b74-b4b5-21c355781a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_linear(z):\n",
    "    return z\n",
    "\n",
    "def dg_linear(z):\n",
    "    return np.ones(z.shape)\n",
    "\n",
    "z_in = np.linspace(-5, 5, 100)\n",
    "plt.plot(z_in, g_linear(z_in), color=\"b\", label=\"g(z)\")\n",
    "plt.plot(z_in, dg_linear(z_in), color=\"r\", label=\"dg(z)\")\n",
    "plt.xlabel(\"z\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d26e2-0fe6-4a7e-bfe2-d3b42b79f1c5",
   "metadata": {},
   "source": [
    "#### Sigmoid\n",
    "Returns values between 0 and 1 with most values being close to those two asymptotes. This value corresponds to the probability that $y = 1$. Typically used in the final layer for logistic regression (binary classification).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) & = \\frac{1}{1+e^{-z}} \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} & = g(z) \\cdot (1 - g(z))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9834a3-feee-4e4e-a20e-d7d23839159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def dg_sigmoid(z):\n",
    "    gz = g_sigmoid(z)\n",
    "    return np.multiply(gz, 1 - gz)\n",
    "\n",
    "z_in = np.linspace(-5, 5, 100)\n",
    "plt.plot(z_in, g_sigmoid(z_in), color=\"b\", label=\"g(z)\")\n",
    "plt.plot(z_in, dg_sigmoid(z_in), color=\"r\", label=\"dg(z)\")\n",
    "plt.xlabel(\"z\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdd20d-de7a-47d3-8d89-9c2f7829a094",
   "metadata": {},
   "source": [
    "#### Rectified Linear Unit (ReLU)\n",
    "Like the linear activation, returns the input $z$ unchanged when $z$ is positive. When $z$ is negative, returns 0. The function is technically undefined when $z = 0$, but most implementations will return 0 for simplicity.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) = &\n",
    "\\begin{cases}\n",
    "z \\lt 0: & 0 \\\\\n",
    "z \\ge 0: & z\n",
    "\\end{cases} \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} = &\n",
    "\\begin{cases}\n",
    "z \\lt 0: & 0 \\\\\n",
    "z \\ge 0: & 1\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9c8bdb-2857-4f67-bbd5-c8e40f0ed476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_relu(z):\n",
    "    return z * (z > 0)\n",
    "\n",
    "def dg_relu(z):\n",
    "    return 1.0 * (z > 0)\n",
    "\n",
    "z_in = np.linspace(-5, 5, 100)\n",
    "plt.plot(z_in, g_relu(z_in), color=\"b\", label=\"g(z)\")\n",
    "plt.plot(z_in, dg_relu(z_in), color=\"r\", label=\"dg(z)\")\n",
    "plt.xlabel(\"z\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82932718-bc85-4ac8-808d-deac82d26f03",
   "metadata": {},
   "source": [
    "#### Leaky ReLU\n",
    "Like the ReLU activation, except when $z$ is negative, returns a small positive number. This allows $z$ values to be preserved but heavily discounted.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) = &\n",
    "\\begin{cases}\n",
    "z \\lt 0: & 0.01z \\\\\n",
    "z \\ge 0: & z\n",
    "\\end{cases} \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} = &\n",
    "\\begin{cases}\n",
    "z \\lt 0: & 0.01 \\\\\n",
    "z \\ge 0: & 1\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d80014-630e-45aa-bd1d-222b30093821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Large neg_slope used for visual aid only\n",
    "\n",
    "def g_leakyrelu(z, neg_slope=0.2):\n",
    "    return np.where(z > 0, z, neg_slope * z)\n",
    "\n",
    "def dg_leakyrelu(z, neg_slope=0.2):\n",
    "    return np.where(z > 0, 1, neg_slope)\n",
    "\n",
    "z_in = np.linspace(-5, 5, 100)\n",
    "plt.plot(z_in, g_leakyrelu(z_in), color=\"b\", label=\"g(z)\")\n",
    "plt.plot(z_in, dg_leakyrelu(z_in), color=\"r\", label=\"dg(z)\")\n",
    "plt.xlabel(\"z\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b37374-dded-49a4-8549-e28f3c87b0eb",
   "metadata": {},
   "source": [
    "#### Hyperbolic tangent (tanh)\n",
    "Similar in shape to the sigmoid function, it returns values between -1 and 1. The theoretical mean of the activation results is closer to 0, compared to 0.5 with sigmoid.\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z) & = \\frac{e^z - e^{-z}}{{e^z} + e^{-z}} \\\\\n",
    "\\frac{\\partial g(z)}{\\partial z} & = 1 - g(z)^{2}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ad45e1-3f42-4be3-a1fc-eb8407b6e711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x12e391510>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def g_tanh(z):\n",
    "    epz = np.exp(z)\n",
    "    enz = np.exp(-z)\n",
    "    return (epz - enz)/(epz + enz)\n",
    "\n",
    "def dg_tanh(z):\n",
    "    return 1 - np.square(g_tanh(z))\n",
    "\n",
    "z_in = np.linspace(-4, 4, 10000)\n",
    "plt.plot(z_in, g_tanh(z_in), color=\"b\", label=\"g(z)\")\n",
    "plt.plot(z_in, dg_tanh(z_in), color=\"r\", label=\"dg(z)\")\n",
    "plt.xlabel(\"z\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3aa55-d036-4919-9e0e-0c5e6ce9d088",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "Across $k$ units (classes) in a layer, returns values between 0 and 1 corresponding to the probability of a match for each class. The sum of all activation values is one. Typically used in the final layer for softmax regression (multiple class classification). Cannot be easily visualized as it typically has many dimensions.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "g(z_{j}) & = \\frac{e^{z_{j}}}{\\sum^{n}_{k=1}{e^{z_{k}}}} = P(y = j|\\vec{x}) \\\\\n",
    "\\frac{\\partial g(z_{j})}{\\partial z_{j}} & = g(z_{j}) \\cdot (1 - g(z_{j})) \\\\\n",
    "\\frac{\\partial g(z_{j})}{\\partial z_{k}} & = -g(z_{j}) \\cdot g(z_{k})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deea487-2d11-4f5c-872c-d6df20e846d6",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6236e57a-5eff-4bc2-9ca9-4db78e99a7fa",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes the criteria for identifying underfitting and overfitting models. Definitions:\n",
    "  1. $J_{bayes}$: theoretically lowest possible cost (Bayes error)\n",
    "  2. $J_{human}$: human baseline performance\n",
    "  3. $J_{train}$: total cost of training set $X_{train}$\n",
    "  4. $J_{cv}$: total cost of cross validation (aka development or dev) set $X_{cv}$\n",
    "\n",
    "Examples of $X$ should be shuffled between these three sets, especially if they come from different distributions. This may include different sources (real vs. synthetic, official database vs. social media, etc), different geographical regions, or any other differentiating factor. This is especially important with respect to the cross validation and test sets.\n",
    "\n",
    "Selection of a model is based on two types of metrics:\n",
    "  1. __Optimizing__: the single metric that should be improved to the greatest degree. For example, maximizing accuracy or minimizing false positives.\n",
    "  2. __Satisficing__: the remaining metrics that must be satisfied, but not necessarily optimized. For example, ensuring that predictions take less than 10 seconds or ensuring that the model fits into 200 megabytes of memory space.\n",
    "\n",
    "Given these metrics, use the __evaluation criteria__ to intelligently combine them. First, find all models that meet the satisficing metrics. Of those models, select the one with the best optimizing metric. Suppose that accuracy is the optimizing metric while prediction time and memory consumption are satisficing metrics. Following this process, model D is superior in the example table below.\n",
    "\n",
    "|                | Accuracy % | Pred time (s) | Mem con (MB) | Analysis           |\n",
    "| -------------- | ---------- | ------------- | ------------ | ------------------ |\n",
    "| Eval criteria  | $\\ge$ 95   | $\\le$ 10      | $\\le$ 200    |                    |\n",
    "| Model A        | 100        | 5             | 1500         | mem con too high   |\n",
    "| Model B        | 99         | 30            | 75           | pred time too high |\n",
    "| Model C        | 93         | 5             | 150          | accuracy too low   |\n",
    "| Model D        | 97         | 9             | 190          | best option        |\n",
    "\n",
    "To most rapidly improve accuracy, identify the biggest contributor to the error. For example, if 30% of incorrect predictions are due to blurry images, but only 10% are due to upside down images, you should focus on tuning the model to tolerate blurry images first. This isn't rigorously proven, but it provides a general direction for taking the first remedial steps. Make small changes in the chosen direction (e.g. improving burry image performance), see if it helps, then continue to iterate.\n",
    "\n",
    "Use the plots below to help visualize the concepts discussed in the following texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475b9cd0-2d5d-4138-beba-e53c016a0b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: imprecise numbers and curves; conceptual only\n",
    "error_in = np.linspace(0.1, 10.1, 100)\n",
    "desc = []\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"error - $J$\")\n",
    "\n",
    "ax[0].plot(error_in, np.ones(error_in.shape), color=\"b\", label=\"$J_{human}$\")\n",
    "ax[0].axvline(x=5, color=\"gray\", linestyle=\"--\"); ax[0].text(5.1,2,\"good fit\")\n",
    "ax[0].axvline(x=9.5, color=\"gray\", linestyle=\"--\"); ax[0].text(8.2,1.1,\"overfit\")\n",
    "ax[0].axvline(x=0.5, color=\"gray\", linestyle=\"--\"); ax[0].text(0.6,1.1,\"underfit\")\n",
    "\n",
    "ax[1].plot(error_in, np.ones(error_in.shape), color=\"y\", label=\"$J_{bayes}$\")\n",
    "ax[1].plot(error_in, np.ones(error_in.shape) + 0.4, color=\"b\", label=\"$J_{human}$\")\n",
    "\n",
    "for axis in ax:\n",
    "    axis.plot(error_in, -np.log(error_in) * 0.3 + 2, color=\"r\", label=\"$J_{train}$\")\n",
    "    axis.plot(error_in, 0.05 * np.square(error_in) - error_in/2 + 3, color=\"g\", label=\"$J_{cv}$\")\n",
    "    axis.set_xlabel(\"degree of polynomial/model complexity\")\n",
    "    axis.set_xticks([]); axis.set_yticks([]); axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef440f6-0861-4df1-a64f-a66508896817",
   "metadata": {},
   "source": [
    "#### Test for \"good\" fit\n",
    "If the errors of the training and cross validation sets are close to the human baseline, the model is a good fit. Both the bias and variance are low.\n",
    "\n",
    "$$\n",
    "J_{human} \\approx J_{train} \\approx J_{cv}\n",
    "$$\n",
    "\n",
    "Note that the human baseline error may still be significantly better than the training and cross validation errors. This is acceptable if the training and cross validation errors are tolerably low and do not suggest an underfit.\n",
    "\n",
    "$$\n",
    "J_{human} \\lesssim J_{train} \\lesssim J_{cv}\n",
    "$$\n",
    "\n",
    "It is sometimes possible to surpass human performance. The theoretical minimum cost is the Bayes error rate (also called Bayes optimal error) which can never be surpassed. However, once the model's performance reaches this point, progress becomes much slower. Determining what to change is difficult because there is no obvious baseline against which to compare.\n",
    "\n",
    "$$\n",
    "J_{human} \\gtrsim J_{train} \\gtrsim J_{cv}\n",
    "$$\n",
    "\n",
    "#### Test for underfit\n",
    "If the error of the training set is far higher than human baseline, and the error of the cross validation set is equally high, the model has underfitted to the training set. The model consistently makes poor predictions on any kind of input. Consider adding features, creating polynomial features to match more complex curves, or removing/decreasing regularization. It is not recommended to remove training examples at random. Also known as \"high bias\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{human} & \\ll J_{train} \\\\\n",
    "J_{train} & \\approx J_{cv}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that $J_{train} - J_{human}$ is called \"avoidable\" bias because it is theoretically possible to eliminate via training.\n",
    "\n",
    "#### Test for overfit\n",
    "If the error of the training set is close to the human baseline, but the error of the cross validation set is far higher than both, the model has overfitted to the training set. It will likely generalize poorly to unseen inputs and make inaccurate predictions. Consider adding training examples, removing features, or applying/increasing regularization. Also known as \"high variance\".\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{human} & \\approx J_{train} \\\\\n",
    "J_{train} & \\ll J_{cv}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You can approximate the variance by computing $J_{cv} - J_{train}$ which can theoretically be reduced, similar to avoidable bias.\n",
    "\n",
    "#### Test for \"worst\" fit\n",
    "If the error of the training set is far higher than human baseline, and the error of the cross validation set is far higher the training error, the model exhibits high bias and high variance simultaneously. This is a rare case that appears impossible to depict in two dimensions. Compute the avoidable bias and variance using the subtraction formulas above and first solve whichever is larger.\n",
    "\n",
    "$$\n",
    "J_{human} \\ll J_{train} \\ll J_{cv}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6e2c7c-f7f0-4cbf-8474-c51ef18872a0",
   "metadata": {},
   "source": [
    "#### Skewed Data Sets\n",
    "Maximizing accuracy (minimizing error) is a good measure of success for regression models and classification models with uniform/diverse data sets. However, it is a poor choice for assessing classification models with skewed data sets. A model can be highly accurate by simply predicting $\\hat{y}=1$ every single time if the data sete contains very few $y=0$ labels, or vice versa.\n",
    "\n",
    "A \"confusion matrix\" has shape $(k, k)$ whereby the $k$ is the number of classes. Assuming binary classification, the shape is $(2, 2)$. The rows and columns represent all possible values and the intersecting cells enumerate the permutations. Assign the results of each training example into one of these four categories. From this, compute accuracy $A$, precision $P$, and recall $R$.\n",
    "\n",
    "|             | $y=1$          | $y=0$          |\n",
    "| ----------- |----------------| -------------- |\n",
    "| $\\hat{y}=1$ | true positive  | false positive |\n",
    "| $\\hat{y}=0$ | false negative | true negative  |\n",
    "\n",
    "As an example, suppose this model predicts whether a patient has a rare and deadly disease.\n",
    "\n",
    "Accuracy $A$: for each training example, how often was the model correct $y=\\hat{y}$? Total the number of \"true\" values then divide by the total.\n",
    "\n",
    "$$\n",
    "A = \\frac{true\\_pos + true\\_neg}{true\\_pos + true\\_neg + false\\_pos + false\\_neg}\n",
    "$$\n",
    "\n",
    "Precision $P$: for each prediction of true $(\\hat{y}=1)$, how often did the model correctly diagnose the disease $(y=1)$? A high precision indicates that the model rarely misdiagnoses a healthy person; the number of false positives will be small.\n",
    "\n",
    "$$\n",
    "P = \\frac{true\\_pos}{true\\_pos + false\\_pos}\n",
    "$$\n",
    "\n",
    "Recall $R$ : for each patient with the disease $(y=1)$, how often did the model correct predict the disease's presence $(\\hat{y}=1)$? A high recall indicates that the model rarely forgets to include a sick person; the number of false negatives will be small.\n",
    "\n",
    "$$\n",
    "R = \\frac{true\\_pos}{true\\_pos + false\\_neg}\n",
    "$$\n",
    "\n",
    "Much like bias and variance, precision and recall must be traded off. In some cases, one metric is more important than the other. As it relates to a rare disease, recall is more important. This ensures the model does not fail to identify a sick patient (fewer false negatives), although this will likely lead to more healthy patients being misdiagnosed (more false positives). In the context of logistic regression, you would select a threshold less than 0.5, leading to a larger share of positive predictions $(\\hat{y}=1)$. A bank lending money to a borrower may prefer high precision. It is less important for the bank lend to all borrowers of interest, but more important that the selected borrowers are capable of repayment. The logistic regression threshold would be increased above 0.5 here.\n",
    "\n",
    "Another way to think about this is in the context of business/personal costs. Maximizing precision is important when the cost of action is high (lending money to a stranger), but the cost of inaction is low (plenty of other candidates exist). Maximizing recall is important when the cost of action is low (treatment for the disease), but the cost of inaction is high (diminished health or death).\n",
    "\n",
    "#### F-score\n",
    "This algorithm helps with the analysis and selection of models. The hyperparameter $\\beta \\in \\mathbb{R}\\,|\\,\\beta > 0$ controls the relative importance of precision and recall.\n",
    "\n",
    "$$\n",
    "F_\\beta = (1 + \\beta^2) \\cdot \\frac{P \\cdot R}{(\\beta^2 \\cdot P) + R}\n",
    "$$\n",
    "\n",
    "Although $\\beta$ can vary widely, three F-scores are most commonly used:\n",
    "  1. $F_1$-score: evenly balance the impact of precision and recall. Good for general-purpose use when there isn't an obvious preference. It provides greater weight to whichever value is lower, believing that a balanced model is better than a lobsided one.\n",
    "\n",
    "$$\n",
    "F_1 = 2 \\cdot \\frac{P \\cdot R}{P + R}\n",
    "$$\n",
    "\n",
    "  2. $F_2$-score: gives greater weight to recall over precision. Good for the rare disease example.\n",
    "\n",
    "$$\n",
    "F_2 = 5 \\cdot \\frac{P \\cdot R}{(4 \\cdot P) + R}\n",
    "$$\n",
    "\n",
    "  3. $F_{0.5}$-score: gives greater weight to precision over recall. Good for the bank lending example.\n",
    "\n",
    "$$\n",
    "F_{0.5} = 1.25 \\cdot \\frac{P \\cdot R}{(0.25 \\cdot P) + R}\n",
    "$$\n",
    "\n",
    "Consider four scenarios arrayed side by side, each representing a model trained with 100 examples.\n",
    "\n",
    "  1. This model performs well in both categories with a slightly higher recall than precision because there are more false positives than false negatives.\n",
    "  2. This model has good precision but poor recall given the small number of false positives and large number of false negatives.\n",
    "  3. This model has poor precision but good recall given the large number of false positives and small number of false negatives.\n",
    "  4. This model performs poorly by all measures with many false positives false negatives.\n",
    "\n",
    "| Scenario 1  | $y=1$        | $y=0$         | Scen 2  | $y=1$ | $y=0$  | Scen 3  | $y=1$ | $y=0$  | Scen 4  | $y=1$ | $y=0$  |\n",
    "| ----------- |--------------| ------------- | ------- |-------| ------ | ------- |-------| ------ | ------- |-------| ------ |\n",
    "| $\\hat{y}=1$ | true pos: 50 | false pos: 10 |         | 30    | 5      |         | 40    | 20     |         | 35    | 20     |\n",
    "| $\\hat{y}=0$ | false neg: 5 | true neg: 35  |         | 25    | 40     |         | 5     | 35     |         | 25    | 15     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5702a2ff-d020-4189-8be0-b4795b759b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = np.array([[50, 10, 5, 35], [30, 5, 25, 40], [40, 20, 5, 35], [35, 20, 25, 15]])\n",
    "desc = [\"good overall\", \"high false neg\", \"high false pos\", \"high fp & fn\"]\n",
    "beta = np.array([0.25, 0.5, 1.0, 2.0, 3.0])\n",
    "\n",
    "def accuracy(tp, fp, fn, tn): return (tp + tn) / (tp + tn + fp + fn)\n",
    "def precision(tp, fp): return tp / (tp + fp)\n",
    "def recall(tp, fn): return tp / (tp + fn)\n",
    "def fbeta_score(P, R, B): return (1 + B**2) * ((P * R) / ((B**2 * P) + R))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(scenario), sharey=True)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(r\"$F_{\\beta}$ score\")\n",
    "\n",
    "for i in range(len(scenario)):\n",
    "    A = accuracy(*scenario[i])\n",
    "    P = precision(scenario[i, 0], scenario[i, 1])\n",
    "    R = recall(scenario[i, 0], scenario[i, 2])\n",
    "    fscore = np.array([fbeta_score(P, R, B) for B in beta])\n",
    "    ax[i].set_title(f\"Scenario {i+1}: {desc[i]}\")\n",
    "    ax[i].plot(beta, fscore, marker=\"o\")\n",
    "    ax[i].axhline(y=A, color=\"orange\", linestyle=\"--\", label=\"accuracy\")\n",
    "    ax[i].axhline(y=P, color=\"green\", linestyle=\"--\", label=\"precision\")\n",
    "    ax[i].axhline(y=R, color=\"red\", linestyle=\"--\", label=\"recall\")\n",
    "    ax[i].set_xlabel(r\"$\\beta$\");\n",
    "\n",
    "ax[0].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a0251-d9be-43fa-8234-2b9a18cc95b2",
   "metadata": {},
   "source": [
    "### Model Recycling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b049a-b987-41c6-abea-06ff5dfbef24",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes solving multiple problems using some of all of an existing model, ultimately reducing the total training effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd4cb19-a3cb-4841-9f40-b2ba65b2cecd",
   "metadata": {},
   "source": [
    "#### Transfer Learning\n",
    "This technique involves first training a model for a generic purpose, such as image recognition, speech transcription, or text generation. Later, perhaps someone introduces a more specific use case for each of these categories, such radiology (image), translation to a specific language (speech), or composing Python code (text). One can \"transfer\" all of the existing layers, excluding the output layer, to a new model. Technically, it transfers the trained parameters, which required time and computing power to learn. In this way, the new model leverages all of the effort already invested. This process is called __pre-training__ and the addition of one or more layers to the end of the model is called __fine-tuning__.\n",
    "\n",
    "Note that the pre-trained model may be effective in its own right, but a finely-tuned model is specialized for a single use case. Such a model can be trained with a relatively small data set. For example, there are many images on the Internet. The total number of radiological images is far fewer, allowing the radiology use case to consume the generic image recognition techniques within the pre-trained layers of the model. In addition to reducing the training time and the burden of collecting training data, transfer learning also enables entirely new use cases that would otherwise be difficult or impossible to implement.\n",
    "\n",
    "The last layers of a neural network are considered at the \"top\" in some frameworks, which are typically retrained, if not entirely replaced or augmented. In general, the less training data available for fine-tuning, the more topmost layers should be retrained. If you have a sizable amount of data for fine-tuning, you may want to retrain a handful of the topmost layers (say, the topmost 5 layers). The pre-trained parameters can serve as good initial values for those final layers. This retraining technique is effective because it enables the high-level features to be retrained (such as facial expressions) while retaining the low-level (such as lines, corners, and curves) seen earlier in the model's architecture. Note that transfer learning is still useful even if you have an enormous training set. You can start from a known-good model, using the trained parameters as initial values, then retraining everything.\n",
    "\n",
    "As an implementation detail, layers that are untrainable effectively turn into a static function that gives a consistent results for any given input. After all, their weights, biases, activations, and hyperparameters never change. Sometimes it can be effective to process all of training examples through the untrainable layers of the model and write those activations at the disk. Suppose that the model has 22 layers with the first 20 being untrainable. The activations of layer 20 $(A^{[20]})$ are stored on disk and serve as inputs to layer 21, the first trainable layer. This can further reduce the computing power required.\n",
    "\n",
    "Also note that the prediction matrix shape $\\hat{Y}$ of the model may also change. For example, consider a generic model with a single sigmoid-activated unit in the output layer (mimicking logistic regression) that determines whether a patient has any kind of cancer $y=1$ or not $y=0$ given a tumor image. This model could be finely-tuned into a softmax regression layer that offers probabilities on the 5 different stages of lung cancer given a tumor image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844af9f0-0362-4f3d-9575-d481945cb1da",
   "metadata": {},
   "source": [
    "#### Multi-task Learning\n",
    "In contrast to single task learning, multi-task learning creates a single model to solve multiple problems at the same time. For example, if you are developing an image recognition model to determine if the image contains cars, people, and buildings, the shape of the prediction matrix $\\hat{Y}$ would be $(3,m)$ where $m$ the number of training examples. Each row of this matrix represents the yes $y=1$ or no $y=0$ answer to the 3 aforementioned questions, effectively combining 3 otherwise separate binary classification problems. Note that this is unlike softmax regression as these 3 results are not dependent upon one another.\n",
    "\n",
    "It is also possible to create 3 separate single-task models, each yielding a prediction matrix $\\hat{Y}$ of shape $(1,m)$ to classify the 3 attributes independently. This is a reasonable solution and has the benefit of decoupling the problem set. However, the models would need to be independently trained, despite having nearly identical images as inputs, which requires more time and/or computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f95752-064c-438f-a2d1-e06ed8c6d867",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a363b85e-805a-4a71-8de5-4f6c26315523",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section contains techniques to regularize (desensitize) an overfitted model. Regularization reduces the impact of the weights applied to each feature, sometimes entirely eliminating features. Note that this only applies to training, not predictions.\n",
    "\n",
    "Use this conceptual plot to see how $\\lambda$ impacts the quality of a model's fit to various data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc5c714-3fd4-41ac-977b-9f33af2f8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: imprecise numbers and curves; conceptual only\n",
    "error_in = np.linspace(0.1, 10.1, 100)\n",
    "plt.plot(error_in, np.ones(error_in.shape), color=\"b\", label=\"$J_{human}$\")\n",
    "plt.plot(error_in, np.exp(error_in/5)/5 + 1, color=\"r\", label=\"$J_{train}$\")\n",
    "plt.plot(error_in, 0.05 * np.square(error_in) - error_in/2 + 3, color=\"g\", label=\"$J_{cv}$\")\n",
    "plt.axvline(x=5, color=\"gray\", linestyle=\"--\"); plt.text(5.1,2,\"good fit\")\n",
    "plt.axvline(9.5, color=\"gray\", linestyle=\"--\"); plt.text(8.2,1.1,\"underfit\")\n",
    "plt.axvline(0.5, color=\"gray\", linestyle=\"--\"); plt.text(0.6,1.1,\"overfit\")\n",
    "plt.xticks([]); plt.yticks([])\n",
    "plt.xlabel(\"regularization hyperparameter $\\lambda$\"); plt.ylabel(\"error\"); plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6cfd4-ca93-4375-b278-765278f47b15",
   "metadata": {},
   "source": [
    "#### LASSO (L1 norm)\n",
    "Short for Least Absolute Shrinkage and Selection Operator. It sums the absolute values of the weights and can perform automatic feature selection by applying a weight of 0 to certain features. Use this method when there are many useless/nonpredictive features.\n",
    "\n",
    "General definition:\n",
    "\n",
    "$$\n",
    "\\|W\\|_1=\\lambda \\cdot {\\sum\\limits^{n}_{i=1}{|W_{n}|}}\n",
    "$$\n",
    "\n",
    "Adaptation to combine with a given cost function, and its derivative with respect to a specific weight $w_j$, as the \"regularization term\". Note that $|w_j|$ cannot be differentiated (and thus cannot be represented in closed-form) as the function is not continuous.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & := J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\biggl(\\frac{1}{m} \\cdot \\|\\vec{w}\\|_1\\biggr) \\\\\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & := J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\biggl(\\frac{\\lambda}{m} \\cdot {\\sum\\limits^{n}_{i=1}{|\\vec{w}_{n}|}}\\biggr) \\\\\n",
    "\\frac{\\partial}{\\partial w_j}J_{\\vec{w},b}(\\vec{x}^{(i)}) & := \\frac{\\partial}{\\partial w_j}J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\frac{\\lambda}{m} \\cdot\n",
    "\\begin{cases}\n",
    "w_j \\neq 0: & \\frac{w_j}{|w_j|} \\\\\n",
    "w_j = 0: & \\text{undefined}\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In both plots below, there is a steep change in shape when $w=0$ for both the cost function and derivative. This abrupt change becomes more prominent as $\\lambda$ increases. It drives the weights down to 0, eliminating their impact on the overall cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959f9f33-e19a-41fd-ba0b-65ccd1c37626",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 1, 3, 5]\n",
    "size = 101\n",
    "w = np.linspace(-2, 3, size)\n",
    "\n",
    "# let y=1, x=1, b=0 for simplicity, J(w) = mean squared error\n",
    "x = np.ones(size); y = np.ones(size); b = np.zeros(size)\n",
    "def J(w, b, x, y, lambd):\n",
    "    return ((w * x - y + b) ** 2) + (lambd * np.abs(w))\n",
    "\n",
    "def dJ_dw(w, b, x, y, lambd):\n",
    "    sign = np.sign(w)\n",
    "    sign[sign == 0.] = np.nan\n",
    "    return (w * x - y + b) * 2 + (lambd * sign)\n",
    "\n",
    "params = [\n",
    "    (J, \"L1 Regularization\", \"J(w)\"),\n",
    "    (dJ_dw, \"L1 Regularization Derivative\", \"$\\partial J(w)\\,/\\,\\partial w$\"),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, param in zip(ax, params):\n",
    "    func, title, ylabel = param\n",
    "    for lambd in lambdas:\n",
    "        axis.plot(w, func(w, b, x, y, lambd), label=f\"$\\lambda$={lambd}\")\n",
    "    axis.set_title(title)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.set_xlabel(\"w\")\n",
    "    axis.legend()\n",
    "    \n",
    "ax[1].axhline(y=0, color=\"gray\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18275a4f-ed02-4224-8d0c-ef35d36e52e1",
   "metadata": {},
   "source": [
    "#### Ridge (L2 norm)\n",
    "It sums the squares of the weights. Use this method to reduce the impact of weights without eliminating them entirely. Ridge is more commonly used than LASSO for this reason. The L2 regularization term is so called when applied to vectors, but is the Frobenius norm when applied to matrices.\n",
    "\n",
    "General definition:\n",
    "\n",
    "$$\n",
    "\\|W\\|_2=\\lambda \\cdot {\\sum\\limits^{n}_{i=1}{(W_{n})^2}}\n",
    "$$\n",
    "\n",
    "Adaptation to combine with a given cost function, and its derivative with respect to a specific weight $w_j$, as the \"regularization term\":\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & := J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\biggl(\\frac{1}{2m} \\cdot \\|\\vec{w}\\|_2\\biggr) \\\\\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & := J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\biggl(\\frac{\\lambda}{2m} \\cdot {\\sum\\limits^{n}_{i=1}{(\\vec{w}_{n})}^2}\\biggr) \\\\\n",
    "\\frac{\\partial}{\\partial w_j}J_{\\vec{w},b}(\\vec{x}^{(i)}) & := \\frac{\\partial}{\\partial w_j}J_{\\vec{w},b}(\\vec{x}^{(i)}) +\n",
    "\\frac{\\lambda}{m} \\cdot w_j\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In the plot below, notice that the parabolic distributions continue to move their minimums towards 0. This drives the weights down ever smaller, reducing their impact on the overall cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c816dd1-c414-485b-9a09-4cab5502d32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = [0, 1, 3, 5]\n",
    "size = 101\n",
    "w = np.linspace(-2, 3, size)\n",
    "\n",
    "# let y=1, x=1, b=0 for simplicity, J(w) = mean squared error\n",
    "x = np.ones(size); y = np.ones(size); b = np.zeros(size)\n",
    "def J(w, b, x, y, lambd):\n",
    "    return ((w * x - y + b) ** 2) + ((lambd / 2) * w**2)\n",
    "\n",
    "def dJ_dw(w, b, x, y, lambd):\n",
    "    return (w * x - y + b) * x + (lambd * w)\n",
    "\n",
    "params = [\n",
    "    (J, \"L2 Regularization\", \"J(w)\"),\n",
    "    (dJ_dw, \"L2 Regularization Derivative\", \"$\\partial J(w)\\,/\\,\\partial w$\"),\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, param in zip(ax, params):\n",
    "    func, title, ylabel = param\n",
    "    for lambd in lambdas:\n",
    "        axis.plot(w, func(w, b, x, y, lambd), label=f\"$\\lambda$={lambd}\")\n",
    "    axis.set_title(title)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.set_xlabel(\"w\")\n",
    "    axis.legend()\n",
    "\n",
    "ax[1].axhline(y=0, color=\"gray\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3755a9a1-3458-4117-b676-acc4221e93ac",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Dropout\n",
    "Randomly deactivate certain units in a neural network layer for a given iteration of gradient descent. This makes layers more sparse and the remaining units are trained normally. When applied, the model cannot rely heavily on a single feature as the weight values are spread out. Dropout is commonly used in computer vision as there is rarely enough data, making overfitting common.\n",
    "\n",
    "Most implementations define a `keep_prob` input, identifying the probability that any given unit remains in service. Other implementations define a `drop_prob` input, identifying the probability that any given unit is dropped from service. Focusing on the `keep_prob` logic, the range is $\\{p \\in \\mathbb{R}\\,|\\,0 \\lt p \\le 1\\}$. Setting `keep_prob = 1` disables dropout. It is specified on a per-layer basis, and it typically limited to hidden layers. Applying dropout to the input layer means discarding entire features. Applying dropout to the output layer means discarding categorical classes in a softmax regression, for example.\n",
    "\n",
    "It is also common to divide the resulting activation matrix $A$ by the `keep_prob` so that the activation values more closely resemble results from the \"full\" layer. This specific implementation is known as \"inverted dropout\".\n",
    "\n",
    "Because the neural network is different at each iteration of gradient descent, there is no well-defined cost function. Thus, the training and cost cannot be plotted to ensure it continuously decreases. To troubleshoot a model using dropout, set `keep_prob = 1` to temporarily disable dropout, ensure the cost function decreases at each iteration, then re-enable it.\n",
    "\n",
    "Note that some implementations reverse the logic by specifying a `dropout_prob` value in the range $\\{p \\in \\mathbb{R}\\,|\\,0 \\le p \\lt 1\\}$. This determines the probability that a unit is dropped rather than the probability that a unit is kept. The concept is otherwise identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2d2a6a-a94f-4c08-b456-363c36a5619a",
   "metadata": {},
   "source": [
    "#### Other Techniques\n",
    "Overfitting can sometimes be solved by supplying additional training examples. Sometimes, sufficient data simply does not exist. Instead, you can create additional examples that are realistic representations of real data. Using the computer vision example, you can flip a picture horizontally or vertically, or perform slight rotations, to create variants of an existing image. Other distortions, such as crops, stretches, and minor color shifts can also help. One particular use of color shifting is to simulate different lighting environments. For example, \"cooler\" light tends to be more blue while warmer light tends to be more yellow. Adjust the RGB values accordingly when creating new images to reflect (no pun intended) these different lighting styles.\n",
    "\n",
    "Not all of these techniques are universally valid. For example, if you are training a model to recognize human faces, you would not flip images vertically as it would yield upside down faces. Perhaps this is useful in some scenarios, but likely not most; context matters. Synthesizing training examples is not mutually exclusive with the aforementioned regularization methods. As an added bonus, data augmentation can also help with transfer learning (described elsewhere in this document).\n",
    "\n",
    "You can also stop the gradient descent process early, preferably when the difference between the cross-validation error and training error is minimized. This is depicted as \"good fit\" in the conceptual plot. First, train the model and plot both the training and cross-validation error curves. Then, find the required minimum and stop the process after the correct number of iterations. The drawback of this solution is that it is tightly couples the training and regularization processes. It limits future options because if you want to modify the training process, you may also inadvertently impact the regularization process. Many experts prefer to use a dedicated regularization method to separate these two efforts (ie, to keep them orthogonal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d6414-5d36-4703-970a-0e6e258e9f07",
   "metadata": {},
   "source": [
    "### Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f428841-1fd1-4e4d-883d-2328e16f3030",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes the prediction/inference process of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c713ee5d-aebc-43b8-b917-c69c68888a2b",
   "metadata": {},
   "source": [
    "#### Computing Z\n",
    "Each unit performs two steps in series. First, the linear outputs are computed using the formula below. The dot product of matrices $W^{[l]}$ and $A^{[l-1]}$ yields a matrix of shape $(n_l,n_{l-1}) \\bullet (n_{l-1},m) = (n_l,m)$. In order for matrix multiplication to work, the number of columns in $W^{[l]}$ must match the number of rows in $A^{[l-1]}$, which is always equal to the number of units in the previous layer $n_{l-1}$. Adding the column vector $b^{[l]}$ retains the same shape, but applies the bias values on a per column basis. Superscript $[l]$ notations are omitted from the individual matrix elements for cleanliness.\n",
    "\n",
    "$$\n",
    "Z^{[l]} = W^{[l]} \\bullet A^{[l-1]} + b^{[l]}\n",
    "$$\n",
    "\n",
    "First layer matrix expansion for $X = A^{[0]}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[1]} & = \n",
    "\\begin{bmatrix}\n",
    "z^{(1)}_1 & z^{(2)}_1 & \\cdots & z^{(m)}_1 \\\\\n",
    "z^{(1)}_2 & z^{(2)}_2 & \\cdots & z^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "z^{(1)}_{n_l} & z^{(2)}_{n_l} & \\cdots & z^{(m)}_{n_l}\n",
    "\\end{bmatrix}\n",
    "{=}\n",
    "\\begin{bmatrix}\n",
    "w_{1,1} & w_{1,2} & \\cdots & w_{1,n_{l-1}} \\\\\n",
    "w_{2,1} & w_{2,2} & \\cdots & w_{2,n_{l-1}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "w_{n_l,1} & w_{n_l,2} & \\cdots & w_{n_l,n_{l-1}}\n",
    "\\end{bmatrix}\n",
    "\\bullet\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1 & x^{(2)}_1 & \\cdots & x^{(m)}_1 \\\\\n",
    "x^{(1)}_2 & x^{(2)}_2 & \\cdots & x^{(m)}_2 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x^{(1)}_{n_0} & x^{(2)}_{n_0} & \\cdots & x^{(m)}_{n_0}\n",
    "\\end{bmatrix}\n",
    "{+}\n",
    "\\begin{bmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "\\vdots \\\\\n",
    "b_{n_l}\n",
    "\\end{bmatrix} \\\\\n",
    "Z^{[1]} & = \n",
    "\\begin{bmatrix}\n",
    "% first row\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{1,j} \\cdot x^{(1)}_j)} + b_1 &\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{1,j} \\cdot x^{(2)}_j)} + b_1 & \\cdots & \n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{1,j} \\cdot x^{(m)}_j)} + b_1 \\\\\n",
    "% second row\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{2,j} \\cdot x^{(1)}_j)} + b_2 & \n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{2,j} \\cdot x^{(2)}_j)} + b_2 & \\cdots &\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{2,j} \\cdot x^{(m)}_j)} + b_2 \\\\\n",
    "% third row\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "% fourth row\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{n_l,j} \\cdot x^{(1)}_j)} + b_{n_l} & \n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{n_l,j} \\cdot x^{(2)}_j)} + b_{n_l} & \\cdots &\n",
    "\\sum\\limits^{n_{l-1}}_{j=1}{(w_{n_l,j} \\cdot x^{(m)}_j)} + b_{n_l}\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc41bb9-a275-4d93-8b51-51c931e5f7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb46e35-f15e-40f6-9156-0bb5dd7d81b7",
   "metadata": {},
   "source": [
    "#### Computing A\n",
    "\n",
    "Matrix $Z$ serves as the input for the activation function $g$. The result is the activation matrix $A$ which is the same shape as $Z$ because the activation function acts element-wise on $Z$. Activation functions may vary between layers, hence the $[l]$ superscript applied to $g$. Note that $A^{[0]} = X$ and $A^{[L]} = \\hat{Y}$ for a neural network with $L$ layers.\n",
    "\n",
    "$$\n",
    "g^{[l]}(Z^{[l]}) = A^{[l]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bda9b9-9a1e-4e92-9503-c44882a4d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2ca76a-1036-4e41-ab76-3605fa0c3541",
   "metadata": {},
   "source": [
    "### Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6748484-6624-49b4-9855-3cad5ff470d2",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes common cost functions and their derivatives. Computing the cost happens after a prediction is made (forward propagation) and seeds the process of training the model (backward propagation and gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076caad2-d999-455e-97d5-1a5ad0941b62",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "Primarily used for regression problems, such as predicting a continuous value rather than a classification. There is only a single global minimum due to its convex shape (no local minima), assuming linear activation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}^{(i)} & = \\vec{w} \\bullet \\vec{x}^{(i)} + b \\\\\n",
    "J_{w,b}(x^{(i)}) & = \\frac{1}{2m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The cost function can be partially differentiated with respect to $W$ and $b$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{2m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\\\\n",
    "\\frac{\\partial}{\\partial w}J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)}) \\cdot \\vec{x}^{(i)} \\\\\n",
    "\\frac{\\partial}{\\partial b}J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8bb6a2-6413-40bd-9b4a-891000780346",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Binary Cross Entropy\n",
    "Primarily used for logistic regression problems whereby training examples are labeled with $y = 1$ to indicate presence in a category or $y = 0$ to indicate absence from a category. There is only a single global minimum due to its convex shape (no local minima), assuming sigmoid activation.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}^{(i)} = \\sigma(\\vec{w} \\bullet \\vec{x}^{(i)} + b)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Consider a loss function for a single training example/label pair. Only half of the formula is relevant based on the value of $y^{(i)}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L_{\\vec{w},b}(\\hat{y}^{(i)}, y^{(i)}) & =\n",
    "\\begin{cases}\n",
    "y^{(i)} = 1: & -\\ln{(\\hat{y}^{(i)})} \\\\\n",
    "y^{(i)} = 0: & -\\ln{(1 - \\hat{y}^{(i)})}\n",
    "\\end{cases} \\\\\n",
    "L_{\\vec{w},b}(\\hat{y}^{(i)}, y^{(i)}) & = (-y^{(i)} \\cdot \\ln{(\\hat{y}^{(i)}))} - (1 - y^{(i)}) \\cdot \\ln{(1 - \\hat{y}^{(i)})}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Define the total cost function by expanding the per-example loss function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}L_{\\vec{w},b}(\\hat{y}^{(i)}, y^{(i)}) \\\\\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & = -\\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}(y^{(i)} \\cdot \\ln{(\\hat{y}^{(i)}))} + (1 - y^{(i)}) \\cdot \\ln{(1 - \\hat{y}^{(i)})} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The cost function can be partially differentiated with respect to $W$ and $b$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{2m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)})^2 \\\\\n",
    "\\frac{\\partial}{\\partial w}J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)}) \\cdot \\vec{x}^{(i)} \\\\\n",
    "\\frac{\\partial}{\\partial b}J_{\\vec{w},b}(\\vec{x}^{(i)}) & = \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}(\\hat{y}^{(i)} - y^{(i)}) \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75648df0-6346-4678-9464-31f6bd6b781f",
   "metadata": {},
   "source": [
    "#### Sparse Categorical Cross Entropy\n",
    "Primarily used for softmax regression problems whereby training examples are labeled with values representing their categories where the number of categories is greater than 2. \"Sparse\" means each logit can identify only one class. The loss logic is identical to binary cross entropy except expanded to multiple classes. $n$ is the number of features in the final layer, which is also the number of classes.\n",
    "\n",
    "$$\n",
    "L(a_1, a_2, \\cdots , a_n, y^{(i)}) =\n",
    "\\begin{cases}\n",
    "y^{(i)} = 1: & -\\ln(a_1) \\\\\n",
    "y^{(i)} = 2: & -\\ln(a_2) \\\\\n",
    "& \\vdots \\\\\n",
    "y^{(i)} = n: & -\\ln(a_n)\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Formally: for each training example, for each class, check if the current class matches the expected class. If so, compute the loss. Note that the quantity inside the $\\ln()$ is the result of softmax activation. This function is always convex.\n",
    "\n",
    "$$\n",
    "J_{\\vec{w},b}(\\vec{x}^{(i)}) = -\\frac{1}{m} \\cdot \\left[\\sum\\limits^m_{i=1}\\sum\\limits^n_{j=1}1\\{y^{(i)}=j\\} \\cdot \\ln \\left(\\frac{e^{z_j}}{\\sum^n_{k=1}e^{z_k}}\\right)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c704a52-edbb-4d45-818f-997fc5a04cca",
   "metadata": {},
   "source": [
    "### Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f59a9-1b11-428b-aac2-5d5aaeae78db",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Overview\n",
    "This section describes the process by which the model computes derivatives with respect to its parameters at each layer. Just as forward propagation unpacks the action of a single unit into two discrete steps (computing $Z$ then computing $A$), so too does backward propagation (back prop) separate the discrete steps of differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb0f1af-76f7-49b4-be28-d10ea37a0a8e",
   "metadata": {},
   "source": [
    "#### Computing dJ/dZ\n",
    "The derivative of the cost function $J$ with respect to $A^{[l]}$ is computed using $Z^{[l]}$ as an input, which was computed during forward prop. Put simply, this involves differentiating the activation function, resulting in derivative of the cost function with respect to $Z^{[l]}$. The \"chain rule\" of calculus helps show the logical reduction. Each activation has a different derivative to yield exact numbers, which isn't shown here. Note that the $\\odot$ means element-wise multiplication (Hadamard product).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{[l]} = & g^{[l]}(Z^{[l]}) \\\\\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} = & \\frac{\\partial J}{\\partial A^{[l]}} \\odot g^{[l]'}(Z^{[l]}) \\\\\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} = & \\frac{\\partial J}{\\partial A^{[l]}} \\odot \\frac{\\partial g^{[l]}(Z^{[l]})}{\\partial Z^{[l]}} \\\\\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} = & \\frac{\\partial J}{\\cancel{\\partial A^{[l]}}} \\odot \\frac{\\cancel{\\partial g^{[l]}(Z^{[l]})}}{\\partial Z^{[l]}} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dea7975-3e5a-46ff-ae84-ccc0e21a3d6b",
   "metadata": {},
   "source": [
    "#### Computing dJ/dA, dJ/dW, and dJ/db\n",
    "All three of the derivatives computed in this section rely on $\\frac{\\partial J}{\\partial Z^{[l]}}$ from the previous step. Note that $W^{[l]}$ and $A^{[l-1]}$ are both required below; both were computed during forward prop. Also note that $b^{[l]}$ is not needed for any calculation and thus should not be retained in the forward prop cache during implementation.\n",
    "\n",
    "Two formulas require the use of matrix transpose using superscript $T$. This is required to compute $\\frac{\\partial J}{\\partial A^{[l-1]}}$ given $W^{[l]}$ of shape $(n_l, n_{l-1})$ and $\\frac{\\partial J}{\\partial Z^{[l]}}$ of shape $(n_l, m)$. $W^{[l]}$ must be transposed so that the $n_l$ dimension matches, yielding a matrix of shape $(n_{l-1}, m)$ for $\\frac{\\partial J}{\\partial A^{[l-1]}}$. The same idea applies to the computation of $\\frac{\\partial J}{\\partial W^{[l]}}$. Recall that $\\frac{\\partial J}{\\partial Z^{[l]}}$ has shape $(n_l, m)$ while $A^{[l-1]}$ has shape $(n_{l-1}, m)$. The shapes of the parameter matrices $W$ and $b$ are defined by the neural network's architecture, not the number of training examples, and the transpose operation aligns the $m$ dimensions. This yields a $\\frac{\\partial J}{\\partial W^{[l]}}$ matrix of shape $(n_{l-1}, n_l)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} = & W^{{[l]}^T} \\bullet \\frac{\\partial J}{\\partial Z^{[l]}} \\\\\n",
    "\\frac{\\partial J}{\\partial W^{[l]}} = & \\frac{1}{m} \\cdot (\\frac{\\partial J}{\\partial Z^{[l]}} \\bullet A^{{[l-1]}^T}) \\\\\n",
    "\\frac{\\partial J}{\\partial b^{[l]}} = & \\frac{1}{m} \\cdot \\sum\\limits^{m}_{i=1}\\frac{\\partial J}{\\partial Z^{[l](i)}} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$\\frac{\\partial J}{\\partial W^{[l]}}$ and $\\frac{\\partial J}{\\partial b^{[l]}}$ measure the direction and magnitude of changes required to the weight and bias parameters at a given layer, respectively. Larger values of either sign indicate that the parameter in question is a relatively large contributor to the overall cost, requiring a larger adjustment by the optimization algorithm. $\\frac{\\partial J}{\\partial A^{[l-1]}}$ is passed backwards to the previous layer. At the end of the process, a generalized implementation may also compute $\\frac{\\partial J}{\\partial A^{[0]}}$ or $\\frac{\\partial J}{\\partial X}$, but this value is unused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed6b17-83e3-4d78-a94f-37b667cd17bb",
   "metadata": {},
   "source": [
    "### Optimization Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ebe9a-fe9a-4a5b-a817-ca687c13fc50",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes algorithms to train a model given a cost function, its derivative, plus weights and biases computed through forward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1944498-07eb-4a30-b696-be3650dfef93",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent\n",
    "Update the weights $W$ and biases $b$ at a given layer $[l]$ after a full pass of the training set. It subtracts the product of the learning rate $\\alpha$ and derivative with respect to the parameters in question $\\frac{\\partial J}{\\partial W^{[l]}}$ or $\\frac{\\partial J}{\\partial b^{[l]}}$. The derivative determines the parameters's contribution to the total cost $J$. If the parameter was a major contributor to the total cost, the derivative will be a large positive or negative number. Negative derivatives will result in a positive change to the value, bringing it closer to 0. Positive derivatives achieve the same goal in the opposite direction, reducing the value to bring it closer to 0.\n",
    "\n",
    "The point at which the derivative is 0 is the minimum (the \"valley\" on a 2D or 3D plot). Such a point could be a local minimum rather than the global minimum. Given a highly dimensional plot with many parameters, such local minima are rare as all dimensions would need to be convex (sloping upwards) at once. It is more likely that there is a single global minimum with many \"saddles\" along the way, which slope upwards in some dimensions and downwards in others.\n",
    "\n",
    "$$\n",
    "\\text{repeat until convergence}\n",
    "\\begin{cases}\n",
    "W^{[l]} := & W^{[l]} - \\alpha \\cdot \\frac{\\partial J}{\\partial W^{[l]}} \\\\\n",
    "b^{[l]} := & b^{[l]} - \\alpha \\cdot \\frac{\\partial J}{\\partial b^{[l]}}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b1395f-4911-4d5d-b8a3-fdaa8e35c2f6",
   "metadata": {},
   "source": [
    "#### Mini-batch Gradient Descent\n",
    "When the number of training examples $m$ is large, batch gradient descent cannot act until all training examples have been processed. Minibatches break up this process, allowing gradient descent to act sooner, albeit without seeing the entire training set. This makes the weight and bias adjustments less precise but faster and more frequent.\n",
    "\n",
    "Be sure to shuffle the training data and the corresponding labels before dividing into mini-batches. Use a common random seed to ensure $X$ and $Y$ are shuffled according to the same scheme. It is common to create mini-batches sized by powers of 2, such as 32, 64, 128, 256, and 512. The sizes are chosen so that they can fit inside of GPUs memory, which vary. Each mini-batch matrix is notated as $X^{\\{t\\}}$ where $t$ is the mini-batch number. Given a mini-batch size of $N$ examples, the shape of $X^{\\{t\\}}$ is $(n_0, N)$. The final mini-batch is likely to be smaller than the mini-batch size to collect all the remaining examples, but this is an implementation detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52a0d88-eaf8-4488-a266-224997d6dbbb",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent (SGD)\n",
    "This is conceptually identical to mini-batch gradient descent, except the number of training examples in each mini-batch $N$ is only 1. This allows the model to run gradient descent very quickly given that only one training example is being processed, although the cost may not monotonically decrease. That is to say, the cost may occasionally increase, although it is expected to trend downward towards. The drawback of SGD is that it requires $m$ iterations, one per training example, which could number in the millions. This can be computationally expensive as it reduces the efficiency benefits of a fully vectorized implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0c6b5-a910-4aa0-9373-8c3f6cf29ce3",
   "metadata": {},
   "source": [
    "#### Gradient Descent with Momentum\n",
    "This algorithm enhances gradient descent by using an exponentially weighted moving average (EWMA) to compute the gradients, then using that EWMA to update the parameters.\n",
    "\n",
    "The relevant formulas to compute EWMA are shown below. The first value $V_0$ starts off at 0. The next value $V_1$ is a combination of the previous value $V_0$ and the resulting data point $f(t)$ at a given time $t$. The hyperparameter $\\beta$ controls the algorithm's sensitivity to changes.\n",
    "\n",
    "Larger values of $\\beta$ account for more data over a longer time period, making the average less responsive to recent changes. Smaller values of $\\beta$ give more weight to newer samples, making the average more responsive to recent changes. As a rough estimate, the algorithm averages over $\\frac{1}{1-\\beta}$ time entries.\n",
    "\n",
    "The $V_t^{corr}$ value represents the optional bias correction process. During the first few iterations of the algorithm, the EWMA will routinely underestimate the true average. Bias correction yields more accurate results early in the training process but becomes less relevant over time as more data is processed.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "V_0 = & 0 \\\\\n",
    "V_t = & \\beta \\cdot V_{t-1} + (1-\\beta) \\cdot f(t) \\\\\n",
    "V_t^{corr} = & \\frac{V_t}{1-\\beta^t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e83ce38-6a09-4f71-abdc-022553eefae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in = 20\n",
    "beta = np.array([0.8, 0.9, 0.95])\n",
    "x_in = np.arange(1, size_in + 1, 1)\n",
    "y_in = np.random.rand(size_in) * 5\n",
    "\n",
    "def ewma(size, y, B):\n",
    "    v, v_corr = np.zeros(size), np.zeros(size)\n",
    "    for t in range(1, len(y)):\n",
    "        v[t] = (B * v[t-1]) + ((1 - B) * y[t])\n",
    "        v_corr[t] = v[t] / (1 - B**t)\n",
    "\n",
    "    return v, v_corr\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(beta), sharey=True)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"temperature (celsius)\")\n",
    "\n",
    "for B, axis in zip(beta, ax):\n",
    "    v, v_corr = ewma(size_in, y_in, B)\n",
    "    axis.set_title(f\"B = {B} (avg over ~{int(round(1/(1-B)))} days)\")\n",
    "    axis.set_xlabel(\"time (days)\")\n",
    "    axis.plot(x_in, y_in, label=\"data\", linestyle=\"\", marker=\".\")\n",
    "    axis.plot(x_in, v, label=\"v\")\n",
    "    axis.plot(x_in, v_corr, label=\"v_corr\")\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ea320-35c8-4068-be56-c2ab8faa836a",
   "metadata": {},
   "source": [
    "In the context of gradient descent, the EWMA resembles \"momentum\", allowing the algorithm to trend downhill more quickly without oscillations in dimensions that diverge from the global minimum. This strategy can be combined with any of the aforementioned gradient descent variants (batch, mini-batch, or stochastic).\n",
    "\n",
    "The matrix $v_{\\partial W^{[l]}}$ has the same shape as the weights $W^{[l]}$ and the weight derivatives. The matrix $v_{\\partial b^{[l]}}$ has the same shape as the biases $b^{[l]}$ and the bias derivatives. Both are initialized to all zeros. The derivatives ${\\partial W^{[l]}}$ and ${\\partial b^{[l]}}$ are the normal values computed by backwards propagation, omitting the numerator $\\partial J(\\theta)$ for brevity. The new matrices $v_{\\partial W^{[l]}}$ and $v_{\\partial b^{[l]}}$ represent the computed EWMA values for weight and bias derivatives, respectively.\n",
    "\n",
    "$$\n",
    "\\text{repeat until convergence}\n",
    "\\begin{cases}\n",
    "v_{\\partial W^{[l]}} & = (\\beta \\cdot v_{\\partial W^{[l]}}) + \\bigl((1 - \\beta) \\cdot {\\partial W^{[l]}}\\bigr) \\\\\n",
    "v_{\\partial b^{[l]}} & = (\\beta \\cdot v_{\\partial b^{[l]}}) + \\bigl((1 - \\beta) \\cdot {\\partial b^{[l]}}\\bigr) \\\\\n",
    "\\\\\n",
    "W^{[l]} & := W^{[l]} - \\alpha \\cdot v_{\\partial W^{[l]}} \\\\\n",
    "b^{[l]} & := b^{[l]} - \\alpha \\cdot v_{\\partial b^{[l]}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The algorithm can be modified by including the optional $v_{\\partial W^{[l]}\\_corr}$ and $v_{\\partial b^{[l]}\\_corr}$ bias correction steps. In practice, this is not commonly implemented because given a relatively large $\\beta$ value, the EWMA algorithm will decay the older values. This makes the first few results less relevant over time.\n",
    "\n",
    "$$\n",
    "\\text{repeat until convergence}\n",
    "\\begin{cases}\n",
    "v_{\\partial W^{[l]}} & = (\\beta \\cdot v_{\\partial W^{[l]}}) + \\bigl((1 - \\beta) \\cdot {\\partial W^{[l]}}\\bigr) \\\\\n",
    "v_{\\partial b^{[l]}} & = (\\beta \\cdot v_{\\partial b^{[l]}}) + \\bigl((1 - \\beta) \\cdot {\\partial b^{[l]}}\\bigr) \\\\\n",
    "v_{\\partial W^{[l]}}^{corr} & = \\frac{v_{\\partial W^{[l]}}}{1 - \\beta^t} \\\\\n",
    "v_{\\partial b^{[l]}}^{corr} & = \\frac{v_{\\partial b^{[l]}}}{1 - \\beta^t} \\\\\n",
    "\\\\\n",
    "W^{[l]} & := W^{[l]} - \\alpha \\cdot v_{\\partial W^{[l]}}^{corr} \\\\\n",
    "b^{[l]} & := b^{[l]} - \\alpha \\cdot v_{\\partial b^{[l]}}^{corr}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e738fb-690b-42fd-9b36-8b67bdeaf586",
   "metadata": {},
   "source": [
    "#### Adaptive Moment Estimation (ADAM)\n",
    "ADAM adds Root Mean Squared (RMS) propagation to the momentum-based gradient descent algorithm. RMS propagation allows gradient descent to speed up in the direction towards the global minimum while slowing down in directions orthogonal to the global minimum. It is similar in concept to momentum except computes the square of the derivatives ${\\partial W^{[l]}}$ and ${\\partial b^{[l]}}$ when computing an EWMA. When updating the weights, it multiplies $\\alpha$ by the ratio of the momentum EWMA and the square root of that RMS-computed EWMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8d050-024d-4e47-97ae-70f955df7aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_in = 20\n",
    "beta = np.array([0.9, 0.99, 0.999])\n",
    "x_in = np.arange(1, size_in + 1, 1)\n",
    "y_in = np.random.rand(size_in) * 5\n",
    "\n",
    "def rms_prop(size, y, B, epsilon=1e-8):\n",
    "    s, s_corr = np.zeros(size), np.zeros(size)\n",
    "    for t in range(1, len(y)):\n",
    "        s[t] = (B * s[t-1]) + ((1 - B) * y[t]**2)\n",
    "        s_corr[t] = s[t] / (1 - B**t)\n",
    "\n",
    "        s[t] = y[t] / (np.sqrt(s[t]) + epsilon)\n",
    "        s_corr[t] = y[t] / (np.sqrt(s_corr[t]) + epsilon)\n",
    "        \n",
    "    return s, s_corr\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(beta), sharey=False)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(\"temperature (celsius)\")\n",
    "\n",
    "for B, axis in zip(beta, ax):\n",
    "    s, s_corr = rms_prop(size_in, y_in, B)\n",
    "    axis.set_title(f\"B = {B}\")\n",
    "    axis.set_xlabel(\"time (days)\")\n",
    "    axis.plot(x_in, y_in, label=\"data\", linestyle=\"\", marker=\".\")\n",
    "    axis.plot(x_in, s, label=\"s\")\n",
    "    axis.plot(x_in, s_corr, label=\"s_corr\")\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5834f5-a90f-4696-a7b9-46f34f1ab488",
   "metadata": {},
   "source": [
    "The matrices $v_{\\partial W^{[l]}}$ and $s_{\\partial W^{[l]}}$ have the same shape as the weights $W^{[l]}$ and the weight derivatives. The matrices $v_{\\partial b^{[l]}}$ and $s_{\\partial b^{[l]}}$ have the same shape as the biases $b^{[l]}$ and the bias derivatives. These four matrices are initialized to all zeros. Note that both algorithms use $\\beta$ in their formal definitions, which are rewritten as $\\beta_1$ and $\\beta_2$ to differentiate them. Their default values in most implementations are $\\beta_1 = 0.9$ and $\\beta_2 = 0.999$ which are rarely tuned. The tiny value $\\varepsilon = 1 \\times 10^{-8}$ is almost never tuned and only prevents against division by zero errors. The learning rate $\\alpha$ remains the most interesting hyperparameter in ADAM.\n",
    "\n",
    "$$\n",
    "\\text{repeat until convergence}\n",
    "\\begin{cases}\n",
    "v_{\\partial W^{[l]}} & = (\\beta_1 \\cdot v_{\\partial W^{[l]}}) + \\bigl((1 - \\beta_1) \\cdot {\\partial W^{[l]}}\\bigr) \\\\\n",
    "v_{\\partial b^{[l]}} & = (\\beta_1 \\cdot v_{\\partial b^{[l]}}) + \\bigl((1 - \\beta_1) \\cdot {\\partial b^{[l]}}\\bigr) \\\\\n",
    "v_{\\partial W^{[l]}}^{corr} & = \\frac{v_{\\partial W^{[l]}}}{1 - \\beta_1^t} \\\\\n",
    "v_{\\partial b^{[l]}}^{corr} & = \\frac{v_{\\partial b^{[l]}}}{1 - \\beta_1^t} \\\\\n",
    "\\\\\n",
    "s_{\\partial W^{[l]}} & = (\\beta_2 \\cdot s_{\\partial W^{[l]}}) + \\bigl((1 - \\beta_2) \\cdot {\\partial W^{[l]}}^2\\bigr) \\\\\n",
    "s_{\\partial b^{[l]}} & = (\\beta_2 \\cdot s_{\\partial b^{[l]}}) + \\bigl((1 - \\beta_2) \\cdot {\\partial b^{[l]}}^2\\bigr) \\\\\n",
    "s_{\\partial W^{[l]}}^{corr} & = \\frac{s_{\\partial W^{[l]}}}{1 - \\beta_2^t} \\\\\n",
    "s_{\\partial b^{[l]}}^{corr} & = \\frac{s_{\\partial b^{[l]}}}{1 - \\beta_2^t} \\\\\n",
    "\\\\\n",
    "W^{[l]} & := W^{[l]} - \\alpha \\cdot \\frac{v_{\\partial W^{[l]}}^{corr}}{\\sqrt{s_{\\partial W^{[l]}}^{corr}} + \\varepsilon} \\\\\n",
    "b^{[l]} & := b^{[l]} - \\alpha \\cdot \\frac{v_{\\partial b^{[l]}}^{corr}}{\\sqrt{s_{\\partial b^{[l]}}^{corr}} + \\varepsilon}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f2995-e9b0-45b3-a654-60fd0dc87846",
   "metadata": {},
   "source": [
    "#### Optional: Gradient Checking\n",
    "This technique is used for troubleshooting, not training, and can determine if your gradients (derivatives) have been computed correctly. Consider this definition of a derivative that includes sampling on both sides of the function:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta_i}J(\\vec{\\theta}) = \\lim_{\\varepsilon \\to 0}\n",
    "\\frac{J(\\theta_1, \\theta_2, \\cdots, \\theta_i + \\varepsilon) -\n",
    "J(\\theta_1, \\theta_2, \\cdots, \\theta_i - \\varepsilon)}{2 \\cdot \\varepsilon}\n",
    "$$\n",
    "\n",
    "In this context, $\\vec{\\theta}$ is a 1D vector that contains all of the weights $W$ and biases $b$. The weight matrices are unrolled into one dimension and the biases are concatenated afterwards. This process repeats for every layer, resulting in a giant vector.\n",
    "\n",
    "Next, repeat this flattening process for all of the derivatives computed with respect to each parameter $W$ and $b$. These derivatives are the result of back propagation and are the specific values that gradient checking will validate. Let's call this vector of gradients $\\partial{\\vec{\\theta}}$ for short which has the same length as $\\vec{\\theta}$.\n",
    "\n",
    "The gradient checking algorithm iterates over elements of $\\vec{\\theta}$, computing the cost using fixed values for each element in $\\vec{\\theta}$ except for the current one under evaluation $\\theta_i$. This is how we approximate the derivatives with respect to individual parameters; we must fix all other elements of $\\vec{\\theta}$ and only change $\\theta_i$ up by a tiny amount $\\varepsilon$. The algorithm stores these gradient approximations in a vector that is also the same length as $\\vec{\\theta}$ and $\\partial{\\vec{\\theta}}$. This new vector is $\\partial{\\vec{\\theta}}_a$ with subscript $a$ indicating \"approximate\".\n",
    "\n",
    "Theoretically, the derivatives computed by backward propagation should equal the approximated derivatives computed by gradient checking. In reality, the values will be slightly different. We can measure the similarity of these two vectors using a normalized variant of Euclidean distance.\n",
    "\n",
    "$$\n",
    "d = \\frac{\\|\\partial{\\vec{\\theta}}_a - \\partial{\\vec{\\theta}}\\|_2}{\\|\\partial{\\vec{\\theta}}_a\\|_2 + \\|\\partial{\\vec{\\theta}}\\|_2}\n",
    "$$\n",
    "\n",
    "If the resulting distance $d$ is less than a tiny threshold, such as $1 \\times 10^{-7}$, then backward propagation has computed the correct gradients. If the resulting distance is too large, it suggests a software bug. Examine the two gradient vectors closely and look for pairwise values that have large differences. This indicates the specific parameter at the specific layer where the bug is likely to be.\n",
    "\n",
    "Note that the cost function $J(\\vec{\\theta})$ must include regularization, if applied, when performing gradient checking. If not, the approximated gradients and backprop-computed gradients will be very different. Also, gradient checking cannot work with dropout regularization as the cost function is not well-defined. You can use `keep_prob = 1` temporarily to run gradient checking on models using dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf2d480-b670-4f75-ba24-594ad8adb813",
   "metadata": {},
   "source": [
    "#### Optional: Learning Rate Decay\n",
    "This optional enhancement allows the learning rate $\\alpha$ to send to slowly decrease (decay) as gradient descent runs. This may reduce oscillations near the global minimum, thus speeding up training. As $t$ increases over time, $\\alpha$ decays more slowly, regardless of the implementation. Several plausible options are shown here; many others also exist.\n",
    "\n",
    "1. Reduce the learning rate $\\alpha$ with each epoch $t$ based on a specified rate of decay $r$ based on some initial learning rate $\\alpha_0$. Larger values of $r$ yield faster rates of decay (faster reduction of $\\alpha$), although values slightly greater than 0 are most reasonable. This update occurs after every iteration.\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{\\alpha_{0}}{1+(r \\cdot t)}\n",
    "$$\n",
    "\n",
    "2. Reduce the learning rate $\\alpha$ after a certain number $n$ epochs $t$. This is a step-based decay using the mathematical \"floor\" function. A smaller value of $n$ yields more rapid decay as the learning rate changes more frequently.\n",
    "$$\n",
    "\\alpha = \\frac{\\alpha_{0}}{1+\\lfloor{\\frac{t}{n}}\\rfloor}\n",
    "$$\n",
    "\n",
    "3. Alternative implementation to per-iteration decay, relying on base $b \\in \\mathbb{R}\\,|\\,0 < b < 1$. It decays more rapidly at first, whereby smaller values of $b$ yield faster rates of decay in general. Values slightly less than 1 are most reasonable.\n",
    " \n",
    "$$\n",
    "\\alpha = \\alpha_{0} \\cdot b^{t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfd0c0-c261-44ff-bdf8-331b37a0c86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run with 1000 epochs and an initial learning rate\n",
    "t = np.arange(1, 1001, 1, dtype=int)\n",
    "a0 = 1.0e-01\n",
    "\n",
    "# Make 3 subplots, one per option, then graph each function\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, sharey=True)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(r\"learning rate - $\\alpha$\")\n",
    "\n",
    "for r in np.array([1.0e-3, 3.0e-3, 9.0e-3]):\n",
    "    ax[0].plot(t, a0 / (1 + r * t), label=f\"$r$={r}\")\n",
    "\n",
    "for n in np.array([50, 100, 200]):\n",
    "    ax[1].plot(t, a0 / (1 + np.floor(t / n)), label=f\"$n$={n}\")\n",
    "\n",
    "for b in np.array([0.995, 0.998, 0.999]):\n",
    "    ax[2].plot(t, a0 * np.power(b, t), label=f\"$b$={b}\")\n",
    "\n",
    "for i, axis in enumerate(ax, start=1):\n",
    "    axis.set_title(f\"Option #{i}\")\n",
    "    axis.set_xlabel(\"epochs - $t$\")\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e0e73-bd20-4137-a826-14b3b73f37b2",
   "metadata": {},
   "source": [
    "#### Optional: Hyperparameter Tuning\n",
    "Hyperparameters are human-selected values that are used by the model but are not trained by it. This section describes common techniques for selecting hyperparameters. Note that as the input data changes over time, hyperparameters should be re-evaluated to ensure they are still adequate for data set.\n",
    "\n",
    "To use an Information Technology (IT) analogy, AI models can be seen as pets or cattle. As pets, we closely monitor them, making small adjustments to hyperparameters to optimize performance. This approach is typically used when compute resources are limited or during training/experimentation. Perhaps only one model can be trained at once. \n",
    "\n",
    "As cattle, we raise several at the same time, combining different hyperparameters together to yield several models. This assumes that compute resources are ample and that hyperparameter combinations have been determined ahead of time. At the end of the process, select the best model based on some metric such as lowest cost, highest accuracy/precision/recall, etc.\n",
    "\n",
    "Suppose we are using gradient descent with momentum, which has 2 important hyperparameters: the learning rate $\\alpha$, and the momentum $\\beta$. There are several methods for selecting values.\n",
    "\n",
    "  1. __Grid uniform__: On the linear scale, select values at fixed intervals across a range of reasonable values for each hyperparameter. When visualized on a 2D plot, it looks like a grid. This approach is conceptually simple and deterministic, but it limits the number of unique hyperparameter values available.\n",
    "\n",
    "  2. __Random uniform__: On a linear scale, randomly generate reasonable values for each hyperparameter. The coverage area is the same as the grid, but tends to work better. If one hyperparameter has a very small impact on the learning algorithm (say $\\beta$), the randomness will yield more options for the other hyperparameter (say $\\alpha$), which presumably has a larger impact.\n",
    "\n",
    "  3. __Random log__: On a logarithmic scale, randomly generate reasonable values for each hyperparameter. This provides the benefits of diverse value selection while also more evenly distributing samples across logarithmic ranges. Consider $\\beta$ which typically ranges from 0.9 to 0.999. Rather than uniformly sample across this range, it's more effective to collect a roughly equal number of samples across the different orders of magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba18adb-9450-4063-9810-96eaaa273aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [\n",
    "    np.tile(np.arange(0.1, 1.1, 0.1), 10),\n",
    "    np.random.uniform(1e-5, 1,(100,)),\n",
    "    np.array([[10**(-power * np.random.rand()) for _ in range(20)] for power in range(1,5)]).flatten(),\n",
    "]\n",
    "beta = [\n",
    "    np.repeat(np.arange(0.91, 1.01, 0.01), 10),\n",
    "    np.random.uniform(1 - 1e-1, 1 - 1e-5,(100,)),\n",
    "    np.array([[1 - (10**(-power * np.random.rand())) for _ in range(20)] for power in range(4)]).flatten(),\n",
    "]\n",
    "desc = [\"grid uniform\", \"random uniform\", \"random log\"]\n",
    "scale = [\"linear\", \"linear\", \"log\"]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=len(desc), sharey=False)\n",
    "fig.set_size_inches(12, 4)\n",
    "ax[0].set_ylabel(r\"momentum - $\\beta$\")\n",
    "\n",
    "for i in range(len(desc)):\n",
    "    ax[i].set_xscale(scale[i]); plt.yscale(scale[i])\n",
    "    ax[i].set_title(desc[i])\n",
    "    ax[i].set_xlabel(r\"learning rate - $\\alpha$\")\n",
    "    ax[i].plot(alpha[i], beta[i], marker=\".\", linestyle=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a849c51-915d-4d22-8a95-91bd725e1406",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b135d616-e5ba-4862-b0ee-2b355125ece9",
   "metadata": {},
   "source": [
    "### Convolutional Layer (CONV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e31c31-90e8-4b91-90d9-51efbd4a5894",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This layer forms the basis of a CNN. Primarily used for computer vision use-cases, CONV layers examine subsets of an image at a time, and reducing that subset to a single value. Different filters are used to examine different subsets and are used to detect different features by panning across the image using a stride interval. The total number of parameters is greatly reduced when compared to MLP layers, requiring less time and compute power to train.\n",
    "\n",
    "Convolutional layers are effective because the feature-detecting filters are likely to be useful across different parts of the image, which reduces the number of parameters to be trained. Additionally, there are far fewer connections between convolutional layers compared to fully-connected layers. Each convolutional output is depends only on a few inputs (those in the filter's window).\n",
    "\n",
    "Because 2D images are so pervasive, they dominate the discussion of CNNs. However, the concepts can also apply to 1D data (such as EKG streams) and 3D data (such as CT/MRI scans)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4914f15a-9a1a-48e1-b4de-fd59c6682292",
   "metadata": {},
   "source": [
    "#### Notation and Matrix Shapes\n",
    "\n",
    "Many of the notational conventions from the MLP section are retained here. The CNN-specific notation of key values at layer $l$ is below.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f^{[l]} & = \\text{filter height and width (if square, which is common)} \\\\\n",
    "f_H^{[l]} & = \\text{filter height (if not square)} \\\\\n",
    "f_W^{[l]} & = \\text{filter width (if not square)} \\\\\n",
    "s^{[l]} & = \\text{number of strides} \\\\\n",
    "p^{[l]} & = \\text{thickness of padded cells (number of surrounding layers)} \\\\\n",
    "n_H^{[l]} & = \\text{input matrix height (vertical)} \\\\\n",
    "n_W^{[l]} & = \\text{input matrix width (horizontal)} \\\\\n",
    "n_C^{[l]} & = \\text{input matrix channels (depth)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The following equations summarize how to compute the matrix dimensions at layer $l$. In general, a larger value of $f$ implies a larger filter, resulting in smaller values of $n_H^{[l]}$ and $n_W^{[l]}$. Larger values of $p$ imply additional layers of padding, resulting in larger values of $n_H^{[l]}$ and $n_W^{[l]}$. Larger values of $s$ imply longer strides when panning, resulting in smaller values of $n_H^{[l]}$ and $n_W^{[l]}$. In summary, $f$ and $s$ are indirectly proportional to $n_H^{[l]}$ and $n_W^{[l]}$ while $p$ is directly proportional to $n_H^{[l]}$ and $n_W^{[l]}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_H^{[l]} & = \\Bigl\\lfloor \\frac{(n_H^{[l-1]} - f^{[l]}) + (2 \\times p^{[l]})}{s^{[l]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_W^{[l]} & = \\Bigl\\lfloor \\frac{(n_W^{[l-1]} - f^{[l]}) + (2 \\times p^{[l]})}{s^{[l]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_C^{[l]} & = \\text{number of filters applied}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Each filter is a feature selector which is squared-shaped, often using an odd natural number for $f^{[l]}$. The depth of the filter is always equals the depth of the matrix being evaluated $n_C^{[l-1]}$. This is because the filter only pans along the height/width 2D plane. For this reason, filters of shape $(1,1,n_C^{[l-1]})$ can be useful as they evaluate the entire depth of the 3D matrix despite only panning one cell at a time in 2D. This special type of filter is called \"network in network\" and can reduce the depth of a 3D volume by ensuring there are fewer such filters at the present layer compared to the previous layer $n_C^{[l]} < n_C^{[l-1]}$. Although less commonly used, these filters can also increase the depth of the volume using a larger number of filters.\n",
    "\n",
    "$$\n",
    "f^{[l]}_{dim} = f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]}\n",
    "$$\n",
    "\n",
    "Each value in the filter is a trainable weight. The number of channels in the current layer $n_C^{[l]}$ represents the number of filters applied by that layer. Therefore, the total number of weights at a layer $w^{[l]}_{dim}$ equals the dimensions of the filter times the number of filters at that layer. Also note that sometimes the word \"kernel\" is used instead of \"filter\" and is the denoted as $k$; they are fully synonymous.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w^{[l]}_{dim} & = f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]} \\times n_C^{[l]} \\\\\n",
    "w^{[l]}_{dim} & = f^{[l]}_{dim} \\times n_C^{[l]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "A single bias is applied element-wise (broadcasted) to each value in each filter. It is also trainable. Therefore, the total number of biases at a layer $b^{[l]}_{dim}$ equals the number to filters at that layer.\n",
    "$$\n",
    "b^{[l]}_{dim} = n_C^{[l]}\n",
    "$$\n",
    "\n",
    "The total number of parameters at a layer $t^{[l]}_{dim}$ is the sum of the numbers of weights and biases at that layer.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "t^{[l]}_{dim} & = w^{[l]}_{dim} + b^{[l]}_{dim} \\\\\n",
    "t^{[l]}_{dim} & = f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]} \\times n_C^{[l]} + n_C^{[l]} \\\\\n",
    "t^{[l]}_{dim} & = (f^{[l]} \\times f^{[l]} \\times n_C^{[l-1]} + 1) \\times n_C^{[l]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cc1038-eabe-4c38-a405-aeecc3d00fe4",
   "metadata": {},
   "source": [
    "#### Padding Types\n",
    "\n",
    "1. __valid__: No padding applied ($p=0$).\n",
    "2. __same__: Apply padding such that the the output matrix preserves the height and width dimensions of the input matrix. This technique enables the existence of a deeper CNN that doesn't keep shrinking the image after each convolution. It guarantees $n_H^{[l]} = n_H^{[l-1]}$ and $n_W^{[l]} = n_W^{[l-1]}$. Note that $n_C^{[l]}$ and $n_C^{[l-1]}$ may vary as they represent the number of filters applied at their respective layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abe829-6089-46c2-a3f8-e264cf826436",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randint(low=1, high = 9, size=(4,4))\n",
    "for i in range(3):\n",
    "    X_pad = np.pad(X, ((i, i), (i, i)), mode=\"constant\", constant_values = (0, 0))\n",
    "    print(f\"\\nX padding={i} shape{X_pad.shape}\\n{X_pad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa233279-9a88-435a-95d4-9088453b9108",
   "metadata": {},
   "source": [
    "#### Computing Z\n",
    "Unlike MLP inputs, the 2D or 3D matrix is not flattened/unrolled. The formula below describes how to compute a given cell $Z^{[l]}_{m,n}$ of the resulting matrix $Z^{[l]}$. Somewhat similar to an MLP, it is a function of the previous layer's activation matrix $A^{[l-1]}$ and the current layers weight matrix $F^{[l]}$.\n",
    "\n",
    "$$\n",
    "Z^{[l]}_{m,n} = \\sum\\limits^{f^{[l]}_H-1+m}_{i=m} \\sum\\limits^{f^{[l]}_W-1+n}_{j=n} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-m, j-n}\n",
    "$$\n",
    "\n",
    "Consider a grayscale ($n_C^{[0]}=1$) input image matrix with shape $n_H^{[0]} \\times n_W^{[0]} = (6,6)$ for a total of 36 pixels. Assuming a filter size of 3 $(f^{[1]}=3)$, the filter matrix has shape $(3,3)$. Valid padding is used ($p^{[1]}=0$) with the minimum stride ($s^{[1]}=1$). The resulting matrix at layer 1 after convolution will have shape $(4,4)$ after computing $n_H^{[1]}$ and $n_W^{[1]}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_H^{[1]} & = \\Bigl\\lfloor \\frac{(n_H^{[0]} - f^{[l]}) + (2 \\times p^{[1]})}{s^{[1]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_H^{[1]} & = \\Bigl\\lfloor \\frac{(6 - 3) + (2 \\times 0)}{1} \\Bigr\\rfloor + 1 = 4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_W^{[1]} & = \\Bigl\\lfloor \\frac{(n_W^{[0]} - f^{[l]}) + (2 \\times p^{[1]})}{s^{[1]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_W^{[1]} & = \\Bigl\\lfloor \\frac{(6 - 3) + (2 \\times 0)}{1} \\Bigr\\rfloor + 1 = 4\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[l]} = \n",
    "\\begin{bmatrix}\n",
    "\\biggl(Z^{[l]}_{0,0} = \\sum\\limits^2_{i=0} \\sum\\limits^2_{j=0} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i, j}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{0,1} = \\sum\\limits^2_{i=0} \\sum\\limits^3_{j=1} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i, j-1}\\biggr) &\n",
    "\\biggl(Z^{[l]}_{0,2} = \\sum\\limits^2_{i=0} \\sum\\limits^4_{j=2} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i, j-2}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{0,3} = \\sum\\limits^2_{i=0} \\sum\\limits^5_{j=3} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i, j-3}\\biggr) \\\\\n",
    "\\biggl(Z^{[l]}_{1,0} = \\sum\\limits^3_{i=1} \\sum\\limits^2_{j=0} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-1, j}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{1,1} = \\sum\\limits^3_{i=1} \\sum\\limits^3_{j=1} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-1, j-1}\\biggr) &\n",
    "\\biggl(Z^{[l]}_{1,2} = \\sum\\limits^3_{i=1} \\sum\\limits^4_{j=2} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-1, j-2}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{1,3} = \\sum\\limits^3_{i=1} \\sum\\limits^5_{j=3} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-1, j-3}\\biggr) \\\\\n",
    "\\biggl(Z^{[l]}_{2,0} = \\sum\\limits^4_{i=2} \\sum\\limits^2_{j=0} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-2, j}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{2,1} = \\sum\\limits^4_{i=2} \\sum\\limits^3_{j=1} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-2, j-1}\\biggr) &\n",
    "\\biggl(Z^{[l]}_{2,2} = \\sum\\limits^4_{i=2} \\sum\\limits^4_{j=2} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-2, j-2}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{2,3} = \\sum\\limits^4_{i=2} \\sum\\limits^5_{j=3} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-2, j-3}\\biggr) \\\\\n",
    "\\biggl(Z^{[l]}_{3,0} = \\sum\\limits^5_{i=3} \\sum\\limits^2_{j=0} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-3, j}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{3,1} = \\sum\\limits^5_{i=3} \\sum\\limits^3_{j=1} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-3, j-1}\\biggr) &\n",
    "\\biggl(Z^{[l]}_{3,2} = \\sum\\limits^5_{i=3} \\sum\\limits^4_{j=2} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-3, j-2}\\biggr) & \n",
    "\\biggl(Z^{[l]}_{3,3} = \\sum\\limits^5_{i=3} \\sum\\limits^5_{j=3} A^{[l-1]}_{i,j} \\cdot F^{[l]}_{i-3, j-3}\\biggr) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This concrete example of the previous explanation explains further. The image is a picture of a white plus sign on a gray background. No activation functions are applied for simplicity. It is a 4-bit grayscale image where -8 means black, 7 means white, and values in between are shades of gray. If it were an RGB image, there would be 3 channels ($n_C^{[0]}=3$), each of the same 2D shape $n_H^{[0]} \\times n_W^{[0]} = (6,6)$, with values representing the intensity of red, green, and blue, respectively.\n",
    "\n",
    "$$\n",
    "X = Z^{[0]} =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 7 & 7 & 0 & 0 \\\\\n",
    "0 & 0 & 7 & 7 & 0 & 0 \\\\\n",
    "7 & 7 & 7 & 7 & 7 & 7 \\\\\n",
    "7 & 7 & 7 & 7 & 7 & 7 \\\\\n",
    "0 & 0 & 7 & 7 & 0 & 0 \\\\\n",
    "0 & 0 & 7 & 7 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Two filters of shape $(3,3)$ will be applied. All filters applied at a given layer must have the same dimensions. The first filter will detect vertical edges (dark on left, light on right). The second filter will detect horizontal edges (dark on top, light on bottom). The number 3 was chosen to make it easier to visualize when plotting, although other numbers also work.\n",
    "\n",
    "$$\n",
    "F^{[l]}_{vert} = \n",
    "\\begin{bmatrix}\n",
    "3 & 0 & -3 \\\\\n",
    "3 & 0 & -3 \\\\\n",
    "3 & 0 & -3\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}F^{[l]}_{horiz} =\n",
    "\\begin{bmatrix}\n",
    "3 & 3 & 3 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "-3 & -3 & -3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The resulting matrix $Z^{[l]}$ will have two channels $(n_C^{[1]}=2)$, one for the result of each filter applied. The final shape of $Z^{[l]}$ will therefore be $(4,4,2)$. The two layers are represented as $Z^{[l]}_{vert}$ for the vertical edge filter result and $Z^{[l]}_{horiz}$ for the horizontal edge filter result. The sharpness of the edge is determined by the difference between adjacent values. The greatest difference occurs along the vertical midpoint division of $Z^{[l]}_{vert}$ and along the horizontal midpoint division of $Z^{[l]}_{horiz}$.\n",
    "\n",
    "$$\n",
    "Z^{[l]}_{vert} = \n",
    "\\begin{bmatrix}\n",
    "-42 & -42 & 42 & 42 \\\\\n",
    "-21 & -21 & 21 & 21 \\\\\n",
    "-21 & -21 & 21 & 21 \\\\\n",
    "-42 & -42 & 42 & 42\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}Z^{[l]}_{horiz} =\n",
    "\\begin{bmatrix}\n",
    "-42 & -21 & -21 & -42 \\\\\n",
    "-42 & -21 & -21 & -42 \\\\\n",
    "42 & 21 & 21 & 42 \\\\\n",
    "42 & 21 & 21 & 42\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The code below demonstrates a simple implementation of the above example. Note that values of $Z^{[l]}$ less than -7 round up to -7 and values greater than 7 round down to 7. The addition of an element-wise bias $b^{[l]}$ is excluded as it doesn't make sense for static, instructive filters. In a real implementation, the bias $b^{[l]}$ is trainable, as are the weights $w^{[l]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7017c3-859c-4353-9600-bf81ba9ace79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a007700 = np.array([0, 0, 7, 7, 0, 0])\n",
    "a777777 = np.array([7, 7, 7, 7, 7, 7])\n",
    "X = np.array([a007700, a007700, a777777, a777777, a007700, a007700]) \n",
    "F_vert = np.array([[3, 0, -3], [3, 0, -3], [3, 0, -3]])\n",
    "F_horiz = np.array([[3, 3, 3], [0, 0, 0], [-3, -3, -3]])  # f_vert.T\n",
    "\n",
    "def conv2D(X, F):\n",
    "    X_H, X_W = X.shape\n",
    "    f_H, f_W = F.shape\n",
    "\n",
    "    # Initialize Z based on dimension formula for height/width\n",
    "    Z_H = X_H - f_H + 1\n",
    "    Z_W = X_W - f_W + 1\n",
    "    Z = np.zeros((Z_H, Z_W))\n",
    "\n",
    "    # Update Z[m, n] based on various i,j iterations over X and f\n",
    "    for m in range(Z_H):\n",
    "        for n in range(Z_W):\n",
    "            for i in range(m, f_H + m):  # omitting -1 to include final value\n",
    "                for j in range(n, f_W + n):  # omitting -1 to include final value\n",
    "                    Z[m, n] += X[i, j] * F[i - m, j - n]\n",
    "                    #print(f\"{m=}, {n=}, {i=}, {j=}, X={X[i, j]}, f={F[i - m, j - n]}, Z={Z[m, n]}\")\n",
    "\n",
    "    return Z\n",
    "\n",
    "Z_vert = conv2D(X, F_vert)\n",
    "Z_horiz = conv2D(X, F_horiz)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=5)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, name in zip(ax, [\"X\", \"F_vert\", \"F_horiz\", \"Z_vert\", \"Z_horiz\"]):\n",
    "    matrix = locals()[name]\n",
    "    text = f\"{name} {matrix.shape}\"\n",
    "    axis.set_title(text); print(f\"\\n{text}\\n{matrix}\")\n",
    "    axis.imshow(matrix, cmap=\"gray\", vmin=-8, vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93444117-dfec-4102-b268-72251347f51a",
   "metadata": {},
   "source": [
    "#### Computing dJ/dZ, dJ/dA, dJ/dW, and dJ/db\n",
    "Computing $\\frac{\\partial J}{\\partial Z^{[l]}}$ is the same as an MLP as it requires an element-wise (Hadamard) product between the activation value gradients $\\frac{\\partial J}{\\partial A^{[l]}}$ and the derivative of the activation function $g^{[l]'}(Z^{[l]})$. Unlike MLP, computing $\\frac{\\partial J}{\\partial A^{[l-1]}}$ is not necessary to determine the weight derivatives at layer $l$ but is necessary for backwards propagation calculations at the previous layer. It consists of multiplying a given filter $W_c$ by individual scalar values within $\\frac{\\partial J}{\\partial Z^{[l]}}$, which is the exact same operation as forward propagation in a transverse convolutional layer. Unlike that operation, $A^{[l-1]}$ is required to compute the weights, so the exact code cannot be reused. Computing the weights for a given filter $\\frac{\\partial J}{\\partial W_c^{[l]}}$ multiplies the slice of previous-layer activations $A^{[l-1]}_{slice}$ (used to generate $Z^{[l]}$ during forward propagation) by the comparable values of $\\frac{\\partial J}{\\partial Z^{[l]}}$. Last, computing $\\frac{\\partial J}{\\partial b_c^{[l]}}$, the bias for a given filter, requires summing the values of $\\partial Z^{[l]}$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} & =  \\frac{\\partial J}{\\partial A^{[l]}} \\odot g^{[l]'}(Z^{[l]}) \\\\\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} & = \\sum\\limits_{i=1}^{n_H} \\sum\\limits_{j=1}^{n_W} W_c^{[l]} \\odot \\frac{\\partial J}{\\partial Z^{[l]}_{i,j}} \\\\\n",
    "\\frac{\\partial J}{\\partial W_c^{[l]}} & = \\sum\\limits_{i=1}^{n_H} \\sum\\limits_{j=1}^{n_W} A^{[l-1]}_{slice} \\odot \\frac{\\partial J}{\\partial Z^{[l]}_{i,j}}  \\\\\n",
    "\\frac{\\partial J}{\\partial b_c^{[l]}} & = \\sum\\limits_{i=1}^{n_H} \\sum\\limits_{j=1}^{n_W} \\frac{\\partial J}{\\partial Z^{[l]}_{i,j}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The matrices below support the example shown in code. Note that `W` is the same as `F` in other contexts, such as those describing forward propagation. Also note that `dZ` replaces `X` (or `A_l1`) in the context of  transposed convolutional forward propagation operations given the reversed directionality. For consistency, `db` is drawn as a matrix with shape $(1,1)$ but in reality, it's a per-filter scalar.\n",
    "\n",
    "$$\n",
    "A^{[l-1]} =\n",
    "\\begin{bmatrix}\n",
    "-1 & 6 & 4 & -1 \\\\\n",
    "2 & 2 & -1 & -2 \\\\\n",
    "-7 & 8 & 2 & 6 \\\\\n",
    "-4 & -2 & -6 & -3\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}\\frac{\\partial J}{\\partial Z^{[l]}} =\n",
    "\\begin{bmatrix}\n",
    "-5 & 1 \\\\\n",
    "2 & -2 \n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}W^{[l]}_c =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} =\n",
    "\\begin{bmatrix}\n",
    "-1 & 6 & 4 & -1 \\\\\n",
    "2 & 2 & -1 & -2 \\\\\n",
    "-7 & 8 & 2 & 6 \\\\\n",
    "-4 & -2 & -6 & -3\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}\\frac{\\partial J}{\\partial W_c^{[l]}} =\n",
    "\\begin{bmatrix}\n",
    "-11 & -20 & -19 \\\\\n",
    "-38 & 1 & -5 \\\\\n",
    "39 & -30 & -10\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}\n",
    "\\frac{\\partial J}{\\partial b_c^{[l]}} =\n",
    "\\begin{bmatrix}\n",
    "-4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69983f2-26f4-4f59-9ccc-6d547af2fcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_l1 = np.array([[-1, 6, 4, -1], [2, 2, -1, -2], [-7, 8, 2, 6], [-4, -2, -6, -3]])\n",
    "dZ = np.array([[-5, 1], [2, -2]])\n",
    "W = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]) # F_vert\n",
    "\n",
    "dA_l1 = np.zeros((A_l1.shape)) \n",
    "nH, nW = dZ.shape\n",
    "dW = np.zeros(W.shape)\n",
    "db = np.zeros((1, 1))\n",
    "\n",
    "# Update derivatives based on various i,j iterations over dZ\n",
    "for i in range(nH):\n",
    "    for j in range(nW):\n",
    "            vs = i * s; ve = vs + W.shape[0]\n",
    "            hs = j * s; he = hs + W.shape[1]\n",
    "            dA_l1[vs:ve, hs:he] += W * dZ[i, j]  # same logic as transconv forward prop\n",
    "            dW += A_l1[vs:ve, hs:he] * dZ[i, j]\n",
    "            db += dZ[i, j]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, name in zip(ax, [\"dA_l1\", \"dW\", \"db\"]):\n",
    "    matrix = locals()[name]\n",
    "    text = f\"{name} {matrix.shape}\"\n",
    "    axis.set_title(text); print(f\"\\n{text}\\n{matrix}\")\n",
    "    axis.imshow(matrix, cmap=\"gray\", vmin=-8, vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d17b2-2f04-4c70-92b3-8c015f9534f9",
   "metadata": {},
   "source": [
    "### Pooling Layer (POOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878e59e-6674-4ba7-94ce-3ea7d740a0ce",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Pooling is a mechanism to decrease the height and width of a matrix (downsample) by reducing all cells under the corresponding filter to a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d227b64c-dc58-441f-a910-44a3f029ddd8",
   "metadata": {},
   "source": [
    "#### Computing Z\n",
    "In a convolutional layer, multiple filters (feature selectors) can be applied. Each one is represented as a channel in the output matrix $Z^{[l]}$. Those filters always have the same depth as the input matrix $Z^{[l-1]}$. In contrast, pooling filters are always 2D matrices. They are applied individually to each channel of the input matrix $Z^{[l-1]}$, resulting in an output matrix $Z^{[l]}$ that has the same number of channels. As it relates to the output matrix $Z^{[l-1]}$, \"network in network\" filters of shape $(1,1,n_C^{[l-1]})$ can only shrink the number of channels $n_C^{[l]}$ while pooling can only shrink the height $n_H^{[l]}$ and width $n_H^{[l]}$. The techniques can powerfully combine to reform matrices within the network.\n",
    "\n",
    "Pooling layers seldom use padding ($p^{[l]}=0$), although Inception networks are an exception, which is discussed later. This reduces the convolutional dimension equations to the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n_H^{[l]} & = \\Bigl\\lfloor \\frac{n_H^{[l-1]} - f^{[l]}}{s^{[l]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_W^{[l]} & = \\Bigl\\lfloor \\frac{n_W^{[l-1]} - f^{[l]}}{s^{[l]}} \\Bigr\\rfloor + 1 \\\\\n",
    "n_C^{[l]} & = n_C^{[l-1]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Additionally, pooling does not have any trainable parameters. It has hyperparameters for the filter size $f^{[l]}$, stride $s^{[l]}$, and type of pooling. The two most commonly pooling types are described below:\n",
    "\n",
    "1.  __max__: \"Max pooling\" examines all input matrix $A^{[l-1]}$ cells within the filter's window. It then assigns the maximum value to the proper position in the output matrix $Z^{[l]}_{m,n}$. This technique tends to highlight key aspects with a given window while discarding less important aspects.\n",
    "\n",
    "2.  __average__: \"Average pooling\" examines all input matrix $A^{[l-1]}$ cells within the filter's window. It then assigns the mean (average) value to the proper position in the output matrix $Z^{[l]}_{m,n}$. This technique tends to equally summarize all of the cells within the filter's window.\n",
    "\n",
    "Because there are no trainable parameters for pooling layers, they are often considered \"part\" of the previous convolutional layers in literature. This document treats pooling as a separate layer to be consistent with a notation used for documenting MLP. Pooling layers also have their own formulas for backward propagation. This is only for the benefit of previous layers given the lack of trainable parameters.\n",
    "\n",
    "The code below demonstrates pooling using an input matrix similar to the previous example. Some shades of gray have been changed to differentiate between the types of pooling. Also, the stride is set to the filter size to eliminate any overlap of cell processing. This technique is not required, but is demonstrated for technical variety. Given an input matrix $X=Z^{[0]}$ of shape $(6,6)$, a pooling filter $f^{[l]}$ of shape $(2,2)$, and a stride $s^{[l]}$ of 2, the resulting matrix $Z^{[1]}$ will have shape $(3,3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093938e-952d-40c7-a0a4-0dda32b125a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a007700 = np.array([0, 0, 7, 7, 0, 0])\n",
    "a373573 = np.array([3, 7, 3, 5, 7, 3])\n",
    "X = np.array([a007700, a007700, a373573, a373573, a007700, a007700])\n",
    "f_size = 2\n",
    "print(f\"f_size: {f_size}\")\n",
    "\n",
    "def pool2D(X, f_size, pool_func):\n",
    "\n",
    "    s = f_size\n",
    "    X_H, X_W = X.shape\n",
    "\n",
    "    # Initialize Z based on number of times filter fits\n",
    "    Z_H = X_H // f_size\n",
    "    Z_W = X_W // f_size\n",
    "    Z = np.zeros((Z_H, Z_W))\n",
    "\n",
    "    # Update Z[m, n] based on various i,j iterations over X and f\n",
    "    for m in range(Z_H):\n",
    "        for n in range(Z_W):\n",
    "            for i in range(m, f_size + m, s):  # omitting -1 to include final value\n",
    "                for j in range(n, f_size + n, s):  # omitting -1 to include final value\n",
    "                    Z[m, n] = pool_func(X[i * s:i * s + f_size, j * s:j * s + f_size])\n",
    "                    # print(f\"{m=}, {n=}, {i=}, {j=}, X={X[i, j]}, Z={Z[m, n]}\")\n",
    "    return Z\n",
    "\n",
    "Z_max = pool2D(X, f_size, np.max)\n",
    "Z_avg = pool2D(X, f_size, np.mean)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, name in zip(ax, [\"X\", \"Z_max\", \"Z_avg\"]):\n",
    "    matrix = locals()[name]\n",
    "    text = f\"{name} {matrix.shape}\"\n",
    "    axis.set_title(text); print(f\"\\n{text}\\n{matrix}\")\n",
    "    axis.imshow(matrix, cmap=\"gray\", vmin=-8, vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570a564-173d-4988-849c-a34136231882",
   "metadata": {},
   "source": [
    "#### Computing dJ/dA given dJ/dZ\n",
    "For max pooling, only the largest value contributes to the overall cost. The remaining activations should have a gradient of 0. A bit mask representing the position of the maximum values from $A^{[l-1]}$ is multiplied by $\\frac{\\partial J}{\\partial Z^{[l]}}$ to form $\\frac{\\partial J}{\\partial A^{[l-1]}}$. This matrix has the same shape as $A^{[l-1]}$.\n",
    "\n",
    "$$\n",
    "A^{[l-1]} =\n",
    "\\begin{bmatrix}\n",
    "-1 & 6 & 4 & -1 \\\\\n",
    "2 & 2 & -1 & -2 \\\\\n",
    "-7 & 8 & 2 & 6 \\\\\n",
    "-4 & -2 & -6 & -3\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \\text{maxpool(s=2,f=2)} \\rightarrow\n",
    "Z^{[l]} =\n",
    "\\begin{bmatrix}\n",
    "6 & 4 \\\\\n",
    "8 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} =\n",
    "\\begin{bmatrix}\n",
    "0 & -5 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 2 & 0 & -2 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\leftarrow\\\n",
    "A^{[l-1]}_{mask} =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 1 \\\\\n",
    "0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "\\leftarrow\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} =\n",
    "\\begin{bmatrix}\n",
    "-5 & 1 \\\\\n",
    "2 & -2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "For average pooling, the flow is similar, except the bit mask is replaced by an even distribution across the values of $\\frac{\\partial J}{\\partial Z^{[l]}}$. The shape of this distribution matrix is the same shape as the filter. Given a filter with height $f_H=2$ and width $f_W=2$, the distribution matrix values all become $\\frac{1}{f_H \\cdot f_W} = \\frac{1}{4}$. This is then multiplied by values of $\\frac{\\partial J}{\\partial Z^{[l]}}$ to form $\\frac{\\partial J}{\\partial A^{[l-1]}}$.\n",
    "\n",
    "$$\n",
    "A^{[l-1]} =\n",
    "\\begin{bmatrix}\n",
    "-1 & 6 & 4 & -1 \\\\\n",
    "2 & 2 & -1 & -2 \\\\\n",
    "-7 & 8 & 2 & 6 \\\\\n",
    "-4 & -2 & -6 & -3\n",
    "\\end{bmatrix}\n",
    "\\rightarrow \\text{avgpool(s=2,f=2)} \\rightarrow\n",
    "Z^{[l]} =\n",
    "\\begin{bmatrix}\n",
    "2.25 & 0 \\\\\n",
    "-1.25 & -0.25\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}} =\n",
    "\\begin{bmatrix}\n",
    "-1.25 & -1.25 & 0.25 & 0.25 \\\\\n",
    "-1.25 & -1.25 & 0.25 & 0.25 \\\\\n",
    "0.5 & 0.5 & -0.5 & -0.5 \\\\\n",
    "0.5 & 0.5 & -0.5 & -0.5\n",
    "\\end{bmatrix}\n",
    "\\leftarrow\\\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}_{dist}} =\n",
    "\\begin{bmatrix}\n",
    "0.25 & 0.25 \\\\\n",
    "0.25 & 0.25\n",
    "\\end{bmatrix}\n",
    "\\leftarrow\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}} =\n",
    "\\begin{bmatrix}\n",
    "-5 & 1 \\\\\n",
    "2 & -2\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10a5d5-2ae2-44cc-aa4a-5b1f703ed801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward prop activations from previous layer (via cache) and backprop dJ/dZ are inputs\n",
    "A_l1 = np.array([[-1, 6, 4, -1], [2, 2, -1, -2], [-7, 8, 2, 6], [-4, -2, -6, -3]])\n",
    "dZ = np.array([[-5, 1], [2, -2]])\n",
    "nH, nW = dZ.shape\n",
    "s, f = 2, 2\n",
    "\n",
    "dA_l1_dict = {\"max\": np.zeros(A_l1.shape), \"avg\": np.zeros(A_l1.shape)}\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3)\n",
    "fig.set_size_inches(12, 4)\n",
    "print(f\"dZ\\n{dZ}\\n\")\n",
    "print(f\"A_l1\\n{A_l1}\\n\")\n",
    "ax[0].set_title(\"$A^{[l-1]}$ \" + str(A_l1.shape))\n",
    "ax[0].imshow(A_l1, cmap=\"gray\", vmin=-8, vmax=7)\n",
    "\n",
    "for (mode, dA_l1), axis in zip(dA_l1_dict.items(), ax[1:]):\n",
    "\n",
    "    # Loop over each element of dZ\n",
    "    for i in range(nH):\n",
    "        for j in range(nW):\n",
    "            vs = i * s; ve = vs + f\n",
    "            hs = j * s; he = hs + f\n",
    "            A_l1_slice = A_l1[vs:ve, hs:he]\n",
    "\n",
    "            # Compute binary mask matrix of shape (f,f)\n",
    "            if mode == \"max\":\n",
    "                mask = (A_l1_slice == np.max(A_l1_slice))\n",
    "\n",
    "            # Compute mask value (scalar float)\n",
    "            elif mode == \"avg\":\n",
    "                mask = 1 / (f**2)\n",
    "\n",
    "            # Add product of mask (max or avg) and dZ values to dA_l1.\n",
    "            dA_l1[vs:ve, hs:he] += mask * dZ[i, j]\n",
    "        \n",
    "    print(f\"dA_l1 {mode}\\n{dA_l1}\\n\")\n",
    "    axis.set_title(\"$dJ/dA^{[l-1]}$ \" + f\"{mode} {dA_l1.shape}\")\n",
    "    axis.imshow(dA_l1, cmap=\"gray\", vmin=-8, vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c602e3-1c72-44c8-bfa8-c7a20fd0f445",
   "metadata": {},
   "source": [
    "### Fully Connected Layer (FC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a7f2e-521d-4ebb-a0f9-ec9d9559a457",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This is a \"dense\" layer resembling those within an MLP. The input features must be unrolled/flattened, effectively applying: $n_H^{[l]} \\times n_W^{[l]} \\times n_C^{[l]} = n^{[l]}$. These are typically introduced towards the end of the CNN to make classification predictions, either binary classifications (sigmoid) or multiple classifications (softmax). By this point in the network, the number of units per layer is relatively small, such as on the order of hundreds rather than thousands. This is significantly reduces the dimensions of the weights $W^{[l]}$ and biases $b^{[l]}$ at these dense layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df63d33-bd4e-4dff-a56b-4d222138efc4",
   "metadata": {},
   "source": [
    "### Tranpose Convolution (TRANSCONV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ee2b0-969b-4e64-acec-63d83cbe25e8",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Valid-padded convolution and pooling techniques tend to decrease the height and width of volume while increasing the number of channels (depth). Transpose convolution, sometimes called de-convolution or un-convolution, reverses this process. This is useful when implementing __semantic segmentation__, enabling precise per-pixel classifications rather than using boundary boxes or landmarks. The U-Net architecture makes extensive use of this technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64564d12-a62f-48c4-bdbe-698db0ee08c6",
   "metadata": {},
   "source": [
    "#### Computing Z\n",
    "Each value of the previous layer's activations $A^{[l-1]}$ are multiplied by all of the filter's values in element-wise (broadcasted) fashion, resulting in a new matrix of shape $(f^{[l]}, f^{[l]})$, the same as the filter. This new matrix is overlaid onto $Z^{[l]}$ based on the stride and padding configured. If padding is applied, the padded cells surrounding $Z^{[l]}$ act as sinks. Those cells are only used for alignment with the projected matrix, but don't retain any values. The example below assumes valid padding $(p^{[l]}=0)$ with the minimum stride $(s^{[l]}=1)$.\n",
    "\n",
    "Let $F$ be the filter matrix at layer $l$ of shape $(f^{[l]}, f^{[l]})$. Let $A^m_n=A^{[l-1]}_{m,n}$ to represent the previous layer's activations at a specific cell at row $m$ and column $n$, starting at 0. Assume $A$ has shape $(2,2)$. Both notations are compressed due to page width limitations. The values in the corners are visited once, those on the edges twice, and those in the center 4 times. An interesting property of this generic matrix is that the sum of the superscripts (row) and subscripts (column) for each $A \\cdot F$ quantity equals the cell location of $Z$. For example, at $Z_{1,1}$ there are 4 quantities that comprise the cell's value:\n",
    "\n",
    "1. $A^0_0 \\cdot F^1_1$: $0,0 + 1,1 = 1,1$\n",
    "2. $A^0_1 \\cdot F^1_0$: $0,1 + 1,0 = 1,1$\n",
    "3. $A^1_0 \\cdot F^0_1$: $1,0 + 0,1 = 1,1$\n",
    "4. $A^1_1 \\cdot F^0_0$: $1,1 + 0,0 = 1,1$\n",
    "\n",
    "$$\n",
    "A^{[l-1]} = A^{row}_{col} =\n",
    "\\begin{bmatrix}\n",
    "A^0_0 & A^0_1 \\\\\n",
    "A^1_0 & A^1_1\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}F^{[l]} = F^{row}_{col} =\n",
    "\\begin{bmatrix}\n",
    "F^0_0 & F^0_1 & F^0_2 \\\\\n",
    "F^1_0 & F^1_1 & F^1_2 \\\\\n",
    "F^2_0 & F^2_1 & F^2_2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{[l]} =\n",
    "\\begin{bmatrix}\n",
    "% first row\n",
    "(A^0_0 \\cdot F^0_0) &\n",
    "(A^0_0 \\cdot F^0_1 + A^0_1 \\cdot F^0_0) &\n",
    "(A^0_0 \\cdot F^0_2 + A^0_1 \\cdot F^0_1) &\n",
    "(A^0_1 \\cdot F^0_2) \\\\\n",
    "% second row\n",
    "(A^0_0 \\cdot F^1_0 + A^1_0 \\cdot F^0_0) &\n",
    "(A^0_0 \\cdot F^1_1 + A^0_1 \\cdot F^1_0 + A^1_0 \\cdot F^0_1 + A^1_1 \\cdot F^0_0) &\n",
    "(A^0_0 \\cdot F^1_2 + A^0_1 \\cdot F^1_1 + A^1_0 \\cdot F^0_2 + A^1_1 \\cdot F^0_1) &\n",
    "(A^0_1 \\cdot F^1_2 + A^1_1 \\cdot F^0_2) \\\\\n",
    "% third row\n",
    "(A^0_0 \\cdot F^2_0 + A^1_0 \\cdot F^1_0) &\n",
    "(A^0_0 \\cdot F^2_1 + A^0_1 \\cdot F^2_0 + A^1_0 \\cdot F^1_1 + A^1_1 \\cdot F^1_0) &\n",
    "(A^0_0 \\cdot F^2_2 + A^0_1 \\cdot F^2_1 + A^1_0 \\cdot F^1_2 + A^1_1 \\cdot F^1_1) &\n",
    "(A^0_1 \\cdot F^2_2 + A^1_1 \\cdot F^1_2) \\\\\n",
    "% fourth row\n",
    "(A^1_0 \\cdot F^2_0) &\n",
    "(A^1_0 \\cdot F^2_1 + A^1_1 \\cdot F^2_0) &\n",
    "(A^1_0 \\cdot F^2_2 + A^1_1 \\cdot F^2_1) &\n",
    "(A^1_1 \\cdot F^2_2) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This formula is a good approximation of the aforementioned algorithm. The logic is similar to computing $\\frac{\\partial J}{\\partial A^{[l-1]}}$ for a regular convolutional layer during backwards propagation. The difference is that $Z^{[l]}$, initialized to all zeros, is updated by the element-wise product of the filter $F^{[l]}$ and previous layer's activation values at a given cell $A^{[l-1]}_{i,j}$. These cells were depicted as $A^i_j$ above.\n",
    "\n",
    "$$\n",
    "Z^{[l]} \\mathrel{+}= \\sum\\limits_{i=1}^{n^{[l-1]}_H} \\sum\\limits_{j=1}^{n^{[l-1]}_W} F^{[l]} \\odot A^{[l-1]}_{i,j}\n",
    "$$\n",
    " \n",
    "The specific example implemented in the following code is depicted below using the same 16-bit grayscale method seen in previous examples.\n",
    "\n",
    "$$\n",
    "X =\n",
    "\\begin{bmatrix}\n",
    "0 & 4 \\\\\n",
    "0 & 4\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}F^{[1]}_{vert}  =\n",
    "\\begin{bmatrix}\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1 \\\\\n",
    "-1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}Z^{[1]}_{vert} =\n",
    "\\begin{bmatrix}\n",
    "0 & -4 & 0 & 4 \\\\\n",
    "0 & -8 & 0 & 8 \\\\\n",
    "0 & -8 & 0 & 8 \\\\\n",
    "0 & -4 & 0 & 4\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}F^{[1]}_{horiz}  =\n",
    "\\begin{bmatrix}\n",
    "-1 & -1 & -1 \\\\\n",
    "0 & 0 & 0 \\\\\n",
    "1 & 1 & 1\n",
    "\\end{bmatrix}\n",
    "\\hspace{35pt}Z^{[1]}_{horiz} =\n",
    "\\begin{bmatrix}\n",
    "0 & -4 & -4 & -4 \\\\\n",
    "0 & -4 & -4 & -4 \\\\\n",
    "0 & 4 & 4 & 4 \\\\\n",
    "0 & 4 & 4 & 4\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9c7539-e8ed-40db-86ee-4ac89ccbcbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 4], [0, 4]])\n",
    "F_vert = np.array([[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]])\n",
    "F_horiz = np.array([[-1, -1, -1], [0, 0, 0], [1, 1, 1]])\n",
    "\n",
    "def transconv2D(X, F):\n",
    "    fh, fw = F.shape\n",
    "    xh, xw = X.shape\n",
    "    Z = np.zeros((xh + fh - 1, xw + fw - 1))\n",
    "    for i in range(xh):\n",
    "        for j in range(xw):\n",
    "            Z[i: i + fh, j: j + fw] += X[i, j] * F\n",
    "    return Z\n",
    "\n",
    "Z_vert = transconv2D(X,F_vert)\n",
    "Z_horiz = transconv2D(X,F_horiz)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=5)\n",
    "fig.set_size_inches(12, 4)\n",
    "for axis, name in zip(ax, [\"X\", \"F_vert\", \"Z_vert\", \"F_horiz\", \"Z_horiz\"]):\n",
    "    matrix = locals()[name]\n",
    "    text = f\"{name} {matrix.shape}\"; print(f\"\\n{text}\\n{matrix}\")\n",
    "    axis.set_title(text)\n",
    "    axis.imshow(matrix, cmap=\"gray\", vmin=-8, vmax=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69ec62-32df-4270-8fee-73fd1ccbb80d",
   "metadata": {},
   "source": [
    "#### Computing dJ/dA given dJ/dZ\n",
    "\n",
    "Just as the forward propagation step of computing Z in a transpose convolutional layer is similar to the backwards propagation step of computing dA, so too is the reverse. Given $\\frac{\\partial J}{\\partial Z^{[l]}}$, apply the forward propagation convolutional layer step to get $\\frac{\\partial J}{\\partial A^{[l-1]}}$.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial A^{[l-1]}_{m,n}} = \\sum\\limits^{f^{[l]}_H-1+m}_{i=m} \\sum\\limits^{f^{[l]}_W-1+n}_{j=n}\n",
    "\\frac{\\partial J}{\\partial Z^{[l]}_{i,j}} \\odot F^{[l]}_{i-m, j-n}\n",
    "$$\n",
    "\n",
    "__TODO__: What about weights and biases? Are they trainable here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb789d2-b4d3-4a01-b05f-e70cd6b88638",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa1617e-d9a5-43f9-8309-1104ac7ca89d",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section discusses algorithms and strategies for detecting objects within an image using bounding boxes, landmarks, and more. Identifying the specific area occupied by an object of interest it is sometimes called __localization__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b0530-4459-433a-ae92-fc91ac3d2e58",
   "metadata": {},
   "source": [
    "#### Bounding Boxes and Landmarks\n",
    "When training a CNN, the basic structure of $Y$ changes. A __bounding box__ attempts to draw a rectangle around the object of interest .For a binary classification problem, there are 5 values for each $y^{(m)}$ column vector. $P_c$, short for \"probability of classification\", is the sigmoid activation output to determine whether the object of interest is contained in the image or not. The next 4 values determine the rectangle's shape. $b_x$ and $b_y$ represent the position at the center of the box relative to the image sized, scaled to values between 0 and 1. $b_h$ and $b_w$ measure the ratio of the bounding box compared to image size. For example, a tall and skinny object (like a person) will occupy a larger share of the image's height compared to its width assuming the image with square-shaped.\n",
    "\n",
    "$$\n",
    "y_{singleclass} (5, m) =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_{P_c} & y^{(2)}_{P_c} & \\cdots & y^{(m)}_{P_c} \\\\\n",
    "y^{(1)}_{b_x} & y^{(2)}_{b_x} & \\cdots & y^{(m)}_{b_x} \\\\\n",
    "y^{(1)}_{b_y} & y^{(2)}_{b_y} & \\cdots & y^{(m)}_{b_y} \\\\\n",
    "y^{(1)}_{b_h} & y^{(2)}_{b_h} & \\cdots & y^{(m)}_{b_h} \\\\\n",
    "y^{(1)}_{b_w} & y^{(2)}_{b_w} & \\cdots & y^{(m)}_{b_w}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If there are multiple classes, additional class variables are neede to specify the type of object contained in the bounding box. If there are 3 classes (e.g., cat, dog, human), then let $k=3$. Thus, $k$ extra class probabilities are appended to $y^{(m)}$. Now, $P_c$ only answers the question as to whether there are any objects of interest in the image. The matrix below generalizes the structure.\n",
    "\n",
    "$$\n",
    "y_{multiclass} (5 + k, m) =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_{P_c} & y^{(2)}_{P_c} & \\cdots & y^{(m)}_{P_c} \\\\\n",
    "y^{(1)}_{b_x} & y^{(2)}_{b_x} & \\cdots & y^{(m)}_{b_x} \\\\\n",
    "y^{(1)}_{b_y} & y^{(2)}_{b_y} & \\cdots & y^{(m)}_{b_y} \\\\\n",
    "y^{(1)}_{b_h} & y^{(2)}_{b_h} & \\cdots & y^{(m)}_{b_h} \\\\\n",
    "y^{(1)}_{b_w} & y^{(2)}_{b_w} & \\cdots & y^{(m)}_{b_w} \\\\\n",
    "y^{(1)}_{P_{c1}} & y^{(2)}_{P_{c1}} & \\cdots & y^{(m)}_{P_{c1}} \\\\\n",
    "y^{(1)}_{P_{c2}} & y^{(2)}_{P_{c2}} & \\cdots & y^{(m)}_{P_{c2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(1)}_{P_{ck}} & y^{(2)}_{P_{ck}} & \\cdots & y^{(m)}_{P_{ck}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A less common alternative to bounding boxes is __landmarks__. These are specific points on the image specified by $(x,y)$ position relative to the image, similar to the center of a bounding box. Note that when training a CNN with landmark labels, the landmarks specified in $Y$ must be consistently positioned. For example, the first landmark $l_1$ must always be above the left eye, while the second landmark $l_2$ must always be above the right eye. Now, the length of each $y^{(m)}$ column vector becomes a function of both of the number of classes $k$ and the number of two-value landmarks $2l$. Landmarks are often used to modify or enhance images, like adding a \"hat\" or \"sunglasses\" filter on a picture-sharing app.\n",
    "\n",
    "$$\n",
    "y_{multiclass} (1 + 2l + k, m) =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}_{P_c} & y^{(2)}_{P_c} & \\cdots & y^{(m)}_{P_c} \\\\\n",
    "y^{(1)}_{l_{1x}} & y^{(2)}_{l_{1x}} & \\cdots & y^{(m)}_{l_{1x}} \\\\\n",
    "y^{(1)}_{l_{1y}} & y^{(2)}_{l_{1y}} & \\cdots & y^{(m)}_{l_{1y}} \\\\\n",
    "y^{(1)}_{l_{2x}} & y^{(2)}_{l_{2x}} & \\cdots & y^{(m)}_{l_{2x}} \\\\\n",
    "y^{(1)}_{l_{2y}} & y^{(2)}_{l_{2y}} & \\cdots & y^{(m)}_{l_{2y}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(1)}_{P_{c1}} & y^{(2)}_{P_{c1}} & \\cdots & y^{(m)}_{P_{c1}} \\\\\n",
    "y^{(1)}_{P_{c2}} & y^{(2)}_{P_{c2}} & \\cdots & y^{(m)}_{P_{c2}} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y^{(1)}_{P_{ck}} & y^{(2)}_{P_{ck}} & \\cdots & y^{(m)}_{P_{ck}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that when $P_c=0$, none of the other values matter. There are no bounding boxes to draw or landmarks to identify. Since the image contains no objects of interest, the probabilities for the individual classes ${P_{ck}}$ also don't matter. The CNN will ignore these values on input during training (evaluating $Y$) and will use random noise as output when making predictions (generating $\\hat{Y}$)\n",
    "\n",
    "With respect to cost functions, a hybrid approach is sometimes used. Although mean squared error (MSE) will likely perform tolerably if used across all of $y^{(m)}$, it is common to use a cross entropy variant for the probability values. If $P_c=0$ on a training example, only the loss between $P_c$ components of $Y$ and $\\hat{Y}$ matter, much like logistic regression, because the other values are noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77915e-9e67-4ea9-9d72-f1931e80c546",
   "metadata": {},
   "source": [
    "#### Sliding Window\n",
    "This algorithm uses a filter (the sliding window) that pan an image, similar to the motion of a convolutional filter.  In order to determine whether the object under the sliding window matches the object of interest, you typically train a CNN using closely-cropped images, containing those objects of interest. That CNN will make predictions for each sub-image under the sliding window. If the desired object is seen within that filter, the algorithm returns 1, otherwise it returns 0. This process repeats with different values of $f$ to control filter size and $s$ to control stride size.\n",
    "\n",
    "This process is computationally expensive given that the CNN must make many predictions for every sub-image under many variable-sized sliding windows. One optimization is to replace the fully-connected layers with convolutional layers. To do that, set the number of filters is equal to the number of units previously contained in the fully-connected layer ($n_C^{[l]}$). Also, set the height and width of the filters equal to the dimensions of the previous layer's outputs $(f_H^{[l]} = n_H^{[l-1]}, f_W^{[l]} = n_W^{[l-1]})$. This yields an output of shape $(1,1,n_C^{[l]})$, which closely resembles the shape of the fully-connected layer. This is more computationally efficient as it shares computations better when sliding window regions overlap. Said another way, the filter doesn't iteratively \"pan and compute\" in isolation using a convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410ec3e2-d97f-44e1-9475-63834f8fd478",
   "metadata": {},
   "source": [
    "#### You Only Look Once (YOLO)\n",
    "This algorithm breaks an image into a grid, make it easier to identify precise bounding boxes for objects of interest detected. Objects within the image are assigned to a single cell within the grid. The upper left corner of the image is considered the origin at position $(0,0)$ in the lower right is position $(1,1)$. This way, all positional values are between 0 and 1. Supposing there are 3 possible classes of interesting objects, the training labels for a grid cell will have 8 elements to account for the probability of any object of interest being present in the grid cell ($P_c$), the bounding box location measurements $(b_x, b_y, b_h, b_w)$, and the probabilities of each class $(P_{c1}, P_{c2}, P_{c3})$. These per-class probabilities are typically one-hot encoded for training, better aligning to the softmax-like probabilities generated for each class during prediction. This process operates on a per grid cell basis, so supposing the grid was $10 \\times 10$, each input image would be represented by 100 vectors of length 8. The output volume's shape from the algorithm would therefore be $(10,10,8)$.\n",
    "\n",
    "To evaluate the accuracy of a bounding box prediction is to use the __Intersection over Union (IoU)__ method. This computes the area of the bounding boxes supplied by $Y$ and $\\hat{Y}$ for a given training example. Next, it divides the intersection of the areas by the union of the areas, then compares the quotient to a threshold. If the ratio is greater than 0.5, it is commonly considered a good prediction. Since this is a human-chosen threshold, raising it will lead to more accurate bounding box predictions.\n",
    "\n",
    "When using a finely-grained grid, many adjacent cells may claim to see an object of interest ($P_c \\ge 0.5$), even if they only see a part of it. YOLO requires the object to be assigned to only one cell even if the bounding box spans multiple cells. __Non-max suppression__ compares all of the $P_c$ values and summarily discards those less than a threshold, say 0.6 for example. These cells don't contain any part of the object and don't require further evaluation. Then, the algorithm finds the cell with the highest $P_c$ value and discards adjacent abounding boxes with a high IoU (say, 0.5) with respect to the best bounding box. The algorithm runs independently for each class to get the final prediction. As described by the name, this enhancement suppresses any boxes that are not \"maximal\", leading to a single answer.\n",
    "\n",
    "Sometimes there are multiple objects of different classes within the same grid cell. YOLO can use __anchor boxes__ to classify these objects independently. Each anchor box is represented by its own set of labels in $Y$. For example, if there are 3 classes, there will be 8 values in the the $y$ column vector associated with a single cell. If 2 separate anchor boxes are adequate to capture these 3 classes, there will be 16 values in $y$, one for each anchor box. Objects are now assigned to a specific anchor box within a specific grid cell. The anchor box to which an object is assigned is based on the highest computed IoU. Given a $10 \\times 10$ grid, the resulting volume has shape $(10,10,16)$. Sometimes this is expressed as a 4 dimensional shape which independently accounts for the number of anchor boxes (2), and the values within each anchor box (8), such as $(10,10,2,8)$. This structure is more explicit but is harder to visualize; this book prefers the consolidated 3D format.\n",
    "\n",
    "You can determine anchor box sizes manually if you have an intuition regarding their dimensions and quantity. Alternatively, you can use the k-means unsupervised learning algorithm to discover clusters of objects, then create the appropriate anchor boxes. Anchor boxes are defined only by their height and width, not their position within a grid cell, unlike a bounding box which has all 4.. Note that if the number of object shapes is greater than the number of anchor boxes, the algorithm performs poorly. Also, if several classes have the same anchor box shape, the algorithm is forced to select only one, also leading to poor performance. In practice, multiple objects appearing in the same cell (assuming a finely-grained grid) is relatively rare.\n",
    "\n",
    "The matrix below visualizes the structure just described using $?$ to represent random noise. Each item below describes a corresponding column in the matrix from left to right. Discrete numbers are plausible sample values.\n",
    "\n",
    "1. Notation reference using superscripts $[A]$ and $[B]$ for anchor box identifiers.\n",
    "2. Cell contains an object for anchor box A (class 1) and anchor box B (class 2).\n",
    "3. Cell contains an object for anchor box A (class 2) but not anchor box B.\n",
    "4. Cell contains an object for anchor box B (class 3) but not anchor box A.\n",
    "5. Cell contains no objects of interest.\n",
    "\n",
    "$$\n",
    "Y =\n",
    "\\begin{bmatrix}\n",
    "y^{[A]}_{P_c} & 1 & 1 & 0 & 0 \\\\\n",
    "y^{[A]}_{b_x} & 0.5 & 0.4 & ? & ? \\\\\n",
    "y^{[A]}_{b_y} & 0.2 & 0.7 & ? & ? \\\\\n",
    "y^{[A]}_{b_h} & 1.4 & 2.1 & ? & ? \\\\\n",
    "y^{[A]}_{b_w} & 0.8 & 0.3 & ? & ? \\\\\n",
    "y^{[A]}_{P_{c1}} & 1 & 0 & ? & ? \\\\\n",
    "y^{[A]}_{P_{c2}} & 0 & 1 & ? & ? \\\\\n",
    "y^{[A]}_{P_{c3}} & 0 & 0 & ? & ? \\\\\n",
    "y^{[B]}_{P_c} & 1 & 0 & 1 & 0 \\\\\n",
    "y^{[B]}_{b_x} & 0.3 & ? & 0.5 & ? \\\\\n",
    "y^{[B]}_{b_y} & 0.6 & ? & 0.7 & ? \\\\\n",
    "y^{[B]}_{b_h} & 1.1 & ? & 1.6 & ? \\\\\n",
    "y^{[B]}_{b_w} & 2.4 & ? & 0.9 & ? \\\\\n",
    "y^{[B]}_{P_{c1}} & 0 & ? & 0 & ? \\\\\n",
    "y^{[B]}_{P_{c2}} & 1 & ? & 0 & ? \\\\\n",
    "y^{[B]}_{P_{c3}} & 0 & ? & 1 & ?\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af0be4f-2351-488e-b1ea-715d44bd5b88",
   "metadata": {},
   "source": [
    "### Face Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a614b4e-7b01-4a82-be1c-85e905333881",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "The section describes facial verification and recognition text. A new concept is the __siamese network__, which is a CNN that evaluates pairs of images concurrently. The weights and biases are updated on pairs of images rather than individual images. As the network is trained, each training example $x^{(i)}$ will consist of 2 images. The corresponding labels $y^{(i)}$ set to $1$ if the images are the same person and $0$ otherwise.\n",
    "\n",
    "__Face verification__ asks the question \"does this person's face match the expected one?\" It compares encodings of the picture taken in the moment with an existing database picture, both of which are supplied to the function. If a similarity threshold is met, the person is verified.\n",
    "\n",
    "__Face recognition__ asks the question \"who is this person?\" It iterates overall names in the database and measures the similarity to the picture taken in the moment. The smallest difference (greatest similarity) wins.\n",
    "\n",
    "The literature often uses $f(x)$ notation to represent a prediction from the model, which is equivalent to the activations at the final layer $A^{[L]}$. The square of the Frobenius norm represents the difference between the two operands, which is how the loss is measured between two images.\n",
    "\n",
    "$$\n",
    "\\| f(x^{(i)}) - f(x^{(j)}) \\|_F^2 = \\| A^{[L](i)} - A^{[L](i)} \\|_F^2 \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33656b8-f066-463d-99b0-4d9a38f011ea",
   "metadata": {},
   "source": [
    "#### Using Triplet Loss\n",
    "Triplet loss compares the two separate predictions using a mix of new formulas and hyperparameters. It was defined in the Facenet paper by Florian Schroff et al in 2015. You can read the details [here](https://arxiv.org/pdf/1503.03832). \n",
    "\n",
    "Each comparison triplet includes three images.\n",
    "\n",
    "1. anchor ($x_a$): the base image containing a person with which other images are compared\n",
    "2. positive ($x_p$): another image depicting the same person as the anchor\n",
    "3. negative ($x_n$): another image depicting someone who is not the anchor\n",
    "\n",
    "Note that the Facenet paper uses notation $x^a_i$ to indicate the anchor image $a$ of a triplet for the $i$-th training example. This section adjusts that notation to $x^{(i)}_a$ for consistency with the rest of the document. The same is true for $p$ and $n$ images.\n",
    "\n",
    "Broadly speaking, the goal is to measure the difference between the anchor and positive, then between the anchor and negative. The first quantity ($a-p$) should be small while the second quantity ($a-n$) should be large. In the context of triplet loss, $\\alpha$ is a hyperparameter called \"margin\". Without $\\alpha$ (or setting it to 0), the algorithm can just set both quantities to 0 for a trivial/useless solution. The algorithm works best when the two quantities are approximately equal so that the algorithm has something to adjust.\n",
    "\n",
    "$$\n",
    "\\| f(x_a) - f(x_p) \\|_F^2 - \\| f(x_a) - f(x_n) \\|_F^2 + \\alpha \\le 0\n",
    "$$\n",
    "\n",
    "Differences will often be large when comparing two random people. Each mini-batch should contain several positive examples along with \"semi-hard\" (ie, low loss) negative examples. The superlatively hard negatives (two people who look very similar) should be avoided as it can lead to local minima by collapsing all operands to 0. The dataset must contain multiple images of same person (e.g., 5,000 images of 500 people). The total cost $J$ is the sum of the losses $L$ for each $m$ training example, which applies a ReLU-like operation to raise all negative costs to 0. Said another way, when the difference between the anchor/positive and the anchor/negative is less than the margin, face verification succeeded and there is no loss.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(x_a,x_p,x_n) & = \\| f(x_a) - f(x_p) \\|_F^2 - \\| f(x_a) - f(x_n) \\|_F^2 + \\alpha \\\\\n",
    "J(x_a,x_p,x_n) & = \\sum\\limits^m_{i=1}\n",
    "\\begin{cases}\n",
    "L(x^{(i)}_a, x^{(i)}_p, x^{(i)}_n) \\le 0: & 0 \\\\\n",
    "L(x^{(i)}_a, x^{(i)}_p, x^{(i)}_n) \\gt 0: & L(x^{(i)}_a, x^{(i)}_p, x^{(i)}_n)\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c560d6-c555-480e-88de-958555337aba",
   "metadata": {},
   "source": [
    "#### Using Binary Classification\n",
    "As an alternate to triplet loss, you can use binary classification. This solution requires passing $f(x^{(i)}$ and $f(x^{(j)}$ into a  logistic regression output layer. Assuming the final fully-connected layer has $L_n$ units, this measures the absolute value of the difference between the image distances, then multiplies by a weight $w_k$ and adds a bias $b$. That result is passed into the sigmoid activation function $\\sigma$ to make a prediction $\\hat{y}$. Note that the cost function is typically binary cross entropy, whereas triplet loss defined a new cost function.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma\\bigl(\\sum\\limits_{k=1}^{L_n} w_k \\cdot |f(x^{(i)})_k - f(x^{(j)})_k| + b\\bigr)\n",
    "$$\n",
    "\n",
    "An alternative implementation for this option is to use the $\\chi^2$ similarity method.\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\sigma\\bigl(\\sum\\limits_{k=1}^{L_n} w_k \\cdot \\frac{(f(x^{(i)})_k  f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k} + b\\bigr)\n",
    "$$\n",
    "\n",
    "Optionally, you can speed up the prediction process by pre-computing $f(x^{(j)})$ (the image from the database) for quicker comparison to $f(x^{(i)})$ (person being evaluated right now) which is computed in real-time. This optimization technique also works for the triplet loss option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21d2eef-2168-4e9a-bfaa-adb0c30bd07c",
   "metadata": {},
   "source": [
    "### Neural Style Transfer (NST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2638a4-99a2-471a-b4f5-60753da44cb4",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This technique allows for two separate images to be supplied as input for a newly-generated output image using a pre-trained model. The content image $C$ depicts the objects to be included in the output $O$ image. The style image $S$ depicts the style in which the content $C$ should be depicted. For example, a picture of a car (C) drawn in the style of van Gogh's \"The Starry Night\" (S). Note that the output image is often written as $G$ for \"generated\", but the common notation overlaps with the $G$ of \"Gram matrix\" described later. This document uses the separate notation of $O$ for \"output\" while retaining $G$ for Gram matrices to reduce confusion.\n",
    "\n",
    "The general process is:\n",
    "1. Select a pre-trained model, such as VGG-19.\n",
    "2. Select a layer, or set of layers, upon which to focus.\n",
    "    1. Shallower layers typically detect basic features such as edges and simple textures.\n",
    "    2. Deeper layers typically detect rich features such as complex textures and object classes.\n",
    "4. Initialize $O$ to the proper dimensions using random noise.\n",
    "5. Define cost function and run gradient descent to train pixels of the image (not training parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5134c1c7-c306-4f98-a352-b45941d2a2ae",
   "metadata": {},
   "source": [
    "#### Cost Function\n",
    "\n",
    "The cost is the sum of the content cost function $J_c$ and style cost function $J_s$. These are scaled by the hyperparameters $\\alpha$ and \\$beta$, respectively. Larger values of these parameters assign a higher level of importance to each cost. Additionally, this document defines $n_T^{[l]}$, which is the product of all 3 dimensions in the output activation volume. This simplifies the notation when describing the cost functions.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J(G) & = \\alpha \\cdot J_c(C,O) + \\beta \\cdot J_s(S,O) \\\\\n",
    "n_T^{[l]} & = n_H^{[l]} \\cdot n_W^{[l]} \\cdot n_C^{[l]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The content cost function $J_c$ is shown below, which is a function of the content image $C$ and the output ($O$) image. The 3D volumes of activations are unrolled/flattened into 1D vectors $\\vec{a}$. Then, the square of the L2-norm is applied. Note that the final squaring operation affectively removes the square root of the L2-norm. The value $n_T^{[l]}$ is used as the summation limit as this is the total number of iteratively-processed elements in the volume. Some implementations will divide this result by 2 (usually by prepending $\\frac{1}{2}$), but this is unnecessary as the hyperparameter $\\alpha$ can scale it. It is included for completeness only.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_c^{[l]}(C,O) & = \\frac{1}{2} \\cdot \\| \\vec{a}^{[l](C)} - \\vec{a}^{[l](O)} \\|_2^2 \\\\\n",
    "J_c^{[l]}(C,O) & = \\frac{1}{2} \\cdot \\sum\\limits_{i=0}^{n_T^{[l]}} \\bigl(\\vec{a}^{[l](C)}_i - \\vec{a}^{[l](O)}_i \\bigr)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The style cost function $J_s$ is shown below, which is a function of the content image $C$ and the style ($S$) image. Similar to the content cost function $J_c$, it relies on the sum of square differences, except technically using the Frobenius norm as the operands are matricies. It also includes a scaling value, just it can be omitted since the hyperparameter $\\beta$ can be adjusted. In this context, $G$ refers to the \"Gram matrix\" which is always square-shaped with dimensions $(n_C^{[l]},n_C^{[l]})$.\n",
    "\n",
    "$$\n",
    "J_s^{[l]}(S,O) = \\frac{1}{(2 \\cdot n_T^{[l]})^2}\\| G^{[l](S)} - G^{[l](O)} \\|_F^2\n",
    "$$\n",
    "\n",
    "Unpacking the norm expression to build the Gram maticies, $k$ and $k'$ refer to different channels in the volume, computing \"correlations\" (technically the unnormalized cross covariance because the mean $\\mu$ is not used) between channel pairs. The value $G^{[l](M)}_{k,k'}$ specifies the value at cell $(k,k')$ as the product of the corresponding activations. $M$ is a variable to signify the matrix input, which can be either the style matrix $S$ or the output matrix $O$. With this intermediate equation defined, the original style cost function $J_s$ can be expanded.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "G^{[l](M)}_{k,k'} & = \\sum\\limits^{n_H^{[l]}}_{i=1} \\sum\\limits^{n_W^{[l]}}_{j=1} \\vec{a}^{[l](M)}_{i,j,k} \\cdot \\vec{a}^{[l](M)}_{i,j,k'} \\\\\n",
    "J_s^{[l]}(S,O) & = \\frac{1}{(2 \\cdot n_T^{[l]})^2}\n",
    "\\sum\\limits_{k=1}^{n_C^{[l]}}\\sum\\limits_{k'=1}^{n_C^{[l]}}\\bigl(G^{[l](S)}_{k,k'} - G^{[l](O)}_{k,k'}\\bigr)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For best results, compute the style cost across multiple layers and scale it using a per-layer hyperparameter $\\lambda$. The initial and final values of $l$ in the summation vary based on which layers are selected for style evaluation.\n",
    "\n",
    "$$\n",
    "J_s(S,O) = \\sum\\limits_l \\lambda^{[l]} \\cdot J_s^{[l]}(S,O)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436da817-cee8-40d2-9bf1-a715fdb05617",
   "metadata": {},
   "source": [
    "### Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9955a0-2d16-475c-9746-76a26a6a6406",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "This section describes common CNN architectures that are formally documented and well-known."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bf8227-c27f-437d-a429-429a90a6f73d",
   "metadata": {},
   "source": [
    "#### Classic Networks\n",
    "This section briefly discusses some classic CNNs make use of the foundational techniques discussed so far.\n",
    "\n",
    "__LeNet-5__ was created by Yann LeCun et al in 1998 and is documented [here](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf). The goal of this network is to recognize grayscale handwritten digits of size $(32,32,1)$. At that time, padding and max pooling were not commonly used. The activation functions were sigmoid and hyperbolic tangent (tanh) rather than ReLU. Aside from these implementation differences, LeNet-5 uses several convolutional + average pooling combination layers. The activation function was applied after pooling, which is also uncommon today. The network ends with 2 fully connected layers to make a prediction against 10 classes (1 per digit) in a manner similar to softmax. A non-softmax implementation with 84 units in the final layer was used to achieve this.\n",
    "\n",
    "__AlexNet__ was created by Alex Krizhevsky et al in 2012 and is documented [here](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). It was designed for general-purpose image recognition. It relies heavily on \"same\" padding to preserve the image size when performing convolutions, while also using max pooling to reduce the image dimension over time. It also relies on the ReLU activation. The general architecture is similar to LeNet-5 as the convolutional/pooling blocks are placed first, followed by some fully-connected layers. The final layer uses a 1000-class softmax activation to classify the images. In terms of total parameters, it was roughly 1,000 times larger than LeNet-5 (60k versus 60M).\n",
    "\n",
    "__VGG-16__ (Visual Geometry Group) was created by Karen Simonyan et al in 2015 and is documented [here](https://arxiv.org/pdf/1409.1556). Although AlexNet worked well for large-scale computer image recognition, VGG-16 proposed a simpler architecture by always using same-padded convolutions combined with uniformly-sized max pooling operations. Specifically, it used convolutional layers with filters of size 3 and stride of 1, plus max pooling of size 2 and stride of 2. This is significantly reduces the number of hyperparameters as they do not vary by layer. Broadly speaking, the pooling operations decrease the height and width by half while the convolutional operations increasing the depth by doubling the number of filters applied. The network is more than twice the size of AlexNet with 138M parameters. Another popular version is VGG-19 which has 19 layers.\n",
    "\n",
    "For their time, these classic architectures were very effective and helped reveal a common pattern in CNN design. As images progress through the network, their heights $n_H$ and widths $n_W$ tend to decrease due to convolutional and pooling layers. Concurrently, their depths $n_C$ tend to increase as more and more filters are applied. Because those filters carry trainable weights, they are unrolled at the end of the convolutional/pooling sequence and fed into fully-connected layers to make a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97179-85a7-46ce-b8e2-91a89bf7daf3",
   "metadata": {},
   "source": [
    "#### Residual Network (ResNet)\n",
    "This architecture was designed by Kaiming He et al in 2014 and is documented [here](https://arxiv.org/pdf/1512.03385). As neural networks become deeper, they may suffer from the vanishing/exploding gradients problem described earlier. Although proper weight initialization can reduce the likelihood of this problem occurring, one can insert __residual blocks__ into the network as well. This block is a pair of layers whereby the activation input to the first layer $A^{[l]}$ is used in two different places. First, it is passed to the next layer where it is used as the input to the linear function and its corresponding non-linearity activation function yields the next activation output. This isn't new.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l+1]} & = W^{[l+1]} \\bullet A^{[l]} + b^{[l+1]} \\\\\n",
    "A^{[l+1]} & = g(Z^{[l+1]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Second, the original activation values $A^{[l]}$ are passed via a \"shortcut\" path to layer $l+2$ and are added to that layer's linear combination, modifying the input to the activation function. Note that the matrix $W_s^{[l+1]}$ is meant to reshape the dimensions of $A^{[l]}$ so that they match those of $Z^{[l+2]}$. It doesn't have to materially change the values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "Z^{[l+2]} & = W^{[l+2]} \\bullet A^{[l+1]} + b^{[l+2]} \\\\\n",
    "A^{[l+2]} & = g(Z^{[l+2]} + W_s^{[l+2]} \\bullet A^{[l]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Expanding $Z^{[l+2]}$ into its component parts, we can eliminate some variables. Assuming some kind of regularization is in use, $W^{[l+2]}$ will be pushed close to 0, reducing its impact on $A^{[l+1]}$. Although the biases $b^{[l+2]}$ are not commonly regularized, it tends to have a minor influence on activation values, especially when compared to the weights. Let's ignore them. Finally, assuming that the activation function is ReLU, positive inputs are returned without modification $ReLU(x) = x, x \\in \\mathbb{R}|x > 0$. This results in the activations at the second layer being equal to activations at the input layer 2 levels before.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "A^{[l+2]} & = ReLU^{[l+2]}(Z^{[l+2]} + W_s^{[l+1]} \\bullet A^{[l]}) \\\\\n",
    "A^{[l+2]} & = ReLU^{[l+2]}(\\cancel{W^{[l+2]}} \\bullet \\cancel{A^{[l+1]}} + \\cancel{b^{[l+2]}} + W_s^{[l+2]} \\bullet A^{[l]}) \\\\\n",
    "A^{[l+2]} & = ReLU^{[l+2]}(W_s^{[l+2]} \\bullet A^{[l]}) \\\\\n",
    "A^{[l+2]} & = W_s^{[l+2]} \\bullet A^{[l]}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This unique property of residual blocks allows the network to easily learn the identity function $f(x) = x$. This prevents the degradation of performance because if a future layer of the neural network has the same activations of a layer 2 levels before it, then the neural network's depth (number of layers) can safely increase. If the identity function is the worst that happens due to extra depth, there is no harm done, and performance could potentially increase as well.\n",
    "\n",
    "Residual networks make extensive use of \"same\" padding in their convolutions to ensure that the dimensions of $A^{[l]}$ and $Z^{[l+2]}$ are identical. When this isn't possible or desirable, $W_s^{[l+2]}$ performs the reshaping, as discussed earlier. In some cases, it contains trainable parameters. It can also be used to pad $A^{[l]}$ using an identity matrix. In practice, the implementation of the aforementioned statements involves differentiating between two block types:\n",
    "\n",
    "1. __Identity block__: the dimensions of $A^{[l]}$ and $A^{[l+2]}$ are the same, so no convolutions are needed.\n",
    "\n",
    "2. __Convolutional block__: the dimensions of $A^{[l]}$ and $A^{[l+2]}$ differ. Along the shortcut path, there is a convolutional operation with no/linear activation to represent $W_s^{[l+2]}$. This reshapes $A^{[l]}$ appropriately.\n",
    "\n",
    "For a simplified, ResNet-independent example, consider a 3-length vector needing to be compatible with a 4-length vector. A $(4,3)$ identity matrix is used for $W_s^{[l+2]}$ to effectively append zeros to the 3-length vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538293be-fd85-4cd2-b947-31da8adedbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_l = np.array([3,3,3])\n",
    "z_l2 = np.array([4,4,4,4])\n",
    "ws_l2 = np.eye(4, 3, dtype=int)\n",
    "print(\"a_l:\", a_l)\n",
    "print(\"z_l2:\", z_l2)\n",
    "print(f\"ws_l2:\\n{i}\")\n",
    "print(\"ws_l2 @ a_l:\", ws_l2 @ a_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8467321-fde3-44c5-a5b5-fec1c62d7fdf",
   "metadata": {},
   "source": [
    "#### Inception Network\n",
    "This architecture was created by Christian Szegedy et al in 2014 and is documented [here](https://arxiv.org/pdf/1409.4842). Sometimes selecting the type of convolution to apply at a given layer, along with its hyperparameters, is difficult. This network architecture suggests using multiple techniques concurrently at a single layer and stacking their resulting volumes depth-wise. This way, then network can adjust whatever weights it needs to, rather than relying on a human to select them beforehand.\n",
    "\n",
    "Consider the following example on an input image of shape $(64,64,3)$.\n",
    "\n",
    "1. Apply 64 filters of shape $(1,1,3)$ yielding a volume of shape $(64,64,64)$.\n",
    "2. Apply 128 filters of shape $(3,3,3)$ with same-padding, yielding a volume of shape $(64,64,128)$.\n",
    "3. Apply 32 filters of shape $(5,5,3)$ with same-padding, yielding a volume of shape $(64,64,32)$.\n",
    "4. Apply 32 max pooling matrices of shape $(3,3)$ with same-padding (rare instance of padding with pooling), yielding a volume of shape $(64,64,32)$.\n",
    "5. Perform channel concatenation to create a volume of shape $(64,64,256)$.\n",
    "\n",
    "This design has high computation cost, but additional $(1,1,3)$ filters can be applied to compress channels along the way. This is sometimes called a \"bottleneck layer\" and doesn't appear to hurt performance when done responsibly. You can apply this compression before the application of a regular filter (steps 2 and 3) and/or after the application of pooling (step 4). It doesn't make sense to apply such a technique when the filter size is $(1,1)$ (step 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85c549-53dd-4fad-ab46-c3e3abf5010d",
   "metadata": {},
   "source": [
    "#### Mobile Network (MobileNet)\n",
    "This architecture was designed for small computers and mobile devices that have a limited computing capacity. As described so far, the standard convolutional process is very computationally expensive, requiring many millions of addition and multiplication operations. An alternative process known as __depthwise separable convolution__ is better suited for small devices. There are two steps:\n",
    "\n",
    "1. Depthwise operation: Convolutional filters typically have the same depth as the volume upon which they are acting ($f_C^{[l]}=n_C^{[l-1]}$). In MobleNet, the filters are flat, 2D matrices ($f_C^{[l]}=1$), but their quantity is equal to the input volume's depth ($n_C^{[l-1]}$). Each filter runs pairwise to a corresponding channel of the input volume, resulting in an intermediate output volume of equal depth as the input volume.\n",
    "2. Pointwise operation: Taking the intermediate output volume of the previous step, apply the desired number of filters $n_C^{[l]}$ of size $(1,1,n_C^{[l-1]})$. This yields the final result $Z^{[l]}$ with the correct number of output channels $n_C^{[l]}$.\n",
    "\n",
    "There are two versions of MobileNet, both of which were written by Andrew Howard et al:\n",
    "1. Version 1 ([2017](https://arxiv.org/pdf/1704.04861)): 13 layers of depthwise-separable convolutions, followed by max pooling, fully-connected layers and a softmax prediction.\n",
    "2. Version 2 ([2019](https://arxiv.org/pdf/1801.04381)): 17 layers of residual connections, expansion (deliberately increase volume with \"network in network\" filters), depthwise convolutions, projection (aka pointwise) convolutions, followed by max pooling, fully-connected layers, and a softmax prediction.\n",
    "\n",
    "In the general case, this modified behavior can result in massive improvements. The formula below computes the ratio between the computational cost required for a MobileNet layer versus a traditional convolutional layer. Suppose that the filter has shape $(3,3)$ and the number of output channels $n_C^{[l]}$ is 512. Smaller numbers represent more dramatic improvements. Dividing 1 by this result provides the magnitude of improvement, which in this example is nearly 9 times better.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "r & = \\frac{1}{n_C^{[l]}} + \\frac{1}{f^2} \\\\\n",
    "r & = \\frac{1}{512} + \\frac{1}{3^2} \\\\\n",
    "r & = \\frac{1}{512} + \\frac{1}{9} \\\\\n",
    "r & \\approx 0.113 \\\\\n",
    "1/r & \\approx 8.845\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Smaller numbers represent more dramatic improvements. Dividing 1 by this result provides the magnitude of improvement, which in this example is about 8.84 times better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4634163-f485-433c-9d3e-dbd3544eb935",
   "metadata": {},
   "source": [
    "#### Efficient Network (EfficientNet)\n",
    "This architecture was created by Mingxing Tan et al in 2020 and is documented [here](https://arxiv.org/pdf/1905.11946). It proposes a method of adjusting the existing network structure for scaling purposes. This includes the depth of network $d$, the width of its layers $w$, and resolution of the input image $r$. Rather than try to manually adjust these hyperparameters to find the optimal result (often via trial-and-error), EfficientNet can uniformly scale them with a fixed ratio.\n",
    "\n",
    "To discover that ratio, each of the 3 variables is scaled by a constant coefficient described below. All coefficients are real numbers greater than or equal to 1 ($n \\in \\mathbb{R}| \\ge 1$). The algorithm then fixes the coefficient exponent $\\phi$ to 1 and performs a grid search to find values of the coefficient to satisfy the constraint. Example values for these coefficients might be $\\alpha=1.2$, $\\beta=1.12$, and $\\gamma=1.15$. \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "d & := d \\cdot \\alpha^\\phi \\\\\n",
    "w & := w \\cdot \\beta^\\phi \\\\\n",
    "r & := r \\cdot \\gamma^\\phi \\\\\n",
    "2 & \\approx \\alpha \\cdot \\beta^2 \\cdot \\gamma^2\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd39c2-cdf6-440d-8e25-9b71a0cff85f",
   "metadata": {},
   "source": [
    "#### U-Net\n",
    "Rather than draw coarse bounding boxes around detected objects (as seen with YOLO), U-Net classifies individual pixels as being part of a certain class. This is called __semantic segmentation__ and provides precise object detection for its intended use case of medical imaging. This network is so named because it resembles the letter \"U\" when drawn with the left downward sloping curve resembling encoding/contraction and the right upward sloping curve resembling decoding/expansion.\n",
    "\n",
    "Typical CNNs will decrease the volume's height and width while increasing the number of channels. A fully convolutional network (FCN) uses transpose convolutional layers (described earlier) instead of fully-connected layers to expand the volume to its original height and width $(n_H^{[0]}$ and $n_W^{[0]})$. U-Net builds on the FCN concept by using an equal number of contracting (downsampling) and expansion (upsampling) layers to accomplish this. It also adds residual \"skip\" connections between corresponding layers when the volume shapes are the same. Set another way, for every max pooling layer that decrease the height and width, there is an opposite but equal transpose convolutional layer that restores it. These opposing layers are connected with a aforementioned skip connection, reducing data loss from the original input while also reducing overfitting.\n",
    "\n",
    "In general, the contracting layers (aka contracting path) use 2 convolutional layers with ReLU activation, followed by a max pooling layer. The expanding layers (aka expanding path) use 2 convolutional layers with ReLU activation, followed by a transpose convolutional layer. None of the aforementioned convolutional layers change the height or width as they all use the \"same\" padding. Given an RBG input image of shape $(n_H^{[0]}, n_W^{[0]}, 3)$, U-Net outputs a prediction of shape $(n_H^{[0]}, n_W^{[0]}, n_C^{[L]})$ where the output channels $n_C^{[L]}$ equals the number of possible classes. U-Net uses the sparse categorical cross entropy loss function, similar to softmax, to predict the probabilities for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7d426a-1159-48e7-afa8-3936b67c2b9c",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88076200-cb19-4c98-9a5a-414916b7404d",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e8ba8-2067-496e-88e4-f00a926f47d2",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Unlike CNNs and MLPs, which are \"feed forward\" networks, RNNs can loop back onto themselves for additional processing. This is effective when making predictions based on time-series data (more context) and also to generate variable-length sequences with a different number of elements than the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c8f3a3-a88c-4731-aa1e-f5f67fd9edfa",
   "metadata": {},
   "source": [
    "#### Categories\n",
    "There are 3 types of RNN with respect to input and output sizes:\n",
    "  1. __Many to many__: Given many time series inputs from $x$, there are many predictions (outputs) $\\hat{y}$, one at each time step. Sometimes the number of inputs and outputs are equal, such as in Named Entity Recognition (NER). For example, \"Nick slept for one hour\" -> \"NICK slept for ONE hour\". Sometimes the input and output lengths are unequal, such as in language translation, whereby the encoder consumes the inputs and the decoder makes predictions. For example, \"what would you like to eat?\" -> \"\".\n",
    "  2. __Many to one__: Given many time series inputs from $x$, there is a single prediction (output) $\\hat{y}$ at the end, just one output at the end. An example might be sentiment analysis, mapping \"this ice cream tastes awful\" to 1-star and \"it's the best ice cream ever\" to 5-star.\n",
    "  3. __One to many__: Given a single input $x$, there are many predictions (outputs) $\\hat{y}$, one at each time step. This can be used for music generation, which is a sequence of notes, given only a genre as input. The input could be larger, like an image, while the output can be a text caption. This is also called a __sequence model__ and uses the previous time-step's prediction as input to the current step given that there is only one $x$ input to start the process. Sometimes $x$ can even be all zeros, allowing the sequence model to predict whatever it wants (based on how it was trained)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edfeff1-7d48-40bf-819c-3b33b661f43c",
   "metadata": {},
   "source": [
    "### Basic RNN Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf8007-95ee-4b58-8e3f-2b8717cb4015",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "Each RNN unit has 2 inputs: the actual training input associated with this time step $x^{<t>}$ and the previous time step's activation values $a^{<t-1>}$. Assuming the network has a single layer (i.e. it is not a Deep RNN), the trainable weights $W$ and biases $b$ are shared across all time steps.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "x^{<t>} & = \\text{t'th time step of the training input. shape: } (n_x, m) \\\\\n",
    "y^{<t>} & = \\text{t'th time step of the training output. shape: } (n_y, m) \\\\\n",
    "T_x & = \\text{number of time steps in the training input} \\\\\n",
    "T_y & = \\text{number of time steps in the training output} \\\\\n",
    "W_{ax} & = \\text{weights multiplied by an x-value to yield an a-value. shape: } (n_a, n_x) \\\\\n",
    "W_{aa} & = \\text{weights multiplied by an a-value to yield an a-value. shape: } (n_a, n_a) \\\\\n",
    "W_{ya} & = \\text{weights multiplied by an a-value to yield an y-value. shape: } (n_y, n_a) \\\\\n",
    "a^{<t>} & = \\text{activations at the t'th time step. shape: } (n_a, m) \\\\\n",
    "b_a & = \\text{biases for the activations. shape: } (n_a, 1) \\\\\n",
    "b_y & = \\text{biases for the labels. shape: } (n_y, 1) \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458062f3-dce2-4b98-a0ab-32912116eb43",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "The basic RNN unit, which lacks any kind of long-term memory cells or gates, takes a matrix $x^{<t>}$ as input along with the previous time step's activation values $a^{<t-1>}$. At the first time step, this is commonly set to all zeros. The weight matrices $W$ and bias column vectors $b$ are not specific to a time step and are shared across all units in the layer.\n",
    "\n",
    "At a minimum, the RNN unit outputs the next time step's activation values $a^{<t>}$. It may also output a prediction $\\hat{y}$ based on those activation values.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{<0>} & = \\vec{0} \\\\\n",
    "a^{<t>} & = g(W_{aa} \\bullet a^{<t-1>} + W_{ax} \\bullet x^{<t>} + b_a) \\\\\n",
    "\\hat{y}^{<t>} & = g(W_{ya} \\bullet a^{<t>} + b_y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The above equations are sometimes written in an alternative notation. The two input matrices $a^{<t-1>}$ and $x^{<t>}$ can be vertically concatenated because they share the same number of columns (equal to the number of training examles $m$). The weight matrices $W_{ax}$ and $W_{aa}$ are likewise concatenated, annotated as simple $W_a$, which carries all the weights in one structure. This is an implementation detail only. For simplicity, $W_{ya\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{<t>} & = \\sigma(W_a \\bullet [a^{<t-1>}, x^{<t>}] + b_a) \\\\\n",
    "\\hat{y}^{<t>} & = \\text{softmax}(W_y \\bullet a^{<t>} + b_y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f4b74410-29b1-4821-aa04-e883d7177dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a_t (4, 5):\n",
      "[[ 0.24301521  0.88478664 -0.79578289  0.66607401  0.57861784]\n",
      " [-0.97160825 -0.25363724  0.90638331  0.68734382 -0.46716548]\n",
      " [ 0.98612867  0.99624801  0.97002273  0.99953763  0.81593238]\n",
      " [-0.32167774 -0.50931501 -0.87771549 -0.89086494 -0.50309409]]\n",
      "\n",
      "yhat_t (3, 5):\n",
      "[[0.10084556 0.31132595 0.83056048 0.75792909 0.22572618]\n",
      " [0.89116408 0.67951562 0.15113202 0.23184855 0.76345607]\n",
      " [0.00799036 0.00915844 0.0183075  0.01022237 0.01081776]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "def basic_block_forward(x_t, a_t1, params):\n",
    "    def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "    def softmax(z): return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "\n",
    "    # Vertically stack a_t1 and x_t because they both have \"m\" columns\n",
    "    a_t1_x_t = np.concatenate((a_t1, x_t), axis=0)\n",
    "    a_t = np.tanh(params[\"Wa\"].dot(a_t1_x_t) + params[\"ba\"])\n",
    "\n",
    "    # Make sure a_t equals the expanded form version of the formula\n",
    "    assert np.allclose(a_t, np.tanh(params[\"Waa\"].dot(a_t1) + params[\"Wax\"].dot(x_t) + params[\"ba\"]))\n",
    "    \n",
    "    yhat_t = softmax(params[\"Wy\"].dot(a_t) + params[\"by\"])\n",
    "    return a_t, yhat_t\n",
    "\n",
    "n_x = 2; n_y = 3; n_a = 4; m = 5; params = {}\n",
    "params[\"ba\"] = np.random.randn(n_a, 1)\n",
    "params[\"Wy\"] = np.random.randn(n_y, n_a)\n",
    "params[\"by\"] = np.random.randn(n_y, 1)\n",
    "params[\"Waa\"] = np.random.randn(n_a, n_a)\n",
    "params[\"Wax\"] = np.random.randn(n_a, n_x)\n",
    "\n",
    "# Horizontally stack Waa and Wax because they both have \"n_a\" rows\n",
    "params[\"Wa\"] = np.concatenate((params[\"Waa\"], params[\"Wax\"]), axis=1)\n",
    "\n",
    "a_t1 = np.random.randn(n_a, m)\n",
    "x_t = np.random.randn(n_x, m)\n",
    "\n",
    "matrices = basic_block_forward(x_t, a_t1, params)\n",
    "for matrix, name in zip(matrices, [\"a_t\", \"yhat_t\"]):\n",
    "    print(f\"\\n{name} {matrix.shape}:\\n{matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee6ef03-8a9d-4a26-b5d9-b2b8ca99acd9",
   "metadata": {},
   "source": [
    "#### Backward Propagation\n",
    "\n",
    "To make equations easier to understand, this document describes the placeholder value $k$ to represent the element-wise product between the next time step's activation gradients $\\frac{\\partial J}{\\partial a^{<t>}}$ and the derivative of the hyperbolic tangent function. For completeness, both styles of notation are included.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "k & = \\frac{\\partial J}{\\partial a^{<t>}} \\odot (1 - \\text{tanh}(W_a \\bullet [a^{<t-1>}, x^{<t>}] + b_a)^2) \\\\\n",
    "k & = \\frac{\\partial J}{\\partial a^{<t>}} \\odot (1 - \\text{tanh}(W_{aa} \\bullet a^{<t-1>} + W_{ax} \\bullet x^{<t>} + b_a)^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Thanks to the place holder value $k$, the following derivatives can be computed. Note that $k$ has the same shape of $\\frac{\\partial J}{\\partial a^{<t>}}$, which has the same shape as $a^{<t>}$, which has shape $(n_a, m)$. The other gradient matricies also have the same shapes as their forward propagation counterparts.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W_{ax}} & = k \\bullet x^{<t>^T} \\\\\n",
    "\\frac{\\partial J}{\\partial W_{aa}} & = k \\bullet a^{<t-1>^T} \\\\\n",
    "\\frac{\\partial J}{\\partial b_{a}} & = \\sum \\limits_{i=0}^m k \\\\\n",
    "\\frac{\\partial J}{\\partial x^{<t>}} & = {W_{ax}}^T \\bullet k \\\\\n",
    "\\frac{\\partial J}{\\partial a^{<t-1>}} & = {W_{aa}}^T \\bullet k\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "aa7c5ba1-edc4-4d39-b7b6-2461e1ab7b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dx_t (2, 5):\n",
      "[[ 0.36173371  0.07258235 -0.3127076   0.51667735  0.23796537]\n",
      " [ 0.16746586  0.0099127  -0.08901304  0.5075427   0.02782837]]\n",
      "\n",
      "da_t1 (4, 5):\n",
      "[[ 0.30914988 -0.33199847  0.41233003  1.17203946  0.06023531]\n",
      " [ 0.09549288  0.01934565 -0.05286536 -0.2580408   0.15143542]\n",
      " [ 0.09603845  0.1104004   0.43067282 -0.7869324  -0.1809735 ]\n",
      " [-0.09767265  0.44923884 -0.20677381 -0.93088201 -0.29272867]]\n",
      "\n",
      "dWax (4, 2):\n",
      "[[ 0.12854002  0.01784723]\n",
      " [-0.7449965  -0.68421813]\n",
      " [-0.51125855 -0.42117767]\n",
      " [-0.60615459 -0.57714979]]\n",
      "\n",
      "dWaa (4, 4):\n",
      "[[ 0.1408728   0.06199351 -0.29395468 -0.03514647]\n",
      " [ 0.78079271  0.99458584 -1.17711032  0.19267877]\n",
      " [-0.19048651  0.35752149 -0.35359639 -0.16566155]\n",
      " [ 0.01796849  0.67614165 -0.17139042 -1.18797038]]\n",
      "\n",
      "dba (4, 1):\n",
      "[[0.08316439]\n",
      " [1.46755994]\n",
      " [0.41245529]\n",
      " [1.10011785]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "def basic_block_backward(da_t, fc):\n",
    "    \n",
    "    a_t1_x_t = np.concatenate((fc[\"a_t1\"], fc[\"x_t\"]), axis=0)\n",
    "    k = da_t * (1 - np.tanh(fc[\"Wa\"].dot(a_t1_x_t) + fc[\"ba\"]) ** 2)\n",
    "\n",
    "    # Make sure k equals the expanded form version of the formula\n",
    "    assert np.allclose(k, da_t * (1 - np.tanh(fc[\"Waa\"].dot(fc[\"a_t1\"]) + fc[\"Wax\"].dot(fc[\"x_t\"]) + fc[\"ba\"]) ** 2))\n",
    "\n",
    "    # Compute derivatives for (a_t1,x_t), Wa(a,x), and b\n",
    "    da_t1_x_t = fc[\"Wa\"].T.dot(k)\n",
    "    dWa = k.dot(a_t1_x_t.T)\n",
    "    dba = np.sum(k, keepdims=True, axis=-1)\n",
    "\n",
    "    # Undo concat to split specific derivatives apart from aggregate matrices\n",
    "    dWaa, dWax = dWa[:, :n_a], dWa[:, n_a:] \n",
    "    da_t1, dx_t = da_t1_x_t[:n_a,:], da_t1_x_t[n_a:,:]\n",
    "\n",
    "    return dx_t, da_t1, dWax, dWaa, dba\n",
    "\n",
    "n_x = 2; n_a = 4; m = 5\n",
    "fwd_cache = {k: np.random.randn(n_a, m) for k in [\"a_t1\", \"a_t\", \"da_t\"]}\n",
    "fwd_cache[\"Waa\"] = np.random.randn(n_a, n_a)\n",
    "fwd_cache[\"Wax\"] = np.random.randn(n_a, n_x)\n",
    "fwd_cache[\"ba\"] = np.random.randn(n_a, 1)\n",
    "fwd_cache[\"x_t\"] = np.random.randn(n_x, m)\n",
    "\n",
    "# Horizontally stack Waa and Wax because they both have \"n_a\" rows\n",
    "fwd_cache[\"Wa\"] = np.concatenate((fwd_cache[\"Waa\"], fwd_cache[\"Wax\"]), axis=1)\n",
    "\n",
    "matrices = basic_block_backward(fwd_cache.pop(\"da_t\"), fwd_cache)\n",
    "\n",
    "for matrix, name in zip(matrices, [\"dx_t\", \"da_t1\", \"dWax\", \"dWaa\", \"dba\"]):\n",
    "    print(f\"\\n{name} {matrix.shape}:\\n{matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3e22d-0641-4623-965a-cfe85aecd339",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit (GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c0466-2b68-49b3-b277-fd847ac43da8",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "\n",
    "This architecture introduces a memory cell $c^{<t>}$ which helps retain information across all the time steps. Somewhat like residual connections in a CNN ResNet, this technique helps resolve the vanishing gradient problem. The architecture introduces 2 gates which helps control the flow of information through the time steps.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "c^{<t>} & = \\text{memory cell at t'th time step. shape: } (n_a, m) \\\\\n",
    "\\tilde{c}^{<t>} & = \\text{candidate value for replacing the memory cell. shape: } (n_a, m) \\\\\n",
    "\\Gamma_u & = \\text{update gate; 0 permits a change, 1 denies a change. shape: } (n_a, m) \\\\\n",
    "\\Gamma_r & = \\text{reset gate; 0 means retain less info, 1 means retain more info. shape: } (n_a, m)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e2ccb-3812-4ebf-be84-49658324af7e",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "The update gate $\\Gamma_u$ determines whether the memory cell $c^{<t>}$ should be updated using a candidate replacement value $\\tilde{c}^{<t>}$. When the update gate is 0, the last time step's memory cell value $c^{<t-1>}$ is passed along. When the update gate is 1, the candidate replacement value $\\tilde{c}^{<t>}$ is passed along.\n",
    "\n",
    "The reset gate $\\Gamma_r$ determines whether the memory cell $c^{<t>}$ should be reset. When the reset gate $\\Gamma_r$ is 0, the last time step's memory cell value $c^{<t-1>}$ is removed from consideration when computing $\\tilde{c}^{<t>}$. When the reset gate $\\Gamma_r$ is 1, the last time step's memory cell value $c^{<t-1>}$ remains in consideration when computing $\\tilde{c}^{<t>}$. Although the implementation is different, the function has so much similar to the forget gate $\\Gamma_f$ in LSTM.\n",
    "\n",
    "Note that all of these new matrices have the same shape of $(n_a, m)$ as they all relate to activation values. Also, note that $a^{<t>}$ and $c^{<t>}$ are identical, but separating them here will make understanding LSTM blocks easier later.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gamma_u & = \\sigma(W_u \\bullet [c^{<t-1>},x^{<t>}] + b_u) \\\\\n",
    "\\Gamma_r & = \\sigma(W_r \\bullet [c^{<t-1>},x^{<t>}] + b_r) \\\\\n",
    "\\tilde{c}^{<t>} & = \\text{tanh}(W_c \\bullet [\\Gamma_r \\odot c^{<t-1>},x^{<t>}] + b_c) \\\\\n",
    "c^{<t>} & = \\Gamma_u \\odot \\tilde{c}^{<t>} + (1-\\Gamma_u) \\odot c^{<t-1>} \\\\\n",
    "a^{<t>} & = c^{<t>} \\\\\n",
    "\\hat{y}^{<t>} & = \\text{softmax}(W_{y} \\bullet a^{<t>} + b_y)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a7fa13ce-a1dd-461e-adf2-94818f146683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_t (4, 5):\n",
      "[[ 0.89958687  0.4040248   0.59584638 -1.41588439  0.75794585]\n",
      " [ 0.21007908  0.96382507 -0.44370123  1.76041557 -0.99684935]\n",
      " [ 0.72514232  0.55184065 -0.29644799  0.81399907 -0.28379372]\n",
      " [-1.54654969 -1.1450336   0.40978402 -0.3784983  -0.07548489]]\n",
      "\n",
      "yhat_t (3, 5):\n",
      "[[0.51235857 0.69898642 0.17868475 0.24544786 0.13121629]\n",
      " [0.40976102 0.22948506 0.06727819 0.65270014 0.11611912]\n",
      " [0.07788042 0.07152852 0.75403706 0.10185201 0.75266459]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "def gru_block_forward(x_t, c_t1, params):\n",
    "    def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "    def softmax(z): return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    # Vertically stack c_t1 and x_t because they both have width \"m\"\n",
    "    c_t1_x_t = np.concatenate((c_t1, x_t), axis=0)\n",
    "    gates = {l: sigmoid(params[f\"W{l}\"].dot(c_t1_x_t) + params[f\"b{l}\"]) for l in \"ur\"}\n",
    "\n",
    "    # Only bcast-multiply the \"top half\" of the concat matrix (rows 0 to n_a)\n",
    "    c_t1_x_t[:c_t1.shape[0], :] *= gates[\"r\"]\n",
    "    c_t_tilde = np.tanh(params[\"Wc\"].dot(c_t1_x_t) + params[\"bc\"])\n",
    "    c_t = (gates[\"u\"] * c_t_tilde) + ((1 - gates[\"u\"]) * c_t1)\n",
    "    a_t = c_t\n",
    "    yhat_t = softmax(params[\"Wy\"].dot(a_t) + params[\"by\"])\n",
    "    return a_t, yhat_t\n",
    "\n",
    "n_x = 2; n_y = 3; n_a = 4; m = 5\n",
    "params = {f\"W{l}\": np.random.randn(n_a, n_a + n_x) for l in \"urc\"}\n",
    "params |= {f\"b{l}\": np.random.randn(n_a, 1) for l in \"urc\"}\n",
    "params[\"Wy\"] = np.random.randn(n_y, n_a)\n",
    "params[\"by\"] = np.random.randn(n_y, 1)\n",
    "\n",
    "c_t1 = np.random.randn(n_a, m)\n",
    "x_t = np.random.randn(n_x, m)\n",
    "\n",
    "a_t, yhat_t = gru_block_forward(x_t, c_t1, params)\n",
    "print(f\"a_t {a_t.shape}:\\n{a_t}\\n\")\n",
    "print(f\"yhat_t {yhat_t.shape}:\\n{yhat_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66016a7-73b0-483d-a0f7-3ee154cbce16",
   "metadata": {},
   "source": [
    "### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f76af-e5e3-4a5a-a547-f13472c3b9f7",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "This block type is older and more powerful than GRU, but more complex to implement and slower to run, especially on larger data sets. It also solves the vanishing gradient problem by passing information through the time steps, except uses different equations for $a^{<t>}$ and $c^{<t>}$, along with 3 gates. Note that these activation-related matrices all have the same shape of $(n_a, m)$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "{c}^{<t>} & = \\text{memory cell at t'th time step. shape: } (n_a, m) \\\\\n",
    "\\tilde{c}^{<t>} & = \\text{candidate value for replacing the memory cell. shape: } (n_a, m) \\\\\n",
    "\\Gamma_u & = \\text{update gate; 0 permits a change, 1 denies a change. shape: } (n_a, m) \\\\\n",
    "\\Gamma_f & = \\text{forget gate; 0 remembers a value, 1 forgets a value. shape: } (n_a, m) \\\\\n",
    "\\Gamma_o & = \\text{output gate; used to compute the next activation. shape: } (n_a, m)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de48649-84ba-41d0-88f1-03f9fe00dd88",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n",
    "\n",
    "The update gate $\\Gamma_u$ plays an similar role in LSTM as it did with GRU, but this time, it only influences whether the candidate memory cell value $\\tilde{c}^{<t>}$ should be included when computing $c^{<t>}$ (when 1) or not (when 0). The forget gate $\\Gamma_f$ determines whether the previous time step's memory cell $c^{<t-1>}$ influences the current time step's memory cell $c^{<t>}$. A value of 0 remembers (does not forget) that previous value, while a value of 1 forgets it. The output gate $\\Gamma_o$ is the final modifier to the time step's activation values $a^{<t>}$, choosing to retain (when 1) or clear (when 0) values before passing to the next time step.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Gamma_u & = \\sigma(W_u \\bullet [a^{<t-1>},x^{<t>}] + b_u) \\\\\n",
    "\\Gamma_f & = \\sigma(W_f \\bullet [a^{<t-1>},x^{<t>}] + b_f) \\\\\n",
    "\\Gamma_o & = \\sigma(W_o \\bullet [a^{<t-1>},x^{<t>}] + b_o) \\\\\n",
    "\\tilde{c}^{<t>} & = \\text{tanh}(W_c \\bullet [a^{<t-1>},x^{<t>}] + b_c) \\\\\n",
    "c^{<t>} & = \\Gamma_u \\odot \\tilde{c}^{<t>} + (1-\\Gamma_f) \\odot c^{<t-1>} \\\\\n",
    "a^{<t>} & = \\Gamma_o \\odot \\text{tanh}(c^{<t>})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0bea1e0a-c420-4df3-bb9a-ea64dd3c8b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_t (4, 5):\n",
      "[[-0.92629771 -1.04634537 -2.09637026  0.15368677 -0.48567077]\n",
      " [ 0.39501359  1.0440999   0.82786095  0.27254698  0.87291927]\n",
      " [-0.91399404 -0.58501897  0.25829643 -0.63065691  0.55144623]\n",
      " [-0.49132468 -0.61167535  0.62097266  0.08373668 -1.22169781]]\n",
      "\n",
      "a_t (4, 5):\n",
      "[[-0.51722213 -0.69928665 -0.93076489  0.13979062 -0.43831087]\n",
      " [ 0.37323665  0.27677095  0.04980654  0.26464559  0.66962105]\n",
      " [-0.17783203 -0.1722247   0.14266404 -0.28891531  0.34834478]\n",
      " [-0.2337893  -0.16626539  0.30157273  0.00344807 -0.14602265]]\n",
      "\n",
      "yhat_t (3, 5):\n",
      "[[0.10232122 0.11523386 0.09073917 0.12042771 0.03047996]\n",
      " [0.58186336 0.59572435 0.64494706 0.49179344 0.61653583]\n",
      " [0.31581542 0.28904179 0.26431377 0.38777885 0.35298421]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5)\n",
    "def lstm_block_forward(x_t, a_t1, c_t1, params):\n",
    "    def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "    def softmax(z): return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "    \n",
    "    # Vertically stack a_t1 and x_t because they both have width \"m\"\n",
    "    a_t1_x_t = np.concatenate((a_t1, x_t), axis=0)\n",
    "    gates = {l: sigmoid(params[f\"W{l}\"].dot(a_t1_x_t) + params[f\"b{l}\"]) for l in \"ufo\"}\n",
    "    c_t_tilde = np.tanh(params[\"Wc\"].dot(a_t1_x_t) + params[\"bc\"])\n",
    "    c_t = gates[\"f\"] * c_t1 + gates[\"u\"] * c_t_tilde\n",
    "    a_t = gates[\"o\"] * np.tanh(c_t)\n",
    "    yhat_t = softmax(params[\"Wy\"].dot(a_t) + params[\"by\"])\n",
    "    return c_t, a_t, yhat_t\n",
    "\n",
    "n_x = 2; n_y = 3; n_a = 4; m = 5\n",
    "params = {f\"W{l}\": np.random.randn(n_a, n_a + n_x) for l in \"ufoc\"}\n",
    "params |= {f\"b{l}\": np.random.randn(n_a, 1) for l in \"ufoc\"}\n",
    "params[\"Wy\"] = np.random.randn(n_y, n_a)\n",
    "params[\"by\"] = np.random.randn(n_y, 1)\n",
    "\n",
    "a_t1 = np.random.randn(n_a, m)\n",
    "c_t1 = np.random.randn(n_a, m)\n",
    "x_t = np.random.randn(n_x, m)\n",
    "\n",
    "c_t, a_t, yhat_t = lstm_block_forward(x_t, a_t1, c_t1, params)\n",
    "print(f\"c_t {c_t.shape}:\\n{c_t}\\n\")\n",
    "print(f\"a_t {a_t.shape}:\\n{a_t}\\n\")\n",
    "print(f\"yhat_t {yhat_t.shape}:\\n{yhat_t}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dc6601-e784-43c1-856b-c9eee9d0e2dc",
   "metadata": {},
   "source": [
    "### Alternative Network Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c29f5-2c1c-45d2-bda3-c9a2da4b1d22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Overview\n",
    "This section describes RN and architectures that use the aforementioned building blocks in unique configurations. Many of the technologies discussed thus far can be mixed and matched freely given their modularity. For example, you can build a bidirectional RNN using GRU or LSTM cells. The same is true for a Deep RNN. You can even build a Deep RNN that is also bidirectional."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130c369b-65ed-4514-9f04-0baef654868c",
   "metadata": {},
   "source": [
    "#### Bidirectional RNN (BRNN)\n",
    "\n",
    "Rather than just computing activation values $a^{<t>}$ in the forward direction from $x^{<1>}$ up to $x^{<T_x>}$, perform the same process in reverse. This backwards process computes $a^{<t>}_{back}$ from $x^{<T_x>}$ down to $x^{<1>}$. Said another way, each time step input $x^{<t>}$ feeds into an RNN block to compute $a^{<t>}_{forw}$ and a separate such block to compute $a^{<t>}_{back}$. This allows any prediction $\\hat{y}^{<t>}$ to see all inputs $x^{<t>}$ from past or future time steps. The result is an acyclic graph, where the forward and backward activation values flow in opposite directions, but remain isolated.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{<t>}_{forw} & = \\sigma(W_{a\\_forw} \\bullet [a^{<t-1>}_{forw}, x^{<t>}] + b_{a\\_forw}) \\\\\n",
    "a^{<t>}_{back} & = \\sigma(W_{a\\_back} \\bullet [a^{<t+1>}_{back}, x^{<t>}] + b_{a\\_back}) \\\\\n",
    "\\hat{y}^{<t>} & = \\text{softmax}(W_y \\bullet [a^{<t>}_{forw},a^{<t>}_{back}] + b_y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that this is not backwards propagation and is unrelated to computing gradients. The directions are sometimes called \"forward recurrent component\" and \"backwards recurrent component\". The drawback is that a BRNN requires all the input before making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45fadde-8498-46f9-88af-7b403aaa8bbe",
   "metadata": {},
   "source": [
    "#### Deep RNN\n",
    "\n",
    "In addition to the temporal component, present in all RNs, a deep RNN adds multiple layers. What was normally a prediction $\\hat{y}^{<t>}$ from the first layer $[0]$ becomes the input to the second layer $[1]$ at the corresponding timestamp $t$. The final output from the topmost layer $[L]$ becomes the true prediction $\\hat{y}^{<t>}$. Much like an MLP, there are different trainable weights $W_a^{[l]}$ and biases $b_a^{[l]}$ at each layer.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "a^{[l]<t>} & = \\sigma(W_a^{[l]} \\bullet [a^{[l]<t-1>},a^{[l-1]<t>}] + b_a^{[l]}) \\\\\n",
    "\\hat{y}^{<t>} & = \\text{softmax}(W_y^{[L]} \\bullet [a^{[L]<t-1>},a^{[L-1]<t>}] + b_y^{[L]})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Unlike an MLP, RNNs already have temporal dimension, so even 3 or 4 layers is considered \"deep\" given that they are computationally expensive to train. It is also common to take the final layer's output prediction from a given time step and feed it as input into an MLP for additional processing. The same MLP is used at each time step, but the these MLPs are not linked horizontally; they process their individual inputs (the outputs from the deep RNN) separately. The final output from his MLP becomes the actual prediction $\\hat{y}^{<t>}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45c59d-19d4-49e0-9f4e-b816d63d3c4e",
   "metadata": {},
   "source": [
    "### Text Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d8617-3331-4552-bacd-1136ad5c3b75",
   "metadata": {},
   "source": [
    "#### Overview\n",
    "\n",
    "Embeddings are numeric representations of text stored in 1D vectors. They allow texts to be contextually compared with one another and rely on many characteristics/features. Using various linear algebra formulas (such as cosine similarity or Euclidean distance), the similarity can be quantified. Viewed as a column vector, each embedding is like a training example, whereas the length of the embedding is the number of features. Many such vectors arrayed horizontally forms an embedding matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a23711-c0b7-40f3-a6e5-731947c15fb8",
   "metadata": {},
   "source": [
    "#### Computation\n",
    "\n",
    "Given a dictionary (list) of words, assign incremental numbers to every word in the dictionary from 1 to $m$ and compute the one-hot encoded (OHE) vector for each word. For example, in a dictionary with 4 words, the second word OHE vector is `[0, 1, 0, 0]`. It is represented as $o_2$ is formulas, or more generally, $o_w$.\n",
    "\n",
    "Given an embedding matrix $E$, take the dot product between the matrix the OHE word vector $o_w$. Matrix $E$ has shape $(D, V)$ where $D$ is the dimensionality of the embedding (number of features) and $V$ is the number of words in the dictionary (vocabulary size). Because vector $o_w$ has shape $(V, 1)$, the resulting word embedding $e_w$ has shape $(D, 1)$. In this way, the word embedding operation selects a column from the embedding matrix.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "o_w & = \\text{onehot\\_encode}(w) \\\\\n",
    "e_w & = E \\bullet o_w\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Given 2 embedding vectors, one can compare them for similarity. There are two methods:\n",
    "\n",
    "  1.  The __cosine similarity__ ($S_c$) method measures only the angle between 2 vectors from the origin, ignoring their magnitude. This has the advantage of comparing directionality rather than exact position. The values range from -1 to 0, with 0 resembling no correlation (\"happy\" and \"tree\"), 1 resembling close correlation (\"happy\" and \"glad\"), and -1 resembling opposite correlation (\"happy\" and \"sad\"). Note that cosine distance is sometimes used as well, which is $1 - S_c$, ranging from 0 to 2. This time, 1 resembles no correlation (\"happy\" and \"tree\"), 0 resembles close correlation (\"happy\" and \"glad\"), and 2 resembles opposite correlation (\"happy\" and \"sad\").\n",
    "\n",
    "  2.  The __Euclidian distance__ ($d$) method compares the exact position of 2 vectors, accounting for both the angle and magnitude. It is sometimes written as simply $\\|\\vec{a}-\\vec{b}\\|_2$ and can never be negative as it measures distance. As such, it cannot easily distinguish a large negative correlation (\"happy\" and \"sad\") with no correlation (\"happy\" and \"tree\") as both distances are large. It would require multiple computations/comparisons to make such a conclusion. As such, it is less commonly used in modern embedding computations.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "S_c(\\vec{a}, \\vec{b}) & = \\frac{\\vec{a} \\bullet \\vec{b}}{\\|\\vec{a}\\|_2 - \\|\\vec{b}\\|_2} \\\\\n",
    "d(\\vec{a}, \\vec{b}) & = \\sqrt{\\sum \\limits_{j=0}^n (\\vec{a}_j - \\vec{b}_j)^2} = \\|\\vec{a}-\\vec{b}\\|_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Consider this example. There are 4 words in the dictionary and 5 feet. The features range from negative one to one, and I've been populated with possible values. Values close to 0 mean that the word has no relation to the feature; none of these animals can fly and thus have no airspeed. Values close to -1 or 1 means a strong relation to the feature, but in opposite directions. Puppies and kittens have a strong age dimension on one side (young), but dogs and cats also have a strong dimension on the other side (not young). All of the animals are mammals; they all have a strong relation to that feature.\n",
    "\n",
    "|              | puppy (1) | dog (2) | kitten (3) | cat (4) |\n",
    "| ------------ | --------- | ------- | ---------- | ------- |\n",
    "| years age    |  -0.87    |  0.65   |  -0.91     |  0.7    |\n",
    "| dig hole     |  0.82     |  0.9    |  0.03      |  0.1    |\n",
    "| retract claw |  0.02     | -0.09   |  0.85      |  0.92   |\n",
    "| airspeed     |  0.01     | -0.01   |  0.03      |  0.02   |\n",
    "| mammal       |  0.94     |  0.97   |  0.95      |  0.98   |\n",
    "\n",
    "Embeddings allow us to predict new words based on relationships between other words. The code below answers the question: __\"Puppy is to dog as kitten is to what?\"__ The answer is __\"cat\"__, and the vector values are printed to the terminal and plotted using PCA. Because the embedding matrix has more than 2 features, 2D visualization techniques (like PCA and others) are useful for learning/testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "737babbc-ee91-486d-9564-8de37b3254da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_matrix (5, 4)\n",
      "[[ 0.87 -0.65  0.91 -0.7 ]\n",
      " [ 0.82  0.9   0.03  0.1 ]\n",
      " [ 0.02 -0.09  0.85  0.92]\n",
      " [ 0.01 -0.01  0.03  0.02]\n",
      " [ 0.94  0.97  0.95  0.98]]\n",
      "\n",
      "red:    [-0.61  0.11  0.74  0.01  0.98]\n",
      "blue 4: [-0.7   0.1   0.92  0.02  0.98]\n",
      "cosine similarity:  0.9952\n",
      "euclidian distance: 0.2017\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cosine_similarity(a, b): return a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "def euclidian_distance(a, b): return np.linalg.norm(a - b)\n",
    "\n",
    "emb_matrix = np.array([\n",
    "    [0.87, -0.65, 0.91, -0.7],\n",
    "    [0.82, 0.9, 0.03, .1],\n",
    "    [0.02, -0.09, 0.85, 0.92],\n",
    "    [0.01, -0.01, 0.03, 0.02],\n",
    "    [0.94, 0.97, 0.95, 0.98],\n",
    "])\n",
    "print(f\"emb_matrix {emb_matrix.shape}\\n{emb_matrix}\\n\")\n",
    "names = [\"puppy\", \"dog\", \"kitten\", \"cat\"]\n",
    "word_embs = {name: emb_matrix.dot(row) for row, name in zip(np.eye(len(names)), names)}\n",
    "\n",
    "# puppy - dog = kitten - ?\n",
    "cat_pred = -word_embs[\"puppy\"] + word_embs[\"dog\"] + word_embs[\"kitten\"]\n",
    "print(f\"red:    {cat_pred}\")\n",
    "print(f\"blue 4: {word_embs['cat']}\")\n",
    "print(f\"cosine similarity:  {cosine_similarity(cat_pred, word_embs['cat']):.4f}\")\n",
    "print(f\"euclidian distance: {euclidian_distance(cat_pred, word_embs['cat']):.4f}\")\n",
    "\n",
    "X = np.append(emb_matrix, cat_pred.reshape(-1,1), axis=1).T\n",
    "pca_2 = PCA(n_components=2)\n",
    "pca_2.fit(X)\n",
    "\n",
    "X_pca_2 = pca_2.transform(X)\n",
    "pc1 = X_pca_2[:, 0]\n",
    "pc2 = X_pca_2[:, 1]\n",
    "evr = np.sum(pca_2.explained_variance_ratio_) * 100\n",
    "\n",
    "plt.scatter(pc1[:-1], pc2[:-1], color=\"b\", label=\"original dictionary words\")\n",
    "plt.scatter(pc1[-1], pc2[-1], color=\"r\", label=r\"-puppy + dog + kitten $\\approx$ cat\")\n",
    "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.title(f\"Embedding PCA - EVR {evr:.4f}%\"); plt.legend()\n",
    "\n",
    "for i in range(X.shape[0] - 1):\n",
    "    plt.annotate(i+1, (pc1[i], pc2[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27138267-44c2-41ef-8053-28320ebd3c0f",
   "metadata": {},
   "source": [
    "#### Learning with Neural Language Model\n",
    "\n",
    "This technique was created by Yoshua Bengio et al in 2003 and is documented [here](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf). The goal is to learn text embeddings using a relatively simple MLP. First, compute the embedding vectors $e_w$ for a fixed number of words within a sentence. This number of words is a hyperparameter $n$ which influences the number of units in the first MLP layer. Given an embedding dimensionality $D$ and the number of words $n$, the shape of the input matrix $X$ becomes $(D \\times n, m)$ where $m$ is the number of $n$-word training examples. The first layer uses hyperbolic tangent activation.\n",
    "\n",
    "The second and final layer uses a softmax activation, interpreted as an OHE vector selecting the correct next word. As such, it has shape $(V,m)$ where $V$ is the vocabulary size. Both of the MLP layers have traditional weights $W^{[l]}$ and biases $b^{[l]}$ to be trained. Additionally, the embedding matrix $E$ is also trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef48121a-7ba4-4bc2-9c2a-c055bc33af54",
   "metadata": {},
   "source": [
    "#### Learning with word2vec\n",
    "\n",
    "This technique was created by Thomas Mikolov et al in 2013 and is documented [here](https://arxiv.org/pdf/1301.3781). word2vec simpler way to learn embeddings and is an implementation of the \"skip gram\" method because given some context, it can skip some words in either direction and predict a target word.\n",
    "\n",
    "create context/target pairs\n",
    "randomly pick context and target (within range of context)\n",
    "\n",
    "just embedding a softmax unit. \n",
    "\n",
    "map c to t (x to y)\n",
    "\n",
    "$\\theta_t$ is change of target word $o_t$ being the correct answer (label).\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "e_c & = o_c \\bullet E \\\\\n",
    "\\hat{y} & = softmax(e_c) \\\\\n",
    "p(t|c) & = \\frac{e^{{\\theta_t}^T} \\bullet e_c}{\\sum_{j=1}^V e^{{\\theta_j}^T} \\bullet e_c}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In practice, this algorithm is computational expensive due to the softmax activation given a large vocabulary $V$. An alternative is to use a hierarchical softmax design by determining if the correct word is in the first or last \"half\" of the vocabularly. These \"halves\" are not necessarily equal in size and the word order is not necessarily preserved. More common words tend to be shallower in the tree (closer to the ropt), causing the algorithm to run faster. This reduces the algorithm's time complexity from linear to logarithmic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a7a2f-7487-4385-b3c2-7d614e99c6fd",
   "metadata": {},
   "source": [
    "#### Learning with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34998451-0a27-4ec1-8500-e0a336e62d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
